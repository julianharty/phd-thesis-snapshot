@comment{https://tex.stackexchange.com/questions/159806/add-text-comment-into-bibliography}
@comment{https://tex.stackexchange.com/questions/439328/adding-comments-in-bib-file-that-survive-bibdesks-save}
@comment{MUST_DO review the content of citations, https://tex.stackexchange.com/questions/400593/warning-empty-institution-and-warning-empty-journal}
@comment{also useful https://tex.stackexchange.com/questions/175106/make-phd-citations-say-dissertation-rather-than-thesis}
@comment{https://www.openoffice.org/bibliographic/bibtex-defs.html provides a succinct guide of the main definition types and their fields.}

@comment{Here's a blank template useful to copy, paste and then populate with the details of the reference.}
\iffalse
@online{_blank_2019,
    title = {},
    url = {},
    abstract = {},
    author = {},
    urldate = {},
    date = {}
}
/fi

@article{acm_shonan_workshops_2020,
    author = {Kawarabayashi, Ken-Ichi},
    title = {The NII Shonan Meeting in Japan},
    year = {2020},
    issue_date = {April 2020},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {63},
    number = {4},
    issn = {0001-0782},
    url = {https://doi.org/10.1145/3378546},
    doi = {10.1145/3378546},
    journal = {Commun. ACM},
    month = mar,
    pages = {48–49},
    numpages = {2}
}

@article{agrawal2020_better_software_analytics_via_duo,
    author={Agrawal, Amritanshu
    and Menzies, Tim
    and Minku, Leandro L.
    and Wagner, Markus
    and Yu, Zhe},
    title={Better software analytics via ``DUO'': Data mining algorithms using/used-by optimizers},
    journal={Empirical Software Engineering},
    year={2020},
    month={May},
    day={01},
    volume={25},
    number={3},
    pages={2099-2136},
    abstract={This paper claims that a new field of empirical software engineering research and practice is emerging: data mining using/used-by optimizers for empirical studies, or DUO. For example, data miners can generate models that are explored by optimizers. Also, optimizers can advise how to best adjust the control parameters of a data miner. This combined approach acts like an agent leaning over the shoulder of an analyst that advises ``ask this question next'' or ``ignore that problem, it is not relevant to your goals''. Further, those agents can help us build ``better'' predictive models, where ``better'' can be either greater predictive accuracy or faster modeling time (which, in turn, enables the exploration of a wider range of options). We also caution that the era of papers that just use data miners is coming to an end. Results obtained from an unoptimized data miner can be quickly refuted, just by applying an optimizer to produce a different (and better performing) model. Our conclusion, hence, is that for software analytics it is possible, useful and necessary to combine data mining and optimization using DUO.},
    issn={1573-7616},
    doi={10.1007/s10664-020-09808-9},
    url={https://doi.org/10.1007/s10664-020-09808-9}
}



@article{ali2019behavior_catrobat,
  title={Behavior-Driven Development as an Error-Reduction Practice for Mobile Application Testing},
  author={Ali, Zulfiqar},
  journal={International Journal of Computer Science Issues (IJCSI)},
  volume={16},
  number={2},
  pages={1--10},
  year={2019},
  publisher={International Journal of Computer Science Issues (IJCSI)},
  doi = {10.5281/zenodo.3234110},
  url = {https://www.ijcsi.org/papers/IJCSI-16-2-1-10.pdf},
}

@article{ali2019using_catrobat,
	author = {Zulfiqar Ali and Aiman M Awwad and Wolfgang Slany},
	title = {Using Executable Specification and Regression Testing for Broadcast Mechanism of Visual Programming Language on Smartphones},
	journal = {International Journal of Interactive Mobile Technologies (iJIM)},
	volume = {13},
	number = {02},
	year = {2019},
	keywords = {Mobile Application, Regression Testing, Behavior Driven Development, Visual Programming Environment, Catrobat},
	abstract = {The rapid advancement of mobile computing technology and the rising usage of mobile apps made our daily life more productive. The mobile app should operate all the time bug-free in order to improve user satisfaction and offers great business value to the end user. At the same time, smartphones are full of special features that make testing of apps more challenging. Actually, the quality is a must for successful applications and it cannot be achieved without testing and verification. In this paper, we present the Behavior Driven Development (BDD) methodology and Cucumber framework to automate regression testing of Android apps. Particularly, the proposed methods use the visual programming language for smartphones (Catrobat) as a reference. The Catrobat program scripts communicate via a broadcast mechanism. The objective is to test the broadcast mechanism from different angles and track regression errors as well as specify and diagnose bugs with the help of executable specifications. The results show that the methods are able to effectively reveal deficiencies in the broadcast mechanism, and ensure that the app matches all expectations and needs of end users.},
	issn = {1865-7923},
	url = {https://onlinejour.journals.publicknowledgeproject.org/index.php/i-jim/article/view/9851},
	pages = {50--65},
}

@article{ALNAWAS2016313,
title = "The effect of benefits generated from interacting with branded mobile apps on consumer satisfaction and purchase intentions",
journal = "Journal of Retailing and Consumer Services",
volume = "31",
pages = "313 - 322",
year = "2016",
issn = "0969-6989",
doi = "https://doi.org/10.1016/j.jretconser.2016.04.004",
url = "http://www.sciencedirect.com/science/article/pii/S0969698916300364",
author = "Ibrahim Alnawas and Faisal Aburub",
keywords = "Usage and gratification approach, Experiential engagement, Mobile apps",
abstract = "This study extends the “Uses and Gratifications” approach (U&G) from a web context to a new one, i.e. mobile applications. It seeks to investigate the effect of the key benefits generated from interacting with branded mobile apps on consumer satisfaction and purchase intentions. A self-administrated questionnaire was used to collect the study data. The questionnaire was distributed to 358 participants inside seven major malls in a Middle Eastern country (Jordan). Purposive sample was employed. The data were analyzed using structural equation modeling (AMOS 18). Four key findings emerged from the current research. First, the study confirms the existence of four interaction-based benefits in the context of mobile apps, namely: learning benefits, social integrative benefits, personal integrative benefits and hedonic benefits. Second, apart from social integrative benefits, the other three benefits are found to influence consumer satisfaction to varying degrees. Third, with regard to purchase intentions, only learning benefits and hedonic benefits are found to generate that. Finally, the study confirms the relationship between consumer satisfaction and purchase intentions in a mobile context. The study contributes to the literature through adopting the U&G approach as a theoretical base to examine the key benefits that consumers gain when interacting with branded mobile apps."
}

@article{alsubaihin2019app_store_effects_on_software_engineering,
  title={App store effects on software engineering practices},
  author={A. {AlSubaihin} and F. {Sarro} and S. {Black} and L. {Capra} and M. {Harman}},
  abstract={In this paper, we study the app store as a phenomenon from the developers perspective to investigate the extent to which app stores affect software engineering tasks. Through developer interviews and questionnaires, we uncover findings that highlight and quantify the effects of three high-level app store themes: bridging the gap between developers and users, increasing market transparency and affecting mobile release management. Our findings have implications for testing, requirements engineering and mining software repositories research fields. These findings can help guide future research in supporting mobile app developers through a deeper understanding of the app store-developer interaction.},
  journal={IEEE Transactions on Software Engineering},
  year={2019},
  publisher={IEEE},
  volume = {47},
  number = {2},
  pages={300-319},
  doi={10.1109/TSE.2019.2891715}},
}

@ARTICLE{anderson2015_docker_software_engineering_podcast,
  author={Anderson, Charles},
  journal={IEEE Software}, 
  title={Docker [Software engineering]},
  year={2015},
  volume={32},
  number={3}, 
  pages={102-c3}, 
  abstract={
    In episode 217 of Software Engineering Radio, host Charles Anderson talks with James Turnbull, a software developer and security specialist who's vice president of services at Docker. Lightweight Docker containers are rapidly becoming a tool for deploying microservice-based architectures.},  
  keywords={},  
  doi={10.1109/MS.2015.62},  
  ISSN={1937-4194},  
  month={May},
  full_podcast = {https://www.se-radio.net/2015/01/episode-217-james-turnbull-on-docker/},
  quote = {
    The DevOps movement, for example, emerged from one of the classic stumbling blocks in a lot of organizations. Developers build code and applications and ship them to the operations people, only to discover that the code and applications don't run in production. This is the classic “it works on my machine; it's operations' problem now.”
  },
  thoughts = {This provides a suitable connection between DevOps and mobile app world, where developers don't seem to actively consider whether their code will work in other environments, other conditions, etc. Many organisations struggle to get their server-side software reliable across various test environments and then into production. Mobile appstore ecosystems exacerbate the challenges as the environments are more varied, owned by third parties and shared with code from many other third-parties (other apps on the device).
  
  The test channels provided by Google Play Console provide a bridge between the app development that happens within an organisation and one that starts to include external, real-world elements including the automated checks and robot tests, and alpha/beta testers with their individual microcosms (devices and device accounts).}
}

@article{automated_testing_android_apps_SLR_2019, 
    author={P. {Kong} and L. {Li} and J. {Gao} and K. {Liu} and T. F. {Bissyandé} and J. {Klein}}, 
    journal={IEEE Transactions on Reliability}, 
    title={Automated Testing of Android Apps: A Systematic Literature Review}, 
    year={2019}, 
    volume={68}, 
    number={1}, 
    pages={45-66}, 
    abstract={Automated testing of Android apps is essential for app users, app developers, and market maintainer communities alike. Given the widespread adoption of Android and the specificities of its development model, the literature has proposed various testing approaches for ensuring that not only functional requirements but also nonfunctional requirements are satisfied. In this paper, we aim at providing a clear overview of the state-of-the-art works around the topic of Android app testing, in an attempt to highlight the main trends, pinpoint the main methodologies applied, and enumerate the challenges faced by the Android testing approaches as well as the directions where the community effort is still needed. To this end, we conduct a systematic literature review during which we eventually identified 103 relevant research papers published in leading conferences and journals until 2016. Our thorough examination of the relevant literature has led to several findings and highlighted the challenges that Android testing researchers should strive to address in the future. After that, we further propose a few concrete research directions where testing approaches are needed to solve recurrent issues in app updates, continuous increases of app sizes, as well as the Android ecosystem fragmentation.}, 
    keywords={mobile computing;program testing;software engineering;market maintainer communities;development model;Android testing researchers;Android ecosystem fragmentation;Android apps automated testing;Testing;Androids;Humanoid robots;Bibliographies;Ecosystems;Java;Systematics;Android;automated testing;literature review;survey}, 
    doi={10.1109/TR.2018.2865733}, 
    ISSN={0018-9529}, 
    month={March},
}

@article{avison1999_action_research,
  title={Action research},
  author={Avison, David E and Lau, Francis and Myers, Michael D and Nielsen, Peter Axel},
  journal={Communications of the ACM},
  volume={42},
  number={1},
  pages={94--97},
  year={1999},
  publisher={ACM New York, NY, USA}
}

@article{avizienis2004_basic_concepts_and_taxonomy,
  key_paper_reason = {Seminal reference of key terms and concepts},
  author={A. {Avizienis} and J. -. {Laprie} and B. {Randell} and C. {Landwehr}},
  journal={IEEE Transactions on Dependable and Secure Computing},
  title={Basic concepts and taxonomy of dependable and secure computing},
  year={2004},
  volume={1},
  number={1},
  pages={11-33},
  abstract={
    This paper gives the main definitions relating to dependability, a generic concept including a special case of such attributes as reliability, availability, safety, integrity, maintainability, etc. Security brings in concerns for confidentiality, in addition to availability and integrity. Basic definitions are given first. They are then commented upon, and supplemented by additional definitions, which address the threats to dependability and security (faults, errors, failures), their attributes, and the means for their achievement (fault prevention, fault tolerance, fault removal, fault forecasting). The aim is to explicate a set of general concepts, of relevance across a wide range of situations and, therefore, helping communication and cooperation among a number of scientific and technical communities, including ones that are concentrating on particular types of system, of system failures, or of causes of system failures.
  },
  keywords={fault tolerant computing;data privacy;security of data;system recovery;software reliability;taxonomy;dependable computing;secure computing;system reliability;system availability;system safety;system integrity;system maintainability;fault prevention;fault tolerance;fault removal;fault forecasting;system security;system failures;system vulnerabilities;system attacks;Taxonomy;Availability;Fault tolerance;Safety;Maintenance;Communication system security;Uncertainty;Standardization;Books;Index Terms- Dependability;security;trust;faults;errors;failures;vulnerabilities;attacks;fault tolerance;fault removal;fault forecasting.},
  doi={10.1109/TDSC.2004.2},
  ISSN={1941-0018},
  month={Jan},
}

@article{ayyal2016automated_bidi_testing,
  title={Automated bidirectional languages localization testing for android apps with rich GUI},
  author={Ayyal Awwad, Aiman M and Slany, Wolfgang},
  journal={Mobile Information Systems},
  volume={2016},
  year={2016},
  publisher={Hindawi},
  doi = {10.1155/2016/2872067},
  pages = {1--13},
}

@article{bach2000_sbtm,
  title = {Session-Based Test Management},
  author = {Jonathan Bach},
  year = {2000},
  journal = {Software Testing and Quality Engineering magazine},
  publisher = {STQE},
  pages = {32--37},
  volume = {2000},
  issue = {06},
  url = {https://www.satisfice.com/download/session-based-test-management},
}

@article{baez2021_chatbot_integrations,
  author={M. {Baez} and F. {Daniel} and F. {Casati} and B. {Benatallah}},
  journal={IEEE Internet Computing}, 
  title={Chatbot integration in few patterns}, 
  year={2021},
  volume={25},
  number={3},
  pages={52-59},
  abstract={Chatbots are software agents that are able to interact with humans in natural language. Their intuitive interaction paradigm is expected to significantly reshape the software landscape of tomorrow, while already today chatbots are invading a multitude of scenarios and contexts. This article takes a developer's perspective, identifies a set of architectural patterns that capture different chatbot integration scenarios, and reviews state-of-the-art development aids.},
  keywords={Computer architecture;Task analysis;Meteorology;Internet;Natural language processing;Software systems},
  doi={10.1109/MIC.2020.3024605},
  ISSN={1941-0131},
  month={},
}

@article{ball2000_putting_ethnography_to_work_cognitive_ethnography,
    title = {Putting ethnography to work: the case for a cognitive ethnography of design},
    journal = {International Journal of Human-Computer Studies},
    volume = {53},
    number = {1},
    pages = {147-168},
    year = {2000},
    issn = {1071-5819},
    doi = {https://doi.org/10.1006/ijhc.2000.0372},
    url = {https://www.sciencedirect.com/science/article/pii/S1071581900903720},
    author = {Linden J. Ball and Thomas C. Ormerod},
    abstract = {
      The methods of ethnography and cognitive psychology are frequently set in opposition to each other. Whilst such a view may be appropriate in defining pure, or prototypical, classes of each activity, the value and necessity of such a distinction is broken down when researchers are goal-directed to study complex work domains in order to foster technological change. In this paper, we outline a rapprochement of these methods, which we term cognitive ethnography. The value of qualifying ethnography in this way is to emphasize systematically the differences between ethnography as a radial category and the kinds of legitimate method used to study work practices which are often referred to as ethnographic, but which in practice differ in important ways from prototypical ethnographic studies. Features of cognitive ethnography such as observational specificity, verifiability and purposivenes challenge many of the tenets of a pure ethnographic method, yet they are essential for studies that are undertaken to inform technological change. We illustrate our arguments with reference to a project to develop a tool for supporting design re-use in innovative design environments.
    }
}

@article{barroca_2018_bridging_the_gap,
	title = {Bridging the gap between research and agile practice: an evolutionary model},
	volume = {9},
	issn = {0976-4348},
	url = {https://doi.org/10.1007/s13198-015-0355-5},
	doi = {10.1007/s13198-015-0355-5},
	journal = {International Journal of System Assurance Engineering and Management},
	abstract = {There is wide acceptance in the software engineering field that industry and research can gain significantly from each other and there have been several initiatives to encourage collaboration between the two. However there are some often-quoted challenges in this kind of collaboration. For example, that the timescales of research and practice are incompatible, that research is not seen as relevant for practice, and that research demands a different kind of rigour than practice supports. These are complex challenges that are not always easy to overcome. Since the beginning of 2013 we have been using an approach designed to address some of these challenges and to bridge the gap between research and practice, specifically in the agile software development arena. So far we have collaborated successfully with three partners and have investigated three practitioner-driven challenges with agile. The model of collaboration that we adopted has evolved with the lessons learned in the first two collaborations and been modified for the third. In this paper we introduce the collaboration model, discuss how it addresses the collaboration challenges between research and practice and how it has evolved, and describe the lessons learned from our experience.},
	pages = {323--334},
	number = {2},
	journaltitle = {International Journal of System Assurance Engineering and Management},
	shortjournal = {International Journal of System Assurance Engineering and Management},
	author = {Barroca, Leonor and Sharp, Helen and Salah, Dina and Taylor, Katie and Gregory, Peggy},
	date = {2018-04-01},
	year = {2018},
}

@article{bavota2014_impact_of_api_change_android,
    author={G. {Bavota} and M. {Linares-Vásquez} and C. E. {Bernal-Cárdenas} and M. D. {Penta} and R. {Oliveto} and D. {Poshyvanyk}},  
    journal={IEEE Transactions on Software Engineering},
    title={The Impact of API Change- and Fault-Proneness on the User Ratings of Android Apps},
    publisher = {IEEE},
    year={2015},
    volume={41},
    number={4},
    pages={384-407},
    abstract = {The mobile apps market is one of the fastest growing areas in the information technology. In digging their market share, developers must pay attention to building robust and reliable apps. In fact, users easily get frustrated by repeated failures, crashes, and other bugs; hence, they abandon some apps in favor of their competition. In this paper we investigate how the fault- and change-proneness of APIs used by Android apps relates to their success estimated as the average rating provided by the users to those apps. First, in a study conducted on 5,848 (free) apps, we analyzed how the ratings that an app had received correlated with the fault- and change-proneness of the APIs such app relied upon. After that, we surveyed 45 professional Android developers to assess (i) to what extent developers experienced problems when using APIs, and (ii) how much they felt these problems could be the cause for unfavorable user ratings. The results of our studies indicate that apps having high user ratings use APIs that are less fault- and change-prone than the APIs used by low rated apps. Also, most of the interviewed Android developers observed, in their development experience, a direct relationship between problems experienced with the adopted APIs and the users' ratings that their apps received.},
    
    keywords = {application program interfaces;data mining;mobile computing;program debugging;software fault tolerance;system recovery;API change-proneness;API fault-proneness;user ratings;Android Apps;mobile Apps market;information technology;software repository mining;Androids;Humanoid robots;Software;History;Computer bugs;Educational institutions;Electronic mail;Mining software repositories;empirical studies;android;API changes;Mining software repositories;empirical studies;android;API changes},
    doi={10.1109/TSE.2014.2367027},
    ISSN={1939-3520},
    month={April},
}

@ARTICLE{baxter2011_socio-technical-systems-from-design-methods-to-systems-engineering, 
    author={Baxter, Gordon and Sommerville, Ian}, 
    journal={Interacting with Computers}, 
    title={Socio-technical systems: From design methods to systems engineering}, 
    year={2011},
    volume={23}, 
    number={1}, 
    pages={4-17}, 
    abstract={
        It is widely acknowledged that adopting a socio-technical approach to system development leads to systems that are more acceptable to end users and deliver better value to stakeholders. Despite this, such approaches are not widely practised. We analyse the reasons for this, highlighting some of the problems with the better known socio-technical design methods. Based on this analysis we propose a new pragmatic framework for socio-technical systems engineering (STSE) which builds on the (largely independent) research of groups investigating work design, information systems, computer-supported cooperative work, and cognitive systems engineering. STSE bridges the traditional gap between organisational change and system development using two main types of activity: sensitisation and awareness; and constructive engagement. From the framework, we identify an initial set of interdisciplinary research problems that address how to apply socio-technical approaches in a cost-effective way, and how to facilitate the integration of STSE with existing systems and software engineering approaches.
    }, 
    keywords={},
    doi={10.1016/j.intcom.2010.07.003},
    ISSN={1873-7951},
    month={Jan},
}

@article{bevan1999_89_quality_in_use_meeting_user_needs_for_quality,
    title = {Quality in use: Meeting user needs for quality},
    journal = {Journal of Systems and Software},
    volume = {49},
    number = {1},
    pages = {89-96},
    year = {1999},
    issn = {0164-1212},
    doi = {https://doi.org/10.1016/S0164-1212(99)00070-9},
    url = {https://www.sciencedirect.com/science/article/pii/S0164121299000709},
    author = {Nigel Bevan},
    abstract = {There is an increasing demand for software that matches real user needs in a working environment. The paper describes the new framework for software product quality developed for ISO/IEC 9126-1: internal quality (static properties of the code), external quality (behaviour of the software when it is executed) and quality in use (the extent to which the software meets the needs of the user). Quality in use is a broader view of the ergonomic concept of usability in ISO 9241-11 (1998). Achieving quality in use requires a user-centred design process which has cultural, strategic and technical implications.}
}

@article{blichfeldt2006creating,
  title={Creating a wider audience for action research: Learning from case-study research},
  author={Blichfeldt, Bodil Stilling and Andersen, Jesper Rank},
  journal={Journal of Research Practice},
  volume={2},
  number={1},
  pages={D2--D2},
  year={2006},
  url ={http://jrp.icaap.org/index.php/jrp/article/view/23/43},
}


@misc{briscoe2014_digital_innovation_the_hackathon_phenomenon,
  title={Digital Innovation: The Hackathon Phenomenon},
  author={Briscoe, Gerard and Mulligan, Catherine},
  year={2014},
  pages = {13},
  number = {},
  volume = {},
}

@ARTICLE{bringsjord2012_red_pill_robots_please,  
    author={Bringsjord, Selmer and Clark, Micah H.},  
    journal={IEEE Transactions on Affective Computing},  
    title={Red-Pill Robots Only, Please},  
    year={2012},  
    volume={3}, 
    number={4},  
    pages={394-397},  
    abstract={
        Blue-pill robots are engineered to deceive (perhaps in an attempt to secure desirable ends). Red-pill robots, on the other hand, are built to do no violence to truth. While “taking the blue pill” is an option some select, this path, in the context of present and future robotics, is an exceedingly bad one by our lights, and we herein defend this position by attempting to show that the production of blue-pill robots via engineering as we know it should be avoided.
    },
    extract = {
        6.1 Objections against Correspondence Account
        No doubt some readers wishing to dodge our argument against blue-pill robots will object that the argument presupposes the so-called correspondence account of truth. According to this account, a proposition p is true if and only if p corresponds to some part of reality. Smith might, for instance, assert the proposition that the cat is on the mat while looking at the front of Bringsjord's house and spying there a large, sleek, black cat dozing on his “welcome” mat beside his front door. In this scenario, nearly all humans are inclined to say that Smith's assertion is true for the simple reason that there is in fact a cat on Bringsjord's (= the) mat.

        In the context of the present dialectic, the objection here entails a contradiction and must therefore be immediately rejected. This is so because the very nature of the question before us (and for that matter the very nature of the question type before us: red pill versus blue pill in general) implies that there is a choice between reality and illusion—and in the case of the latter, say in the case specifically of The Matrix, a human enveloped by the illusion who believes that there is a black cat in front of him believes a proposition that isn't true for the simple reason that in reality there is no cat (but rather a machine suitably stimulating his brain). Put baldly, the fact of the matter is that those intrigued with the fundamental choice at the heart of the present investigation are all operating on the basis of the correspondence account of truth.
        
        6.2 Objection from Truth-Indifferent Engineers
        A truth-indifferent engineer (or an abject cynic) can object to our argument on the grounds that engineers neither enjoy nor seek propositional pleasure. The truth-indifferent engineer might say: “I'm indifferent to truth. I don't care how it works; so long as the machine does work, I am pleased.” Supposing that the engineers at BPR, Inc., are similarly disposed, is our argument overthrown? We think not.

        We are both at institutions rather renowned for engineering acumen, and these associations have afforded us many opportunities to observe and engage engineers while at their practice, but we have yet to discover even one truth-indifferent engineer and fear the species does not exist. 19 But putting aside what skeptics will see as little more than humorous anecdote, there are good reasons to dismiss this objection.
        
        First, the objection is self-defeating. The statement “So long as the machine does work, I am pleased” implies propositional pleasure, namely, pleasure in the knowledge that “the machine does work.” In turn, this pleasure cannot be obtained unless the machine exists and does work; illusions will not do. The truth-indifferent engineer's attitude ought to have been: “So long as I experience the sensations that would have accompanied my engineering a machine that behaved as I intended it to do, I am pleased.”

        Second, our argument contained the proviso “engineering as we know it”; it is far from clear that one can be both indifferent to truth and engaged in engineering as we know it. In practice and pedagogy, engineering emphasizes means and methods for the application of scientific principles, not just achieving outcomes. Indeed, the definitions of “engineering” given by encyclopedias and professional societies alike clearly require the judicious use of propositional knowledge in the production of artifacts (e.g., see [17], [18], [19]). While full-fledged “philosophy of engineering” is still emerging from infancy, 20 it seems highly improbable that one can be truth-indifferent and still attend to the methodological and teleological concerns of “engineering” as a professional discipline. Thus, we conclude that “truth-indifferent engineer” is an oxymoron.
    },
    keywords={}, 
    doi={10.1109/T-AFFC.2011.35}, 
    ISSN={1949-3045},  
    month={Fourth},
}

@article{brooke2018__becoming_professional_a_university_perspective,
	title = {Becoming Professional A University Perspective},
	volume = {60},
	issn = {1746-5702},
	url = {https://doi.org/10.1093/itnow/bwy037},
	doi = {10.1093/itnow/bwy037},
	abstract = {Examples of arguably unprofessional or unethical behaviour related to {IT} are fairly commonly reported events. For a number of years, {BCS} has been encouraging coverage of related issues within accredited higher education provision. Phil Brooke {FBCS}, Tom Prickett {MBCS}, Shelagh Keogh {MBCS} and David Bowers {FBCS} report on this issue.},
	pages = {16--17},
	number = {2},
	journaltitle = {{ITNOW}},
	journal = {IT NOW},
	author = {Brooke, Phil and Prickett, Tom and Keogh, Shelagh and Bowers, David},
	date = {2018-05},
	year = {2018},
	note = {\_eprint: https://academic.oup.com/itnow/article-pdf/60/2/16/24834372/bwy037.pdf}
}

@article{BROOMHEAD1986_217_extracting_qualitative_dynamics_from_experimental_data,
    title = {Extracting qualitative dynamics from experimental data},
    journal = {Physica D: Nonlinear Phenomena},
    volume = {20},
    number = {2},
    pages = {217-236},
    year = {1986},
    issn = {0167-2789},
    doi = {https://doi.org/10.1016/0167-2789(86)90031-X},
    url = {https://www.sciencedirect.com/science/article/pii/016727898690031X},
    author = {D.S. Broomhead and Gregory P. King},
    abstract = {We consider the notion of qualitative information and the practicalities of extracting it from experimental data. Our approach, based on a theorem of Takens, draws on ideas from the generalized theory of information known as singular system analysis due to Bertero, Pike and co-workers. We illustrate our technique with numerical data from the chaotic regime of the Lorenz model.}
}

@article{not_yet_cited_candido2021_log_based_software_monitoring_a_SLR,
 title = {Log-based software monitoring: a systematic mapping study},
 author = {Cândido, Jeanderson and Aniche, Maurício and van Deursen, Arie},
 year = 2021,
 month = may,
 keywords = {Logging practices, Log infrastructure, Log analysis, DevOps, Monitoring},
 abstract = {
    Modern software development and operations rely on monitoring to understand how systems behave in production. The data provided by application logs and runtime environment are essential to detect and diagnose undesired behavior and improve system reliability. However, despite the rich ecosystem around industry-ready log solutions, monitoring complex systems and getting insights from log data remains a challenge. Researchers and practitioners have been actively working to address several challenges related to logs, e.g., how to effectively provide better tooling support for logging decisions to developers, how to effectively process and store log data, and how to extract insights from log data. A holistic view of the research effort on logging practices and automated log analysis is key to provide directions and disseminate the state-of-the-art for technology transfer. In this paper, we study 108 papers (72 research track papers, 24 journals, and 12 industry track papers) from different communities (e.g., machine learning, software engineering, and systems) and structure the research field in light of the life-cycle of log data. Our analysis shows that (1) logging is challenging not only in open-source projects but also in industry, (2) machine learning is a promising approach to enable a contextual analysis of source code for log recommendation but further investigation is required to assess the usability of those tools in practice, (3) few studies approached efficient persistence of log data, and (4) there are open opportunities to analyze application logs and to evaluate state-of-the-art log analysis techniques in a DevOps context.
 },
 volume = 7,
 pages = {38},
 journal = {PeerJ Computer Science},
 issn = {2376-5992},
 url = {https://doi.org/10.7717/peerj-cs.489},
 doi = {10.7717/peerj-cs.489}
}

@article{carlsson2013truth,
  title={The truth, the whole truth, and nothing but the truth—A multiple country test of an oath script},
  author={Carlsson, Fredrik and Kataria, Mitesh and Krupnick, Alan and Lampi, Elina and L{\"o}fgren, {\AA}sa and Qin, Ping and Sterner, Thomas},
  journal={Journal of Economic Behavior \& Organization},
  volume={89},
  pages={105--121},
  year={2013},
  publisher={Elsevier}
}

@article{catolino2019not,
  title={Not all bugs are the same: Understanding, characterizing, and classifying bug types},
  author={Catolino, Gemma and Palomba, Fabio and Zaidman, Andy and Ferrucci, Filomena},
  journal={Journal of Systems and Software},
  volume={152},
  pages={165--181},
  year={2019},
  publisher={Elsevier}
}

@article{cave2020covid,
  title={COVID-19 super-spreaders: Definitional quandaries and implications},
  author={Cave, Emma},
  journal={Asian bioethics review},
  volume={12},
  pages={235--242},
  year={2020},
  publisher={Springer}
}

@article{chamizodominguez2002_false_friends_their_origins_and_semantics_in_some_languages,
    title = {False friends: their origin and semantics in some selected languages},
    journal = {Journal of Pragmatics},
    volume = {34},
    number = {12},
    pages = {1833-1849},
    year = {2002},
    issn = {0378-2166},
    doi = {https://doi.org/10.1016/S0378-2166(02)00024-3},
    url = {https://www.sciencedirect.com/science/article/pii/S0378216602000243},
    author = {Pedro J. {Chamizo Domínguez} and Brigitte Nerlich},
    keywords = {False friends, Semantics, Pragmatics, Metaphor, Metonymy, Idioms, Borrowing},
    abstract = {
        In this article we want to investigate the semantic (figurative) structures that underlie false friends, especially semantic false friends, in various European languages (Spanish, French, German and English). Chance false friends share the same form but have different etymologies and different meanings in different languages. They can be compared to homonyms in a single natural language. Semantic false friends, by contrast, have the same etymological origin, their meanings differ in different language, but one can still detect semantic relations between them. They can be considered to be cross-linguistic equivalents to polysemous words in a single natural language. The links between their meanings in different languages can be based on metaphor, metonymy and euphemism, but also on specialisation and generalisation. Semantic false friends are the semantic relics of pragmatic language use over time and space. Studying false friends is, however, more than an exercise in diachronic pragmatics. It has important implications for translation and cross-linguistic communication, where an awareness of false friends is important together with knowledge of certain pragmatic strategies, which help to avoid misunderstandings or mistranslations. The study of the underlying figurative links between false friends also adds a new dimension to cognitive semantics.
    }
}

@article{chandra2015_how_to_smash_the_next_billion_mobile_app_bugs,
    author = {Chandra, Ranveer and Karlsson, B\"{o}rje F. and Lane, Nicholas D. and Liang, Chieh-Jan Mike and Nath, Suman and Padhye, Jitu and Ravindranath, Lenin and Zhao, Feng},
    title = {How to the Smash Next Billion Mobile App Bugs?},
    year = {2015},
    issue_date = {January 2015},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {19},
    number = {1},
    issn = {2375-0529},
    url = {https://doi.org/10.1145/2786984.2786997},
    doi = {10.1145/2786984.2786997},
    abstract = {
        With users increasingly dependent on their phones, tablets, and wearables, the mobile app ecosystem is more important today than ever before. Creating and distributing apps has never been more accessible. Even single developers can now reach global audiences. But mobile apps must cope with extremely varied and dynamic operating conditions due to factors like diverse device characteristics, wireless network heterogeneity, and varied user behavior. App developers and operators of app marketplaces both lack testing tools that can effectively account for such diversity and, as a result, app failures and performance bugs (like excessive energy consumption) are commonly found today. To address this challenge to mobile app development, we have developed key techniques for scalable automated mobile app testing within two prototype services --- VanarSena and Caiipa. In this paper, we describe our vision for SMASH, a unified cloud-based mobile app testing service that combines the strengths of both previous systems to tackle the complexities presently faced by testers of mobile apps.
    },
    journal = {GetMobile: Mobile Comp. and Comm.},
    month = {jun},
    pages = {34–38},
    numpages = {5}
}

@article{cruz2019_guess_what_test_your_app,
  title={To the attention of mobile software developers: guess what, test your app!},
  author={Cruz, Luis and Abreu, Rui and Lo, David},
  journal={Empirical Software Engineering},
  volume={24},
  number={4},
  issn = {1573-7616},
  url = {https://doi.org/10.1007/s10664-019-09701-0},
  pages={2438--2468},
  year={2019},
  publisher={Springer},
  date = {2019-08-01},
  doi = {10.1007/s10664-019-09701-0},
  	abstract = {
  	Software testing is an important phase in the software development lifecycle because it helps in identifying bugs in a software system before it is shipped into the hand of its end users. There are numerous studies on how developers test general-purpose software applications. The idiosyncrasies of mobile software applications, however, set mobile apps apart from general-purpose systems (e.g., desktop, stand-alone applications, web services). This paper investigates working habits and challenges of mobile software developers with respect to testing. A key finding of our exhaustive study, using 1000 Android apps, demonstrates that mobile apps are still tested in a very ad hoc way, if tested at all. However, we show that, as in other types of software, testing increases the quality of apps (demonstrated in user ratings and number of code issues). Furthermore, we find evidence that tests are essential when it comes to engaging the community to contribute to mobile open source software. We discuss reasons and potential directions to address our findings. Yet another relevant finding of our study is that Continuous Integration and Continuous Deployment ({CI}/{CD}) pipelines are rare in the mobile apps world (only 26\% of the apps are developed in projects employing {CI}/{CD}) – we argue that one of the main reasons is due to the lack of exhaustive and automatic testing.},
}

@article{davenport2006competing_on_analytics,
  title={Competing on analytics},
  author={Davenport, Thomas H and others},
  journal={Harvard Business Review},
  volume={84},
  number={1},
  pages={98},
  year={2006}
}

@article{davison2021_the_ethics_of_action_research_participation,
    author = {Davison, Robert M. and Martinsons, Maris G. and Wong, Louie H. M.},
    title = {The ethics of action research participation},
    journal = {Information Systems Journal},
    volume = {n/a},
    number = {n/a},
    pages = {1--22},
    year = {2021},
    keywords = {action research, ethics, research collaboration, research consent, research method, research principles},
    doi = {https://doi.org/10.1111/isj.12363},
    url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/isj.12363},
    eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/isj.12363},
    abstract = {
      Abstract Action research (AR) involves one or more researchers and a client organisation. Many guidelines for and reports of the research method have been published. However, the ethical issues associated with AR have been largely neglected. Our review of the AR literature found that ethical dilemmas and their resolution are rarely and inconsistently reported. Stimulated by this neglect and our personal experiences, we aim to raise awareness and understanding about the ethics of planning, conducting and reporting AR. We identify and discuss four issues of concern that merit specific ethical attention when conducting AR: collaboration, competence, persistence and consent. We draw on these four issues in an analysis that augments the principles and criteria for canonical AR (CAR), recently reified as Integrated Action Research (IAR). Our guidance includes an additional principle of AR and 10 associated criteria to address the ethics of AR participation.
    }
}


@article{derakhshanfar2020_search_based_crash_reduction,
  author = {Derakhshanfar, Pouria and Devroey, Xavier and Perrouin, Gilles and Zaidman, Andy and van Deursen, Arie},
  title = {Search-based crash reproduction using behavioural model seeding},
  journal = {Software Testing, Verification and Reliability},
  volume = {30},
  number = {3},
  pages = {e1733},
  keywords = {seed learning, crash reproduction, search-based software testing},
  doi = {https://doi.org/10.1002/stvr.1733},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1733},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/stvr.1733},
  note = {e1733 stvr.1733},
  abstract = {
    Summary Search-based crash reproduction approaches assist developers during debugging by generating a test case, which reproduces a crash given its stack trace. One of the fundamental steps of this approach is creating objects needed to trigger the crash. One way to overcome this limitation is seeding: using information about the application during the search process. With seeding, the existing usages of classes can be used in the search process to produce realistic sequences of method calls, which create the required objects. In this study, we introduce behavioural model seeding: a new seeding method that learns class usages from both the system under test and existing test cases. Learned usages are then synthesized in a behavioural model (state machine). Then, this model serves to guide the evolutionary process. To assess behavioural model seeding, we evaluate it against test seeding (the state-of-the-art technique for seeding realistic objects) and no seeding (without seeding any class usage). For this evaluation, we use a benchmark of 122 hard-to-reproduce crashes stemming from six open-source projects. Our results indicate that behavioural model seeding outperforms both test seeding and no seeding by a minimum of 6\% without any notable negative impact on efficiency.},
  year = {2020},
  thoughts = {
    crash reproduction is relevant to my research as it may help practitioners reproduce the causes of a crash so they can address it effectively.
  }
}


@article{diallo2017_what_is_a_fault_why_does_it_matter,
  title={What is a fault? and why does it matter?},
  author={Diallo, Nafi and Ghardallou, Wided and Desharnais, Jules and Frias, Marcelo and Jaoua, Ali and Mili, Ali},
  journal={Innovations in Systems and Software Engineering},
  volume={13},
  number={2-3},
  pages={219--239},
  year={2017},
  publisher={Springer},
  doi = {10.1007/s11334-017-0300-7},
}

@online{dobbing1998reliability,
  title={Reliability of Smart Instrumentation},
  author={Dobbing, A and Clark, N and Godfrey, D and Harris, P M  and Parkin, G and Stevens, M J and Wichmann, B A},
  organization = {National Physical Laboratory and Druck Ltd.},
  year={1998},
  numpages = {8},
}

@article{dromey1996_cornering_the_chimera,
  title={Cornering the chimera [software quality]},
  author={Dromey, R Geoff},
  journal={IEEE Software},
  volume={13},
  number={1},
  pages={33--43},
  year={1996},
  publisher={IEEE},
  abstract={Concrete and useful suggestions about what constitutes quality software have always been elusive. I suggest a framework for the construction and use of practical, testable quality models for requirements, design and implementation. Such information may be used directly to build, compare, and assess better quality software products.},
}

 @article{endert2014_the_human_in_the_loop_new_directions_for_visual_analytics,
   title={The human is the loop: new directions for visual analytics},
   volume={43},
   ISSN={1573-7675},
   DOI={10.1007/s10844-014-0304-9},
   abstractNote={
     Visual analytics is the science of marrying interactive visualizations and analytic algorithms to support exploratory knowledge discovery in large datasets. We argue for a shift from a ‘human in the loop’ philosophy for visual analytics to a ‘human is the loop’ viewpoint, where the focus is on recognizing analysts’ work processes, and seamlessly fitting analytics into that existing interactive process. We survey a range of projects that provide visual analytic support contextually in the sensemaking loop, and outline a research agenda along with future challenges.},
   number={3}, 
   journal={Journal of Intelligent Information Systems}, 
   author={Endert, Alex and Hossain, M. Shahriar and Ramakrishnan, Naren and North, Chris and Fiaux, Patrick and Andrews, Christopher},
   year={2014},
   month={Dec}, 
   pages={411–435} 
}

@article{falessi2010_applying_ESE_to_sw_architecture_etc,
	title = {Applying empirical software engineering to software architecture: challenges and lessons learned},
	volume = {15},
	issn = {1573-7616},
	url = {https://doi.org/10.1007/s10664-009-9121-0},
	doi = {10.1007/s10664-009-9121-0},
	abstract = {In the last 15 years, software architecture has emerged as an important software engineering field for managing the development and maintenance of large, software-intensive systems. Software architecture community has developed numerous methods, techniques, and tools to support the architecture process (analysis, design, and review). Historically, most advances in software architecture have been driven by talented people and industrial experience, but there is now a growing need to systematically gather empirical evidence about the advantages or otherwise of tools and methods rather than just rely on promotional anecdotes or rhetoric. The aim of this paper is to promote and facilitate the application of the empirical paradigm to software architecture. To this end, we describe the challenges and lessons learned when assessing software architecture research that used controlled experiments, replications, expert opinion, systematic literature reviews, observational studies, and surveys. Our research will support the emergence of a body of knowledge consisting of the more widely-accepted and well-formed software architecture theories.},
	pages = {250--276},
	number = {3},
	journaltitle = {Empirical Software Engineering},
	journal = {Empirical Software Engineering},
	shortjournal = {Empirical Software Engineering},
	author = {Falessi, Davide and Babar, Muhammad Ali and Cantone, Giovanni and Kruchten, Philippe},
	date = {2010-06-01},
	year = {2010},
}

@article{fielding2012_triangulation_and_mixed_methods_designs,
    author = {Nigel G. Fielding},
    title ={Triangulation and Mixed Methods Designs: Data Integration With New Research Technologies},
    journal = {Journal of Mixed Methods Research},
    volume = {6},
    number = {2},
    pages = {124-136},
    year = {2012},
    doi = {10.1177/1558689812437101},
    URL = {https://doi.org/10.1177/1558689812437101},
    eprint = {https://doi.org/10.1177/1558689812437101},
    abstract = { Data integration is a crucial element in mixed methods analysis and conceptualization. It has three principal purposes: illustration, convergent validation (triangulation), and the development of analytic density or “richness.” This article discusses such applications in relation to new technologies for social research, looking at three innovative forms of data integration that rely on computational support: (a) the integration of geo-referencing technologies with qualitative software, (b) the integration of multistream visual data in mixed methods research, and (c) the integration of data from qualitative and quantitative methods.}
}


@article{floridi2019_five_risks_of_being_unethical,
author="Floridi, Luciano",
title="Translating Principles into Practices of Digital Ethics: Five Risks of Being Unethical",
journal="Philosophy {\&} Technology",
year="2019",
month="Jun",
day="01",
volume="32",
number="2",
pages="185--193",
issn="2210-5441",
doi="10.1007/s13347-019-00354-x",
url="https://doi.org/10.1007/s13347-019-00354-x"
}

@article{foidl2018_integrating_software_quality_models_into_risk_based_testing,
  title={Integrating software quality models into risk-based testing},
  author={Foidl, Harald and Felderer, Michael},
  journal={Software quality journal},
  volume={26},
  number={2},
  pages={809--847},
  year={2018},
  publisher={Springer},
  address={New York, USA},
  doi={https://doi.org/10.1007/s11219-016-9345-3},
}

@article{garvin1984_what_does_product_quality_really_mean,
  title={What does ``product quality” really mean?},
  author={Garvin, David A},
  organization = {Harvard University},
  journal={Sloan management review},
  volume={25},
  pages={19},
  year={1984}
}

@article{GAVIDIACALDERON2021_game_theoretic_analysis_of_software_development_practices,
    title = {Game-theoretic analysis of development practices: Challenges and opportunities},
    journal = {Journal of Systems and Software},
    volume = {159},
    pages = {110424},
    year = {2020},
    issn = {0164-1212},
    doi = {https://doi.org/10.1016/j.jss.2019.110424},
    url = {https://www.sciencedirect.com/science/article/pii/S0164121219301980},
    author = {Carlos Gavidia-Calderon and Federica Sarro and Mark Harman and Earl T. Barr},
    keywords = {Game theory, Empirical analysis, Technical debt, Software engineering practices},
    abstract = {
      Developers continuously invent new practices, usually grounded in hard-won experience, not theory. Game theory studies cooperation and conflict; its use will speed the development of effective processes. A survey of game theory in software engineering finds highly idealised models that are rarely based on process data. This is because software processes are hard to analyse using traditional game theory since they generate huge game models. We are the first to show how to use game abstractions, developed in artificial intelligence, to produce tractable game-theoretic models of software practices. We present Game-Theoretic Process Improvement (GTPI), built on top of empirical game-theoretic analysis. Some teams fall into the habit of preferring “quick-and-dirty” code to slow-to-write, careful code, incurring technical debt. We showcase GTPI’s ability to diagnose and improve such a development process. Using GTPI, we discover a lightweight intervention that incentivises developers to write careful code: add a singlecode reviewer who needs to catch only 25\% of kludges. This 25\% accuracy is key; it means that a reviewer does not need to examine each commit in depth, making this process intervention cost-effective.
  }
}

@article{gerlich2015app,
  title={App consumption: An exploratory analysis of the uses \& gratifications of mobile apps},
  author={Gerlich, R Nicholas and Drumheller, Kristina and Babb, Jeffry and De'Armond, De'Arno},
  journal={Academy of Marketing Studies Journal},
  volume={19},
  number={1},
  pages={69},
  year={2015},
  publisher={Jordan Whitney Enterprises, Inc}
}

@article{glez2014_web_scraping_in_an_API_world,
  title={Web scraping technologies in an API world},
  author={Glez-Pe{\~n}a, Daniel and Louren{\c{c}}o, An{\'a}lia and L{\'o}pez-Fern{\'a}ndez, Hugo and Reboiro-Jato, Miguel and Fdez-Riverola, Florentino},
  journal={Briefings in bioinformatics},
  volume={15},
  number={5},
  pages={788--797},
  year={2014},
  publisher={Oxford University Press}
}

@article{not_cited_yet_gonzalez2018_owncloud_android_app_in_review_heaven,
 author = {González, David},
 doi = {10.5446/42813},
 title = {ownCloud Android app in review heaven ...},
 url = {https://dx.doi.org/10.5446/42813},
 year = {2018},
 video = {https://av.tib.eu/media/42813},
 abstract = {
    	In this lightning talk, I will go through some keys to achieve better reviews in an Android app and how we have implemented them in the ownCloud Android app. How did the ownCloud Android app get from 2-3 to 4-5 stars ratings? This is the question we are going to answer in this lightning talk through the next points: - OwnCloud Android app reviews in the past and main reasons for the bad ones. - New process to receive better reviews, including some tips and real examples that can be useful for other apps. - How to monitor user reviews by using the Google Play Console and real numbers of the ownCloud Android app. https://cfp.owncloud.com/occon18/talk/BSGK7D
    	
    	In this lightning talk, I will go through some keys to achieve better reviews in an Android app and how we have implemented them in the ownCloud Android app.
    	How did the ownCloud Android app get from 2-3 to 4-5 stars ratings? This is the question we are going to answer in this lightning talk through the next points:
    	OwnCloud Android app reviews in the past and main reasons for the bad ones.
    	New process to receive better reviews, including some tips and real examples that can be useful for other apps.
    	How to monitor user reviews by using the Google Play Console and real numbers of the ownCloud Android app.
 },
 thoughts = {
    Their sourcecode is on github https://github.com/owncloud/android and their app on f-droid and Google Play https://play.google.com/store/apps/details?id=com.owncloud.android  Currently it has 4.0 stars overall on 12.6k reviews. It seems to be crashing a lot according to the promoted reviews (from around 2019 - why does Google show these!?). The devs ask end-users to send them the logs (tap the build number 5 times in the settings, etc.) I wonder if a) they have in app crash reporting, b) what Android Vitals reports. 
    
    Many of the current complaints are on poor functionality, they look like they'd help the dev team improve the testing of the app and the service e.g. to upload a mix of pre-existing and new content. To upload from several devices concurrently (interleaved), etc. 
 }
}



@ARTICLE{not_cited_yet_988583,  
  author={Grad, B.},
  journal={IEEE Annals of the History of Computing},  
  title={A personal recollection: IBM's unbundling of software and services}, 
  year={2002}, 
  volume={24},
  number={1}, 
  pages={64-71},
  abstract={Many people believe that one pivotal event in the growth of the business software products market was IBM's decision, in 1969, to price its software and services separately from its hardware. The author's recollections shed light on the internal process and surrounding business climate that led to IBM's decision.},  keywords={},
  doi={10.1109/85.988583}, 
  ISSN={1934-1547}, 
  month={Jan},
  quote = {("...besides the feeling that “no one got fired for buying IBM systems" p.64)},
  rationale = {How developers might choose their mobile analytics service},
}

@ARTICLE{greiler2022_an_actionable_framework_for_understanding_and_improving_developer_experience,  
    author={Greiler, Michaela and Storey, Margaret-Anne and Noda, Abi},
    journal={IEEE Transactions on Software Engineering},  
    title={An Actionable Framework for Understanding and Improving Developer Experience},  
    year={2022}, 
    volume={},  
    number={}, 
    pages={1-15},  
    abstract={
        Developer experience is an important concern for software organizations as enhancing developer experience improves productivity, satisfaction, engagement, and retention. We set out to understand what affects developer experience through semi-structured interviews with developers from industry, which we transcribed and iteratively coded. Our findings elucidate factors that affect developer experience and characteristics that influence their respective importance to individual developers. We also identify strategies employed by individuals and teams to improve developer experience and the barriers that stand in their way. Lastly, we describe the coping mechanisms of developers when developer experience cannot be sufficiently improved. Our findings result in the DX Framework, an actionable conceptual framework for understanding and improving developer experience. The DX Framework provides a go-to reference for organizations that want to enable more productive and effective work environments for their developers.
    },  
    keywords={developer experience, grounded theory, development practices, satisfaction, productivity},  
    doi={10.1109/TSE.2022.3175660}, 
    ISSN={1939-3520}, 
    month={},
}

@article{grigoreanu2012_end_user_debugging_strategies_a_sensemaking_perspective,
    author = {Grigoreanu, Valentina and Burnett, Margaret and Wiedenbeck, Susan and Cao, Jill and Rector, Kyle and Kwan, Irwin},
    title = {End-User Debugging Strategies: A Sensemaking Perspective},
    year = {2012},
    issue_date = {March 2012},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {19},
    number = {1},
    issn = {1073-0516},
    url = {https://doi.org/10.1145/2147783.2147788},
    doi = {10.1145/2147783.2147788},
    abstract = {
        Despite decades of research into how professional programmers debug, only recently has work emerged about how end-user programmers attempt to debug programs. Without this knowledge, we cannot build tools to adequately support their needs. This article reports the results of a detailed qualitative empirical study of end-user programmers' sensemaking about a spreadsheet's correctness. Using our study's data, we derived a sensemaking model for end-user debugging and categorized participants' activities and verbalizations according to this model, allowing us to investigate how participants went about debugging. Among the results are identification of the prevalence of information foraging during end-user debugging, two successful strategies for traversing the sensemaking model, potential ties to gender differences in the literature, sensemaking sequences leading to debugging progress, and sequences tied with troublesome points in the debugging process. The results also reveal new implications for the design of spreadsheet tools to support end-user programmers' sensemaking during debugging.
    },
    journal = {ACM Trans. Comput.-Hum. Interact.},
    month = may,
    articleno = {5},
    numpages = {28},
    keywords = {gender differences, sensemaking, end-user software engineering, debugging, gender HCI, spreadsheets, debugging strategies, End-user programming}
}

@article{hall2009systematic,
  title={A systematic review of theory use in studies investigating the motivations of software engineers},
  author={Hall, Tracy and Baddoo, Nathan and Beecham, Sarah and Robinson, Hugh and Sharp, Helen},
  journal={ACM Transactions on Software Engineering and Methodology (TOSEM)},
  volume={18},
  number={3},
  pages={1--29},
  year={2009},
  publisher={ACM New York, NY, USA}
}

@ARTICLE{yet_to_cite_hamill2009_common_trends_in_software_fault_and_failure_data,
  author={Hamill, Maggie and Goseva-Popstojanova, Katerina},
  journal={IEEE Transactions on Software Engineering},  
  title={Common Trends in Software Fault and Failure Data},  
  year={2009}, 
  volume={35}, 
  number={4}, 
  pages={484-496},
  abstract={
    The benefits of the analysis of software faults and failures have been widely recognized. However, detailed studies based on empirical data are rare. In this paper, we analyze the fault and failure data from two large, real-world case studies. Specifically, we explore: 1) the localization of faults that lead to individual software failures and 2) the distribution of different types of software faults. Our results show that individual failures are often caused by multiple faults spread throughout the system. This observation is important since it does not support several heuristics and assumptions used in the past. In addition, it clearly indicates that finding and fixing faults that lead to such software failures in large, complex systems are often difficult and challenging tasks despite the advances in software development. Our results also show that requirement faults, coding faults, and data problems are the three most common types of software faults. Furthermore, these results show that contrary to the popular belief, a significant percentage of failures are linked to late life cycle activities. Another important aspect of our work is that we conduct intra- and interproject comparisons, as well as comparisons with the findings from related studies. The consistency of several main trends across software systems in this paper and several related research efforts suggests that these trends are likely to be intrinsic characteristics of software faults and failures rather than project specific.},  
  keywords={}, 
  doi={10.1109/TSE.2009.3},
  ISSN={1939-3520}, 
  month={July},
}

@ARTICLE{hao2011_privacy_preserving_etc_with_data_dynamics,  
  author={Hao, Zhuo and Zhong, Sheng and Yu, Nenghai}, 
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={A Privacy-Preserving Remote Data Integrity Checking Protocol with Data Dynamics and Public Verifiability},
  year={2011}, 
  volume={23}, 
  number={9}, 
  pages={1432-1437},
  abstract={
  Remote data integrity checking is a crucial technology in cloud computing. Recently, many works focus on providing data dynamics and/or public verifiability to this type of protocols. Existing protocols can support both features with the help of a third-party auditor. In a previous work, Sebé et al. propose a remote data integrity checking protocol that supports data dynamics. In this paper, we adapt Sebé et al.'s protocol to support public verifiability. The proposed protocol supports public verifiability without help of a third-party auditor. In addition, the proposed protocol does not leak any private information to third-party verifiers. Through a formal analysis, we show the correctness and security of the protocol. After that, through theoretical analysis and experimental results, we demonstrate that the proposed protocol has a good performance.},  
  keywords={},  
  doi={10.1109/TKDE.2011.62}, 
  ISSN={1558-2191}, 
  month={Sep.},
}

@article{harris2016identifying,
  title={Identifying factors influencing consumers’ intent to install mobile applications},
  author={Harris, Mark A and Brookshire, Robert and Chin, Amita Goyal},
  journal={International Journal of Information Management},
  volume={36},
  number={3},
  pages={441--450},
  year={2016},
  publisher={Elsevier}
}

@article{hodgkin1976chance,
  title={Chance and design in electrophysiology: an informal account of certain experiments on nerve carried out between 1934 and 1952.},
  author={Hodgkin, Alan L},
  journal={The Journal of physiology},
  volume={263},
  number={1},
  pages={21},
  year={1976},
  doi={10.1113/jphysiol.1976.sp011620},
  publisher={PubMed Central},
  PMCID = {PMC1307686},
}


@inproceedings{crosby2002_roles_beacons_play_in_comprehension_etc,
  title={The Roles Beacons Play in Comprehension for Novice and Expert Programmers.},
  author={Crosby, Martha E and Scholtz, Jean and Wiedenbeck, Susan},
  year = {2002},
  volume = {},
  number = {},
  pages={5},
  journal = {PPIG 2002 - 14th Annual Workshop},
  booktitle = {Proceedings of the 12th Annual Workshop of the Psychology of Programming Interest Group, {PPIG} 2002, Brunel University, London, UK, Jun 18-21 2002},
  publisher = {{ppig.org}},
  url = {https://www.ppig.org/files/2002-PPIG-14th-crosby.pdf},
}

@inproceedings{harty2021_logging_practices_with_mobile_analytics,
  author={Harty, Julian and Zhang, Haonan and Wei, Lili and Pascarella, Luca and Aniche, Maurício and Shang, Weiyi}, 
  booktitle={2021 IEEE/ACM 8th International Conference on Mobile Software Engineering and Systems (MobileSoft)}, 
  title={Logging Practices with Mobile Analytics: An Empirical Study on Firebase}, 
  year={2021}, 
  volume={}, 
  number={}, 
  pages={56-60}, 
  abstract={Software logs are of great value in both industrial and open-source projects. Mobile analytics logging enables developers to collect logs remotely from their apps running on end user devices at the cost of recording and transmitting logs across the Internet to a centralised infrastructure.This paper makes a first step in characterising logging practices with a widely adopted mobile analytics logging library, namely Firebase Analytics. We provide an empirical evaluation of the use of Firebase Analytics in 57 open-source Android applications by studying the evolution of code-bases to understand: a) the needs-in-common that push practitioners to adopt logging practices on mobile devices, and b) the differences in the ways developers use local and remote logging.Our results indicate mobile analytics logs are less pervasive and less maintained than traditional logging code. Based on our analysis, we believe logging using mobile analytics is more user centered compared to traditional logging, where the latter is mainly used to record information for debugging purposes.}, 
  keywords={}, 
  doi={10.1109/MobileSoft52590.2021.00013}, 
  ISSN={}, 
  month={May},
  address = {Madrid, Spain},
  publisher = {IEEE},
}

@Article{yet_to_cite_Heumüller2020_publish_or_perish_but_do_not_forget_your_software_artefacts,
    author={Heum{\"u}ller, Robert
    and Nielebock, Sebastian
    and Kr{\"u}ger, Jacob
    and Ortmeier, Frank},
    title={Publish or perish, but do not forget your software artifacts},
    journal={Empirical Software Engineering},
    year={2020},
    month={Nov},
    day={01},
    volume={25},
    number={6},
    pages={4585-4616},
    abstract={Open-science initiatives have gained substantial momentum in computer science, and particularly in software-engineering research. A critical aspect of open-science is the public availability of artifacts (e.g., tools), which facilitates the replication, reproduction, extension, and verification of results. While we experienced that many artifacts are not publicly available, we are not aware of empirical evidence supporting this subjective claim. In this article, we report an empirical study on software artifact papers (SAPs) published at the International Conference on Software Engineering (ICSE), in which we investigated whether and how researchers have published their software artifacts, and whether this had scientific impact. Our dataset comprises 789 ICSE research track papers, including 604 SAPs (76.6 {\%}), from the years 2007 to 2017. While showing a positive trend towards artifact availability, our results are still sobering. Even in 2017, only 58.5 {\%} of the papers that stated to have developed a software artifact made that artifact publicly available. As we did find a small, but statistically significant, positive correlation between linking to artifacts in a paper and its scientific impact in terms of citations, we hope to motivate the research community to share more artifacts. With our insights, we aim to support the advancement of open science by discussing our results in the context of existing initiatives and guidelines. In particular, our findings advocate the need for clearly communicating artifacts and the use of non-commercial, persistent archives to provide replication packages.},
    issn={1573-7616},
    doi={10.1007/s10664-020-09851-6},
    url={https://doi.org/10.1007/s10664-020-09851-6}
}


@inproceedings{naumer2008_sense_making,
  title={Sense-Making: a methodological perspective},
  author={Naumer, Charles and Fisher, Karen and Dervin, Brenda},
  booktitle={Sensemaking Workshop, CHI},
  volume={8},
  year={2008},
  address = {Florence, Italy},
  publisher = {ACM},
  pages = {5},
}

@ARTICLE{9397392,
  author={Hort, Max and Kechagia, Maria and Sarro, Federica and Harman, Mark},
  journal={IEEE Transactions on Software Engineering},
  title={A Survey of Performance Optimization for Mobile Applications}, 
  year={2021}, 
  volume={}, 
  number={}, 
  pages={1-1}, 
  abstract={
    Nowadays there is a mobile application for almost everything a user may think of, ranging from paying bills and gathering information to playing games and watching movies. In order to ensure user satisfaction and success of applications, it is important to provide high performant applications. This is particularly important for resource constraint systems such as mobile devices. Thereby, non-functional performance characteristics, such as energy and memory consumption, play an important role for user satisfaction. This paper provides a comprehensive survey of non-functional performance optimization for Android applications. We collected 155 unique publications, published between 2008 and 2020, that focus on the optimization of non-functional performance of mobile applications. We target our search at four performance characteristics, in particular: responsiveness, launch time, memory and energy consumption. For each performance characteristic, we categorize optimization approaches based on the method used in the corresponding publications. Furthermore, we identify research gaps in the literature for future work.},
  keywords={}, 
  doi={10.1109/TSE.2021.3071193}, 
  ISSN={1939-3520},
  month={},
  remarks = {This cites one of my papers, I understand. Worth weaving into the literature review as a recent and credible source.}
}

@article{HSIAO2016342,
title = "Exploring the influential factors in continuance usage of mobile social Apps: Satisfaction, habit, and customer value perspectives",
journal = "Telematics and Informatics",
volume = "33",
number = "2",
pages = "342 - 355",
year = "2016",
issn = "0736-5853",
doi = "https://doi.org/10.1016/j.tele.2015.08.014",
url = "http://www.sciencedirect.com/science/article/pii/S0736585315001136",
author = "Chun-Hua Hsiao and Jung-Jung Chang and Kai-Yu Tang",
keywords = "Social Apps, Continuance intention, Satisfaction, Habit, Customer value perspectives",
abstract = "The emergence of mobile application software (App) has explosively grown in conjunction with the worldwide use of smartphones in recent years. Among numerous categories of mobile Apps, social Apps were one of those with the greatest growth in 2013. Despite abundant research on users’ behavior intention of mobile App usage, few studies have focused on investigating key determinants of users’ continuance intention regarding social Apps. To fill this gap, we integrated customer value perspectives to explore the influential factors in the continuance intention of social App use. Moreover, users’ satisfaction and habit from both the marketing and psychology literature were also incorporated into the research model. A total of 378 valid questionnaires were collected by survey method, and structural equation modeling was employed in the subsequent data analysis. The results indicate that the continuance usage of social Apps is driven by users’ satisfaction, tight connection with others, and hedonic motivation to use the Apps. In addition, full mediation effects of satisfaction and habit were found between perceived usefulness and intention to continue use. These findings extend our understanding of users’ continuance intention in the context of social Apps. Discussion and implications are provided."
}

@article{humble2018_continuous_delivery_sounds_great,
    author = {Humble, Jez},
    title = {Continuous Delivery Sounds Great, but Will It Work Here?},
    year = {2018},
    issue_date = {April 2018},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {61},
    number = {4},
    issn = {0001-0782},
    url = {https://doi.org/10.1145/3173553},
    doi = {10.1145/3173553},
    abstract = {It's not magic, it just requires continuous, daily improvement at all levels.},
    journal = {Commun. ACM},
    month = mar,
    pages = {34–39},
    numpages = {6}
}

@article{not_cited_yet_10.1145/2076450.2076452,
    author = {Ian Joyner},
    title = {The Jobs Factor},
    year = {2012},
    issue_date = {February 2012},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {55},
    number = {2},
    issn = {0001-0782},
    url = {https://doi.org/10.1145/2076450.2076452},
    doi = {10.1145/2076450.2076452},
    journal = {Commun. ACM},
    month = feb,
    pages = {6–7},
    numpages = {2},
    quote = {Microsoft platform dominance was a legacy of the “IBM factor” that said: “Nobody ever got fired for buying IBM.” p.6},
    rationale = {On how developers select their mobile analytics platform, rather than evaluating the options},
}


@article{KAPOOR2021_socio_technical_platform_ecosystems_etc,
	Abstract = {Business models are becoming more inclined towards platforms, which allow inclusion of diverse participants to promote leveraged growth and modularity of offerings. Despite being closely linked, several aspects of platforms are often studied exclusively from their ecosystems, lacking integrative insights on the topic of platform ecosystems. Most studies are tunnel focussed on the technical aspects, failing to account for the social factors that play a critical role within platform firms. In addressing this gap, the study aims to review research on both social and technical aspects of platform ecosystems to account for the complex interdependencies stemming from platform-oriented actor interactions. The study extends beyond a typical literature review approach to also include a theoretically grounded, yet practically relevant framework of socio-technical systems to offer a holistic review of literature on how platform ecosystems function and sustain in competitive environments.},
	Author = {Kawaljeet Kapoor and Ali {Ziaee Bigdeli} and Yogesh K. Dwivedi and Andreas Schroeder and Ahmad Beltagui and Tim Baines},
	doi = {https://doi.org/10.1016/j.jbusres.2021.01.060},
	issn = {0148-2963},
	Journal = {Journal of Business Research},
	Keywords = {Platforms, Platform ecosystems, Socio-technical systems},
	Pages = {94-108},
	Title = {A socio-technical view of platform ecosystems: Systematic review and research agenda},
	url = {https://www.sciencedirect.com/science/article/pii/S0148296321000680},
	Volume = {128},
	Year = {2021},
	Bdsk-Url-1 = {https://www.sciencedirect.com/science/article/pii/S0148296321000680},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.jbusres.2021.01.060}
}


@article{Kechagia2015_charting_API_minefield_using_telemetry_data,
    author={Kechagia, Maria
    and Mitropoulos, Dimitris
    and Spinellis, Diomidis},
    title={Charting the API minefield using software telemetry data},
    journal={Empirical Software Engineering},
    year={2015},
    month={Dec},
    day={01},
    volume={20},
    number={6},
    pages={1785-1830},
    abstract={Programs draw significant parts of their functionality through the use of Application Programming Interfaces (APIs). Apart from the way developers incorporate APIs in their software, the stability of these programs depends on the design and implementation of the APIs. In this work, we report how we used software telemetry data to analyze the causes of API failures in Android applications. Specifically, we got 4.9 gb worth of crash data that thousands of applications sent to a centralized crash report management service. We processed that data to extract approximately a million stack traces, stitching together parts of chained exceptions, and established heuristic rules to draw the border between applications and the API calls. We examined a set of more than a half million stack traces associated with risky API calls to map the space of the most common application failure reasons. Our findings show that the top ones can be attributed to memory exhaustion, race conditions or deadlocks, and missing or corrupt resources. Given the classes of the crash causes we identified, we recommend API design and implementation choices, such as specific exceptions, default resources, and non-blocking algorithms, that can eliminate common failures. In addition, we argue that development tools like memory analyzers, thread debuggers, and static analyzers can prevent crashes through early code testing and analysis. Finally, some execution platform and framework designs for process and memory management can also eliminate some application crashes.},
    issn={1573-7616},
    doi={10.1007/s10664-014-9343-7},
    url={https://doi.org/10.1007/s10664-014-9343-7}
}

@article{khalid2016_examining_the_relationship_between_findbugs_warnings_and_app_ratings,  
  author={H. {Khalid} and M. {Nagappan} and A. E. {Hassan}},  
  journal={IEEE Software},
  title={Examining the Relationship between FindBugs Warnings and App Ratings}, 
  year={2016},
  volume={33},
  number={4},
  pages={34-39},
  abstract={
    In the mobile-app ecosystem, user ratings of apps (a measure of user perception) are extremely important because they correlate strongly with downloads and hence revenue. A case study examined the relationship between ratings (and the associated review comments) and static-analysis warnings (collected using FindBugs) for 10,000 free-to-download Android apps. Three warning categories - bad practice, internationalization, and performance - were more frequent in low-rated apps and corresponded to the review comment complaints. Thus, these categories were closely related to the user experience. These results suggest that app developers could use static-analysis tools to identify the bugs behind the issues that users complain about, before releasing an app.},
  keywords={mobile computing;program diagnostics;FindBugs warnings;app ratings;mobile-app ecosystem;static-analysis warnings;free-to-download Android apps;warning categories;low-rated apps;user experience;static-analysis tools;Ecosystems;Androids;Humanoid robots;Computer bugs;Software development;Mobile communication;Computer applications;mobile apps;static analysis;user ratings;software quality assurance;FindBugs;Android;software engineering;software development}, doi={10.1109/MS.2015.29},
  ISSN={1937-4194},
  month={July},
}

@ARTICLE{khalid2015_what_do_mobile_app_users_complain_about,  
  author={H. {Khalid} and E. {Shihab} and M. {Nagappan} and A. E. {Hassan}},
  journal={IEEE Software},
  title={What Do Mobile App Users Complain About?},
  year={2015},
  volume={32},
  number={3},
  pages={70-77},
  abstract={
    Mobile-app quality is becoming an increasingly important issue. These apps are generally delivered through app stores that let users post reviews. These reviews provide a rich data source you can leverage to understand user-reported issues. Researchers qualitatively studied 6,390 low-rated user reviews for 20 free-to-download iOS apps. They uncovered 12 types of user complaints. The most frequent complaints were functional errors, feature requests, and app crashes. Complaints about privacy and ethical issues and hidden app costs most negatively affected ratings. In 11 percent of the reviews, users attributed their complaints to a recent app update. This study provides insight into the user-reported issues of iOS apps, along with their frequency and impact, which can help developers better prioritize their limited quality assurance resources.},  
  keywords={mobile computing;operating systems (computers);mobile App users;mobile app quality;app stores;iOS apps;user complaints;frequent complaints;functional errors;feature requests;app crashes;ethical issues;privacy issues;quality assurance resources;Computer crashes;Privacy;Mobile communication;Tagging;Computer applications;Software quality;Quality assurance;Software engineering;mobile applications;software quality;user reviews;quality assurance;software engineering},
  doi={10.1109/MS.2014.50},
  ISSN={1937-4194},
  month={May},
}

@article{kinshuman2011_debugging_in_the_very_large,
    author = {Kinshumann, Kinshuman and Glerum, Kirk and Greenberg, Steve and Aul, Gabriel and Orgovan, Vince and Nichols, Greg and Grant, David and Loihle, Gretchen and Hunt, Galen},
    title = {Debugging in the (Very) Large: Ten Years of Implementation and Experience},
    year = {2011},
    issue_date = {July 2011},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {54},
    number = {7},
    issn = {0001-0782},
    url = {https://doi.org/10.1145/1965724.1965749},
    doi = {10.1145/1965724.1965749},
    abstract = {Windows Error Reporting (WER) is a distributed system that automates the processing
    of error reports coming from an installed base of a billion machines. WER has collected
    billions of error reports in 10 years of operation. It collects error data automatically
    and classifies errors into buckets, which are used to prioritize developer effort
    and report fixes to users. WER uses a progressive approach to data collection, which
    minimizes overhead for most reports yet allows developers to collect detailed information
    when needed. WER takes advantage of its scale to use error statistics as a tool in
    debugging; this allows developers to isolate bugs that cannot be found at smaller
    scale. WER has been designed for efficient operation at large scale: one pair of database
    servers records all the errors that occur on all Windows computers worldwide.},
    journal = {Commun. ACM},
    month = jul,
    pages = {111–116},
    numpages = {6},
    remarks = {This is the newer version of their paper on this topic and easier to read owning to better formatting and layout of the content.}
}

@article{kitchenham1996_software_quality_elusive_target,
  title={Software quality: the elusive target [special issues section]},
  author={Kitchenham, Barbara and Pfleeger, Shari Lawrence},
  journal={IEEE software},
  volume={13},
  number={1},
  pages={12--21},
  year={1996},
  publisher={IEEE},
  abstract={If you are a software developer, manager, or maintainer, quality is often on your mind. But what do you really mean by software quality? Is your definition adequate? Is the software you produce better or worse than you would like it to be? We put software quality on trial, examining both the definition and evaluation of our software products and processes.},
}

@Article{Ko2015_a_practical_guide_to_controlled_experiments_of_sw_eng_tools_with_human_participants,
    author={Ko, Amy J.
    and LaToza, Thomas D.
    and Burnett, Margaret M.},
    title={A practical guide to controlled experiments of software engineering tools with human participants},
    journal={Empirical Software Engineering},
    year={2015},
    month={Feb},
    day={01},
    volume={20},
    number={1},
    pages={110-141},
    abstract={Empirical studies, often in the form of controlled experiments, have been widely adopted in software engineering research as a way to evaluate the merits of new software engineering tools. However, controlled experiments involving human participants actually using new tools are still rare, and when they are conducted, some have serious validity concerns. Recent research has also shown that many software engineering researchers view this form of tool evaluation as too risky and too difficult to conduct, as they might ultimately lead to inconclusive or negative results. In this paper, we aim both to help researchers minimize the risks of this form of tool evaluation, and to increase their quality, by offering practical methodological guidance on designing and running controlled experiments with developers. Our guidance fills gaps in the empirical literature by explaining, from a practical perspective, options in the recruitment and selection of human participants, informed consent, experimental procedures, demographic measurements, group assignment, training, the selecting and design of tasks, the measurement of common outcome variables such as success and time on task, and study debriefing. Throughout, we situate this guidance in the results of a new systematic review of the tool evaluations that were published in over 1,700 software engineering papers published from 2001 to 2011.},
    issn={1573-7616},
    doi={10.1007/s10664-013-9279-3},
    url={https://doi.org/10.1007/s10664-013-9279-3}
}



@article{yet_to_cite_Kong2021_anchor_locating_android_framework_specific_crashing_faults,
author={Kong, Pingfan
and Li, Li
and Gao, Jun
and Riom, Timoth{\'e}e
and Zhao, Yanjie
and Bissyand{\'e}, Tegawend{\'e} F.
and Klein, Jacques},
title={ANCHOR: locating android framework-specific crashing faults},
journal={Automated Software Engineering},
year={2021},
month={Jul},
day={12},
volume={28},
number={2},
pages={10},
abstract={Android framework-specific app crashes are hard to debug. Indeed, the callback-based event-driven mechanism of Android challenges crash localization techniques that are developed for traditional Java programs. The key challenge stems from the fact that the buggy code location may not even be listed within the stack trace. For example, our empirical study on 500 framework-specific crashes from an open benchmark has revealed that 37 percent of the crash types are related to bugs that are outside the stack traces. Moreover, Android programs are a mixture of code and extra-code artifacts such as the Manifest file. The fact that any artifact can lead to failures in the app execution creates the need to position the localization target beyond the code realm. In this paper, we propose Anchor , a two-phase suspicious bug location suggestion tool. Anchor specializes in finding crash-inducing bugs outside the stack trace. Anchor is lightweight and source code independent since it only requires the crash message and the apk file to locate the fault. Experimental results, collected via cross-validation and in-the-wild dataset evaluation, show that Anchor is effective in locating Android framework-specific crashing faults. Finally, we put our empirical study results openly accessible at https://github.com/anchor-locator/anchor.},
issn={1573-7535},
doi={10.1007/s10515-021-00290-1},
url={https://doi.org/10.1007/s10515-021-00290-1}
}

@article{krotov2020_tutorial_legality_and_ethics_of_web_scraping,
  title = {Tutorial: Legality and Ethics of Web Scraping},
  url = {https://aisel.aisnet.org/cais/vol47/iss1/22/},
  source = {https://digitalcommons.murraystate.edu/faculty/86/},
  year = {2020},
  author = {Krotov, Vlad and Johnson, Leigh and Silva, Leiser},
  journal = {Communications of the Association for Information Systems}, 
  volume = {47}, 
  pages = {555--581},
  doi = {https://doi.org/10.17705/1CAIS.04724},
  abstract = {
    Automatic retrieval of data from the Web (often referred to as Web Scraping) for industry and academic research projects is becoming a common practice. A variety of tools and technologies have been developed to facilitate Web Scraping. Unfortunately, the legality and ethics of using these tools for collecting data are often overlooked. Failure to pay due attention to these aspects of Web Scraping can result in serious ethical controversies and lawsuits. This paper reviews legal literature together with the literature on ethics and privacy to identify broad areas of concern together with a list of specific questions that need to be addressed by researchers and practitioners engaged in Web Scraping. Reflecting on these questions and concerns can potentially help the researchers decrease the likelihood of ethical and legal controversies in their work.
  },
}

@article{lee2014_determinants_of_mobile_app_success_evidence_from_the_app_store_market,
    author = { Gunwoong Lee  and T. S. Raghu },
    title = {Determinants of Mobile Apps' Success: Evidence from the App Store Market},
    journal = {Journal of Management Information Systems},
    volume = {31},
    number = {2},
    pages = {133-170},
    year  = {2014},
    publisher = {Routledge},
    doi = {10.2753/MIS0742-1222310206},
    url = {https://doi.org/10.2753/MIS0742-1222310206},
    eprint = {https://doi.org/10.2753/MIS0742-1222310206},
    abstract = {
      Mobile applications markets with app stores have introduced a new approach to define and sell software applications with access to a large body of heterogeneous consumer population. This research examines key seller- and app-level characteristics that impact success in an app store market. We tracked individual apps and their presence in the top-grossing 300 chart in Apple's App Store and examined how factors at different levels affect the apps' survival in the top 300 chart. We used a generalized hierarchical modeling approach to measure sales performance, and confirmed the results with the use of a hazard model and a count regression model. We find that broadening app offerings across multiple categories is a key determinant that contributes to a higher probability of survival in the top charts. App-level attributes such as free app offers, high initial ranks, investment in less-popular (less-competitive) categories, continuous quality updates, and high-volume and high-user review scores have positive effects on apps' sustainability. In general, each diversification decision across a category results in an approximately 15 percent increase in the presence of an app in the top charts. Survival rates for free apps are up to two times more than that for paid apps. Quality (feature) updates to apps can contribute up to a threefold improvement in survival rate as well. A key implication of the results of this study is that sellers must utilize the natural segmentation in consumer tastes offered by the different categories to improve sales performance.},
    related = {lotan2015_apple_apps_and_algorithmic_glitches},    
}

@ARTICLE{lim_investigating_country_differences,
  author={S. L. {Lim} and P. J. {Bentley} and N. {Kanakam} and F. {Ishikawa} and S. {Honiden}},  
  journal={IEEE Transactions on Software Engineering},   
  title={Investigating Country Differences in Mobile App User Behavior and Challenges for Software Engineering},   
  year={2015},  
  volume={41},  
  number={1},  
  pages={40-64},  
  abstract={Mobile applications (apps) are software developed for use on mobile devices and made available through app stores. App stores are highly competitive markets where developers need to cater to a large number of users spanning multiple countries. This work hypothesizes that there exist country differences in mobile app user behavior and conducts one of the largest surveys to date of app users across the world, in order to identify the precise nature of those differences. The survey investigated user adoption of the app store concept, app needs, and rationale for selecting or abandoning an app. We collected data from more than 15 countries, including USA, China, Japan, Germany, France, Brazil, United Kingdom, Italy, Russia, India, Canada, Spain, Australia, Mexico, and South Korea. Analysis of data provided by 4,824 participants showed significant differences in app user behaviors across countries, for example users from USA are more likely to download medical apps, users from the United Kingdom and Canada are more likely to be influenced by price, users from Japan and Australia are less likely to rate apps. Analysis of the results revealed new challenges to market-driven software engineering related to packaging requirements, feature space, quality expectations, app store dependency, price sensitivity, and ecosystem effect.},  keywords={consumer behaviour;mobile computing;smart phones;software engineering;market-driven software engineering;medical applications;data analysis;South Korea;South;Mexico;Australia;Spain;Canada;India;Russia;Italy;United Kingdom;Brazil;France;Germany;Japan;China;USA;applications stores;mobile devices;user behavior;mobile application;Mobile communication;Software;Smart phones;Software engineering;Data mining;Educational institutions;Requirements/specifications;market-driven software engineering;mobile application development;user requirements;survey research;app user behavior;software product lines;software ecosystems;Requirements/specifications;market-driven software engineering;mobile application development;user requirements;survey research;app user behavior;software product lines;software ecosystems},  
  doi={10.1109/TSE.2014.2360674},  
  ISSN={1939-3520},  
  month={Jan},
}

@article{lin2013_scaling_big_data_mining_infrastructure_the_twitter_experience,
author = {Lin, Jimmy and Ryaboy, Dmitriy},
title = {Scaling Big Data Mining Infrastructure: The Twitter Experience},
year = {2013},
issue_date = {December 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1931-0145},
url = {https://doi.org/10.1145/2481244.2481247},
doi = {10.1145/2481244.2481247},
abstract = {The analytics platform at Twitter has experienced tremendous growth over the past few years in terms of size, complexity, number of users, and variety of use cases. In this paper, we discuss the evolution of our infrastructure and the development of capabilities for data mining on "big data". One important lesson is that successful big data mining in practice is about much more than what most academics would consider data mining: life "in the trenches" is occupied by much preparatory work that precedes the application of data mining algorithms and followed by substantial effort to turn preliminary models into robust solutions. In this context, we discuss two topics: First, schemas play an important role in helping data scientists understand petabyte-scale data stores, but they're insufficient to provide an overall "big picture" of the data available to generate insights. Second, we observe that a major challenge in building data analytics platforms stems from the heterogeneity of the various components that must be integrated together into production workflows---we refer to this as "plumbing". This paper has two goals: For practitioners, we hope to share our experiences to flatten bumps in the road for those who come after us. For academic researchers, we hope to provide a broader context for data mining in production environments, pointing out opportunities for future work.},
journal = {SIGKDD Explor. Newsl.},
month = {apr},
pages = {6–19},
numpages = {14}
}

@ARTICLE{liu2020_privacy_risk_analysis_and_mitigation_of_analytics_libraries_in_the_android_ecosystem,  
    author={Liu, Xing and Liu, Jiqiang and Zhu, Sencun and Wang, Wei and Zhang, Xiangliang},  
    journal={IEEE Transactions on Mobile Computing},  
    title={Privacy Risk Analysis and Mitigation of Analytics Libraries in the Android Ecosystem}, 
    year={2020},  
    volume={19},  
    number={5}, 
    pages={1184-1199},  
    abstract={
        While much effort has been made to detect and measure the privacy leakage caused by the advertising (ad) libraries integrated in mobile applications, analytics libraries, which are also widely used in mobile apps have not been systematically studied for their privacy risks. Different from ad libraries, the main function of analytics libraries is to collect users' in-app actions. Hence, by design analytics libraries are more likely to leak users' private information. In this work, we study what information is collected by the analytics libraries integrated in popular Android apps. We design and implement a framework called “Alde”. Given an app, Alde employs both static analysis and dynamic analysis to detect the users' in-app actions collected by analytics libraries. We also study what private information can be leaked by the apps that use the same analytics library. Moreover, we analyze apps' privacy policies to see whether app developers have notified the users that their in-app action data is collected by analytics libraries. Finally, we select eight widely used analytics libraries to study and apply our method to 300 popular apps downloaded from both Chinese app markets and Google play. Our experimental results show that some apps indeed leak users' personal information through analytics libraries even though their genuine purposes of using analytics services are legal. To mitigate such threats, we have developed an app named “ALManager” that leverages the Xposed framework to manage analytics libraries in other apps.
    },  
    keywords={}, 
    doi={10.1109/TMC.2019.2903186}, 
    ISSN={1558-0660},  
    month={May},
}

@article{lo2018_novel_multi_criteria_decision_making_based_FMEA_model_for_risk_assessment,
    title = {A novel multiple-criteria decision-making-based FMEA model for risk assessment},
    journal = {Applied Soft Computing},
    volume = {73},
    pages = {684-696},
    year = {2018},
    issn = {1568-4946},
    doi = {https://doi.org/10.1016/j.asoc.2018.09.020},
    url = {https://www.sciencedirect.com/science/article/pii/S1568494618305374},
    author = {Huai-Wei Lo and James J.H. Liou},
    keywords = {MCDM, BWM, FMEA, Risk assessment, Grey relational analysis},
    abstract = {
      Failure mode and effect analysis (FMEA) is a forward-looking risk-management technique used in various industries for promoting the reliability and safety of products, processes, structures, systems, and services. However, FMEA has many defects in practical experimentation. Therefore, this paper proposes a new model that uses multiple-criteria decision-making in combination with grey theory for FMEA. This approach has several advantages, such as being able to add the expected cost into the original risk priority number (RPN) to reflect the actual resource limitations, consider the different weights of severity, occurrence, detectability, and cost based on the best–worst method in RPN element calculation, and use the grey interval linguistic variables to manage information uncertainty. Furthermore, this study applied probability-based grey relational analysis to calculate the RPN, which preserves the information of prioritized failure modes through interval analysis. To demonstrate the usefulness and effectiveness of the proposed model, real data from an international electronics company were applied. The proposed model can provide an alternative risk priority solution for product development.
    }
}

@ARTICLE{lopez2021_bumps_in_the_code_error_handling_during_software_development,  
  author={Lopez, Tamara and Sharp, Helen and Petre, Marian and Nuseibeh, Bashar}, 
  journal={IEEE Software}, 
  title={Bumps in the Code: Error Handling During Software Development}, 
  year={2021}, 
  volume={38}, 
  number={3}, 
  pages={26-34}, 
  abstract={
    Problems come up during software development all the time. When developers hit these bumps, they must figure out what has gone wrong. Findings from three studies suggest that the way developers handle errors contributes to professional growth.},
  keywords={}, 
  doi={10.1109/MS.2020.3024981}, 
  ISSN={1937-4194},  
  month={May},
}

@article{luca2016study,
  title={A study on quality analysis measuring process},
  author={Luca, Liliana},
  journal={Revista Durabilitate si fiabilitate},
  number={2},
  pages={68--72},
  year={2016},
  quote_1 = {An important issue in the measurement process is optimal accuracy. Thus, in practice measurements should be considered the necessary and sufficient optimal precision. A much greater precision imposed in the measuring operation, causes great expense undue operator training and the means of measurement with an insufficient low accuracy, causing a low quality of measurement results.},
  quote_2 = {In [13] it is shown that obtaining a correct [Ishikawa] diagram is possible only through working in a team with experience},
  observations = {This paper's domain is in physical engineering, not software. There are some formatting and content issues in the paper online, it's not clear whether these are because of processing errors in making the paper available. The 10 references to the paper are all from physical engineering domains.},
}

@article{lulu2003_analytically_unobservable_failure_events,
    author = { Menberu   Lulu  and  Saeed   Maghsoodloo },
    title = {Analytically Unobservable Failure Events},
    journal = {Quality Engineering},
    volume = {16},
    number = {2},
    pages = {283-288},
    year  = {2003},
    publisher = {Taylor & Francis},
    doi = {10.1081/QEN-120024017},
    URL = { 
            https://doi.org/10.1081/QEN-120024017
    },
    eprint = { 
            https://doi.org/10.1081/QEN-120024017
    }
    ,
    abstract = {
      Abstract Based on computational approximations, product designers routinely simplify algebraic expressions of complex transfer functions, which, in turn, simplify post‐design analysis. This practice is justified because both the simplified and actual transfer functions yield approximately equal average performance values. However, utilizing the design of a switching regulator circuit, it is shown that such simplified expressions underestimate output variation. The discrepancy between the actual and underestimated output variation spans an analytically unobservable failure event. Since the observable portion constitutes a partial characterization of product unreliability and quality problems, only a partial solution is possible at the design stage. This severely limits the effectiveness of concurrent product design, reliability, and quality assurance methodologies that are based on variation modeling and analysis.
    }
}

@article{mantyla2014_how_are_software_defects_found,
  title={How are software defects found? The role of implicit defect detection, individual responsibility, documents, and knowledge},
  author={M{\"a}ntyl{\"a}, Mika V and Itkonen, Juha},
  journal={Information and Software Technology},
  volume={56},
  number={12},
  pages={1597--1612},
  year={2014},
  publisher={Elsevier}
}

@article{martin2016survey,
  title={A survey of app store analysis for software engineering},
  author={Martin, William and Sarro, Federica and Jia, Yue and Zhang, Yuanyuan and Harman, Mark},
  journal={IEEE transactions on software engineering},
  volume={43},
  number={9},
  pages={817--847},
  year={2016},
  publisher={IEEE}
}

@ARTICLE{martin2017_survey_in_app_store_analysis_for_software_engineering_IEEE_edition,  
  author={W. {Martin} and F. {Sarro} and Y. {Jia} and Y. {Zhang} and M. {Harman}},  
  journal={IEEE Transactions on Software Engineering},
  title={A Survey of App Store Analysis for Software Engineering},
  year={2017},
  volume={43},
  number={9},
  pages={817-847},
  abstract={
    App Store Analysis studies information about applications obtained from app stores. App stores provide a wealth of information derived from users that would not exist had the applications been distributed via previous software deployment methods. App Store Analysis combines this non-technical information with technical information to learn trends and behaviours within these forms of software repositories. Findings from App Store Analysis have a direct and actionable impact on the software teams that develop software for app stores, and have led to techniques for requirements engineering, release planning, software design, security and testing. This survey describes and compares the areas of research that have been explored thus far, drawing out common aspects, trends and directions future research should take to address open problems and challenges.},
  keywords={Software;Security;Software engineering;Market research;Ecosystems;Mobile communication;Google;App store;analysis;mining;API;feature;release planning;requirements engineering;reviews;security;ecosystem},
  doi={10.1109/TSE.2016.2630689},
  ISSN={1939-3520},
  month={Sep.},
}


@ARTICLE{martinez_fernandez2019_continuously_assessing_and_improving_software_quality_with_software_analytics_tools,  
  author={S. {Martínez-Fernández} and A. M. {Vollmer} and A. {Jedlitschka} and X. {Franch} and L. {López} and P. {Ram} and P. {Rodríguez} and S. {Aaramaa} and A. {Bagnato} and M. {Choraś} and J. {Partanen}},  
  journal={IEEE Access},   
  title={Continuously Assessing and Improving Software Quality With Software Analytics Tools: A Case Study},   
  year={2019},  
  volume={7},  
  number={},  
  pages={68219-68239},  
  abstract={In the last decade, modern data analytics technologies have enabled the creation of software analytics tools offering real-time visualization of various aspects related to software development and usage. These tools seem to be particularly attractive for companies doing agile software development. However, the information provided by the available tools is neither aggregated nor connected to higher quality goals. At the same time, assessing and improving the software quality has also been the key targets for the software engineering community, yielding several proposals for standards and software quality models. Integrating such quality models into software analytics tools could close the gap by providing the connection to higher quality goals. This paper aims at understanding whether the integration of quality models into software analytics tools provides understandable, reliable, useful, and relevant information at the right level of detail about the quality of a process or product and whether practitioners intend to use it. Over the course of more than a year, four companies involved in this case study deployed such a tool to assess and improve software quality in several projects. We used standardized measurement instruments to elicit the perception of 22 practitioners regarding their use of the tool. We complemented the findings with debriefing sessions held at the companies. In addition, we discussed challenges and lessons learned with four practitioners leading the use of the tool. The quantitative and qualitative analyses provided positive results, i.e., the practitioners' perception with regard to the tool's understandability, reliability, usefulness, and relevance was positive. Individual statements support the statistical findings, and constructive feedback can be used for future improvements. We conclude that the potential for future adoption of quality models within software analytics tools definitely exists and encourage other practitioners to use the presented seven challenges and seven lessons learned and adopt them in their companies.},  
  keywords={data analysis;data visualisation;software prototyping;software quality;software reliability;software tools;software analytics tools;agile software development;software quality;software engineering;data analytics technologies;tool understandability;tool reliability;tool usefulness;tool relevance;real-time visualization;Tools;Software quality;Companies;Real-time systems;Monitoring;Agile software development;case study;quality model;software analytics;software analytics tool;software quality},  
  doi={10.1109/ACCESS.2019.2917403},  
  ISSN={2169-3536},  
  month={},
}

@article{maslow1943_a_dynamic_theory_of_human_motivation,
  title={A Dynamic Theory of Human Motivation.},
  author={Maslow, Abraham Harold},
  volume = {50},
  issue = {4},
  year = {1943},
  journal = {Psychological Review},
  pages = {370--396},
  url = {https://psycnet.apa.org/record/1943-03751-001},
  remarks = {
    This bibliographic entry was manually crafted from various sources as it was hard to find a clear, trustworthy reference to the original paper and the internet is awash with later reprints and copies.
  }
}

@article{GoisMateus2019_an_empirical_study_on_the_quality_of_android_apps_in_kotlin,
  author="G{\'o}is Mateus, Bruno and Martinez, Matias",
  title="An empirical study on quality of Android applications written in Kotlin language",
  journal="Empirical Software Engineering",
  year="2019",
  month="Jun",
  day="25",
  abstract="During the last years, developers of mobile applications have the possibility to use new paradigms and tools for developing mobile applications. For instance, since 2017, Android developers have the official support to write Android applications using Kotlin language. Kotlin is programming language fully interoperable with Java that combines object-oriented and functional features.",
  issn="1573-7616",
  doi="10.1007/s10664-019-09727-4",
  url="https://doi.org/10.1007/s10664-019-09727-4",
  volume = {24},
  issue = {6},
  pages = {3356-3393},
}

@ARTICLE{gomez2017_app_store_2.0_from_crowdsourced_info_to_actionable_feedback_in_mobile_ecosystems,  
    author={Gómez, María and Adams, Bram and Maalej, Walid and Monperrus, Martin and Rouvoy, Romain},
    journal={IEEE Software},
    title={App Store 2.0: From Crowdsourced Information to Actionable Feedback in Mobile Ecosystems},
    year={2017},  
    volume={34},
    number={2},  
    pages={81-89},  
    abstract={
        Given the increasing competition in mobile-app ecosystems, improving the user experience has become a major goal for app vendors. App Store 2.0 will exploit crowdsourced information about apps, devices, and users to increase the overall quality of the delivered mobile apps. App Store 2.0 generates different kinds of actionable feedback from the crowd information. This feedback helps developers deal with potential errors that could affect their apps before publication or even when the apps are in the users' hands. The App Store 2.0 vision has been transformed into a concrete implementation for Android devices. This article is part of a special issue on Crowdsourcing for Software Engineering.
    },  
    keywords={},  
    doi={10.1109/MS.2017.46},  
    ISSN={1937-4194},  
    month={Mar},
    thoughts = {
      This probably belongs in the related works chapter to help set the context. Google Play already provides (and provided) aspects of what the authors are proposing.
    }
}

@ARTICLE{maalej2016_towards_data_driven_requirements_engineering,
  author={W. {Maalej} and M. {Nayebi} and T. {Johann} and G. {Ruhe}},
  journal={IEEE Software},
  title={Toward Data-Driven Requirements Engineering},
  year={2016},
  volume={33},
  number={1},
  pages={48-54},
  abstract={
    Nowadays, users can easily submit feedback about software products in app stores, social media, or user groups. Moreover, software vendors are collecting massive amounts of implicit feedback in the form of usage data, error logs, and sensor data. These trends suggest a shift toward data-driven user-centered identification, prioritization, and management of software requirements. Developers should be able to adopt the requirements of masses of users when deciding what to develop and when to release. They could systematically use explicit and implicit user data in an aggregated form to support requirements decisions. The goal is data-driven requirements engineering by the masses and for the masses.},
  keywords={Requirements engineering;Software engineering;Stakeholders;Media;Feature extraction;Market research;app reviews;decision support;requirements engineering;software analytics;usage data;software engineering;software development},
  doi={10.1109/MS.2015.153},
  ISSN={1937-4194},
  month={Jan},
}

@article{marsh2002skin,
  title={A skin not a sweater: Ontology and epistemology in political science},
  author={Marsh, David and Furlong, Paul},
  journal={Theory and methods in political science},
  volume={2},
  pages={17--41},
  year={2002},
  publisher={Palgrave Basingstoke}
}

@article{mcilroy2016_analyzing_and_automatically_labelling_the_types_of_user_issues_raised_in_mobile_app_reviews,
  title={Analyzing and automatically labelling the types of user issues that are raised in mobile app reviews},
  author={McIlroy, Stuart and Ali, Nasir and Khalid, Hammad and Hassan, Ahmed E},
  journal={Empirical Software Engineering},
  volume={21},
  number={3},
  pages={1067--1106},
  year={2016},
  publisher={Springer}
}

@article{menasria2018_purpose_driven_privacy_preservation_accelerometers,
  title={The purpose driven privacy preservation for accelerometer-based activity recognition},
  author={Menasria, Soumia and Wang, Jianxin and Lu, Mingming},
  journal={World Wide Web},
  volume={21},
  number={6},
  pages={1773--1785},
  year={2018},
  publisher={Springer},
  doi={10.1007/s11280-018-0604-z},
  abstract = {Accelerometer-based activity recognition (AAR) attracted a lot of attentions due to the wide spread of smartphones with energy-efficiency. However, since accelerometer data contains individual characteristics; AAR might raise privacy concerns. Although numerous privacy preservation approaches, such as ”privacy filtering, differential privacy, and inferential privacy”, have been proposed to conceal sensitive information, unfortunately they cannot address the privacy problem associated with AAR. In this paper, we report our efforts to control the use of the AAR while preserving the privacy. To achieve this task, our method leverages a connection to agglomerative information bottleneck, through which the amount of disclosed data can be compressed so that irrelevant private information can be reduced, and a connection to general privacy statistical inference framework, where both of the privacy leakage and utility accuracy are considered as mutual information. Our experimental results have shown that the proposed solution can greatly reduce privacy leakage while maintaining a relative good utility.},
}

@article{menzies2013_software_analytics_so_what,
  author={Menzies, Tim and Zimmermann, Thomas},
  journal={IEEE Software}, 
  title={Software Analytics: So What?}, 
  year={2013}, 
  volume={30},
  number={4}, 
  pages={31-37}, 
  abstract={
    The guest editors of this special issue of IEEE Software invited submissions that reflected the benefits (and drawbacks) of software analytics, an area of explosive growth. They had so many excellent submissions that they had to split this special issue into two volumes--you'll see even more content in the September/October issue. They divided the articles on conceptual grounds, so both volumes will feature equally excellent work. The Web extra at http://youtu.be/nO6X0azR0nw is a video interview in which IEEE Software editor in chief Forrest Shull speaks with Tim Menzies about the growing importance of software analytics.},  
  keywords={}, 
  doi={10.1109/MS.2013.86}, 
  ISSN={1937-4194}, 
  month={July},
}

@ARTICLE {menzies2018_unreasonable_effectiveness_of_software_analytics,
    author = {Tim Menzies},
    journal = {IEEE Software},
    title = {The Unreasonable Effectiveness of Software Analytics},
    year = {2018},
    volume = {35},
    number = {02},
    issn = {1937-4194},
    pages = {96-98},
    keywords = {software;complexity theory;software engineering;scrum (software development);frequency selective surfaces;task analysis;software tools},
    doi = {10.1109/MS.2018.1661323},
    publisher = {IEEE Computer Society},
    address = {Los Alamitos, CA, USA},
    month = {mar}
}


@article{menzies2019_badsmells_in_software_analytics,
  title={“Bad smells” in software analytics papers},
  author={Menzies, Tim and Shepperd, Martin},
  journal={Information and Software Technology},
  volume={112},
  pages={35--47},
  year={2019},
  publisher={Elsevier},
  abstract = {
    Context: There has been a rapid growth in the use of data analytics to underpin evidence-based software engineering. However the combination of complex techniques, diverse reporting standards and poorly understood underlying phenomena are causing some concern as to the reliability of studies.

    Objective: Our goal is to provide guidance for producers and consumers of software analytics studies (computational experiments and correlation studies).

    Method: We propose using “bad smells”, i.e., surface indications of deeper problems and popular in the agile software community and consider how they may be manifest in software analytics studies.

    Results: We list 12 “bad smells” in software analytics papers (and show their impact by examples).

    Conclusions: We believe the metaphor of bad smell is a useful device. Therefore we encourage more debate on what contributes to the validity of software analytics studies (so we expect our list will mature over time).
  }
}

@article{merritt2008_not_all_trust_is_created_equal_etc,
    author = {Stephanie M. Merritt and Daniel R. Ilgen},
    title ={Not All Trust Is Created Equal: Dispositional and History-Based Trust in Human-Automation Interactions},
    journal = {Human Factors},
    volume = {50},
    number = {2},
    pages = {194-210},
    year = {2008},
    doi = {10.1518/001872008X288574},
    note ={PMID: 18516832},
    URL = {https://doi.org/10.1518/001872008X288574},
    eprint = {https://doi.org/10.1518/001872008X288574},
    abstract = { Objective: We provide an empirical demonstration of the importance of attending to human user individual differences in examinations of trust and automation use. Background: Past research has generally supported the notions that machine reliability predicts trust in automation, and trust in turn predicts automation use. However, links between user personality and perceptions of the machine with trust in automation have not been empirically established. Method: On our X-ray screening task, 255 students rated trust and made automation use decisions while visually searching for weapons in X-ray images of luggage. Results: We demonstrate that individual differences affect perceptions of machine characteristics when actual machine characteristics are constant, that perceptions account for 52\% of trust variance above the effects of actual characteristics, and that perceptions mediate the effects of actual characteristics on trust. Importantly, we also demonstrate that when administered at different times, the same six trust items reflect two types of trust (dispositional trust and history-based trust) and that these two trust constructs are differentially related to other variables. Interactions were found among user characteristics, machine characteristics, and automation use. Conclusion: Our results suggest that increased specificity in the conceptualization and measurement of trust is required, future researchers should assess user perceptions of machine characteristics in addition to actual machine characteristics, and incorporation of user extraversion and propensity to trust machines can increase prediction of automation use decisions. Application: Potential applications include the design of flexible automation training programs tailored to individuals who differ in systematic ways. }
}

@ARTICLE{miller2013_from_data_to_decisions_a_value_chain_for_big_data,  
  author={Miller, H. Gilbert and Mork, Peter}, 
  journal={IT Professional}, 
  title={From Data to Decisions: A Value Chain for Big Data}, 
  year={2013},
  volume={15}, 
  number={1}, 
  pages={57-59}, 
  abstract={
    With exponential growth in data, enterprises must act to make the most of the vast data landscape-to thoughtfully apply multiple technologies, carefully select key data for specific investigations, and innovatively tailor large integrated datasets to support specific queries and analyses. All these actions will flow from a data value chain-a framework to manage data holistically from capture to decision making and to support a variety of stakeholders and their technologies.
  }, 
  keywords={},
  doi={10.1109/MITP.2013.11}, 
  ISSN={1941-045X}, 
  month={Jan},
}

@ARTICLE {mills1987_cleanroom_software_engineering,
    author = {H. Mills and R. Linger and M. Dyer},
    journal = {IEEE Software},
    title = {Cleanroom Software Engineering},
    year = {1987},
    volume = {4},
    number = {05},
    issn = {1937-4194},
    pages = {19-25},
    keywords = {null},
    doi = {10.1109/MS.1987.231413},
    publisher = {IEEE Computer Society},
    address = {Los Alamitos, CA, USA},
    month = {sep}
}

@article{murphy2004_automating_software_failure_reporting,
    author = {Murphy, Brendan},
    title = {Automating Software Failure Reporting: We Can Only Fix Those Bugs We Know About.},
    year = {2004},
    issue_date = {November 2004},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {2},
    number = {8},
    issn = {1542-7730},
    url = {https://doi.org/10.1145/1036474.1036498},
    doi = {10.1145/1036474.1036498},
    abstract = {There are many ways to measure quality before and after software is released. For
    commercial and internal-use-only products, the most important measurement is the user’s
    perception of product quality. Unfortunately, perception is difficult to measure,
    so companies attempt to quantify it through customer satisfaction surveys and failure/behavioral
    data collected from its customer base. This article focuses on the problems of capturing
    failure data from customer sites. To explore the pertinent issues I rely on experience
    gained from collecting failure data from Windows XP systems, but the problems you
    are likely to face when developing internal (noncommercial) software should not be
    dissimilar.},
    journal = {Queue},
    month = nov,
    pages = {42–48},
    numpages = {7}
}

@ARTICLE{musa1990_software_reliability_engineering_technology_for_the_1990s,  
  author={Musa, J.D. and Everett, W.W.},  
  journal={IEEE Software},  
  title={Software-reliability engineering: technology for the 1990s}, 
  year={1990}, 
  volume={7}, 
  number={6}, 
  pages={36-43}, 
  abstract={
    It argued that software engineering is about to reach a new stage, the reliability stage, that stresses customers' operational needs and that software-reliability engineering will make this stage possible. Software-reliability engineering is defined as the applied science of predicting, measuring, and managing the reliability of software-based systems to maximize customer satisfaction. The basic concepts of software reliability-engineering and the reasons why it is important are examined. The application of software-reliability engineering at each stage of the life cycle is described.}, 
  keywords={},
  doi={10.1109/52.60588},
  ISSN={1937-4194},
  month={Nov},
}

@ARTICLE{musa1993_operational_profiles,  
    author = {J. D. {Musa}},  
    journal = {IEEE Software},   
    title = {Operational profiles in software-reliability engineering},
    year = {1993},  
    volume = {10},  
    number = {2},  
    pages = {14-32},
    
    abstract = {A systematic approach to organizing the process of determining the operational profile for guiding software development is presented. The operational profile is a quantitative characterization of how a system will be used that shows how to increase productivity and reliability and speed development by allocating development resources to function on the basis of use. Using an operational profile to guide testing ensures that if testing is terminated and the software is shipped because of schedule constraints, the most-used operations will have received the most testing and the reliability level will be the maximum that is practically achievable for the given test time. For guiding regression testing, it efficiently allocates test cases in accordance with use, so the faults most likely to be found, of those introduced by changes, are the ones that have the most effect on reliability.<>},
    
    keywords = {program testing;software reliability;software-reliability engineering;operational profile;guiding software development;quantitative characterization;productivity;speed development;schedule constraints;regression testing;Costs;Reliability engineering;Marketing and sales;Automatic testing;Productivity;Resource management;Capacitive sensors;Switching systems;Customer satisfaction;Operating systems},  doi = {10.1109/52.199724},  
    ISSN = {1937-4194},  
    month = {March},
}

@article{not_cited_yet_necmiye2017_a_slr_opinion_mining_studies_from_mobile_app_store_reviews,
  author = {Necmiye Genc-Nayebi, Alain Abran},
  title = {A systematic literature review: Opinion mining studies from mobile app store user reviews},
  journal = {Journal of Systems and Software},
  volume = {125},
  year = {2017},
  pages = {207-219},
  ISSN = {0164-1212},
  url = {https://doi.org/10.1016/j.jss.2016.11.027.},
  hmmm = {(https://www.sciencedirect.com/science/article/pii/S0164121216302291)},
  abstract = {
    As mobile devices have overtaken fixed Internet access, mobile applications and distribution platforms have gained in importance. App stores enable users to search for, purchase and install mobile applications and then give feedback in the form of reviews and ratings. A review might contain information about the user’s experience with the app and opinion of it, feature requests and bug reports. Hence, reviews are valuable not only to users who would like to find out what others think about an app, but also to developers and software companies interested in customer feedback. The rapid increase in the number of applications and total app store revenue has accelerated app store data mining and opinion aggregation studies. While development companies and app store regulators have pursued upfront opinion mining studies for business intelligence and marketing purposes, research interest into app ecosystem and user reviews is relatively new. In addition to studies examining online product reviews, there are now some academic studies focused on mobile app stores and user reviews. The objectives of this systematic literature review are to identify proposed solutions for mining online opinions in app store user reviews, challenges and unsolved problems in the domain, any new contributions to software requirements evolution and future research direction.},
    keywords = {Mobile application; App stores opinion mining; Systematic literature review; Requirements engineering}, 
}

 @article{_duplicated_in_logging_paper_Oliveira_Borges_Silva_Cacho_Castor_2018_android_error_handling, 
   title={Do android developers neglect error handling? a maintenance-Centric study on the relationship between android abstractions and uncaught exceptions},
   volume={136},
   ISSN={0164-1212},
   DOI={https://doi.org/10.1016/j.jss.2017.10.032},
   abstractNote={All the mainstream programming languages in widespread use for mobile app development provide error handling mechanisms to support the implementation of robust apps. Android apps, in particular, are usually written in the Java programming language. Java includes an exception handling mechanism that allows programs to signal the occurrence of errors by throwing exceptions and to handle these exceptions by catching them. All the Android-specific abstractions, such as activities and asynctasks, can throw exceptions when errors occur. When an app catches the exceptions that it or the libraries upon which it depends throw, it can resume its activity or, at least, fail in a graceful way. On the other hand, uncaught exceptions can lead an app to crash, particularly if they occur within the main thread. Previous work has shown that, in real Android apps available at the Play Store, uncaught exceptions thrown by Android-specific abstractions often cause these apps to fail. This paper presents an empirical study on the relationship between the usage of Android abstractions and uncaught exceptions. Our approach is quantitative and maintenance-centric. We analyzed changes to both normal and exception handling code in 112 versions extracted from 16 software projects covering a number of domains, amounting to more than 3 million LOC. Change impact analysis and exception flow analysis were performed on those versions of the projects. The main finding of this study is that, during the evolution of the analyzed apps, an increase in the use of Android abstractions exhibits a positive and statistically significant correlation with the number of uncaught exception flows. Since uncaught exceptions cause apps to crash, this result suggests that these apps are becoming potentially less robust as a consequence of exception handling misuse. Analysis of multiple versions of these apps revealed that Android developers usually employ abstractions that may throw exceptions without adding the appropriate handlers for these exceptions. This study highlights the need for better testing and verification tools with a focus on exception handling code and for a change of culture in Android development or, at least, in the design of its APIs.},
   journal={Journal of Systems and Software},
   author={Oliveira, Juliana and Borges, Deise and Silva, Thaisa and Cacho, Nelio and Castor, Fernando},
   year={2018},
   pages={1–18},
   keywords = {Exception handling, Android, Robustness, Maintainability},
   extracts = {
    Highlights
        • Most of the method calls to Android abstractions are not protected.
        • Use of Android abstractions often decreases robustness.
        • Lack of changes to exception handling code leads to uncaught exception.
        • The change scenarios that decrease robustness are described.
   },
   thoughts = {
    This paper helps explain that mobile apps have their own distinct challenges in terms of maintaining the reliability of the app (however I suspect some frameworks for desktop apps might have similar challenges but predate the researchers interested in these topics). 
   }
}


@article{ormen2015_smartphone_log_data_qualitative_perspective,
  title={Smartphone log data in a qualitative perspective},
  author={{\O}rmen, Jacob and Thorhauge, Anne Mette},
  journal={Mobile Media \& Communication},
  volume={3},
  number={3},
  pages={335--350},
  year={2015},
  publisher={SAGE Publications Sage UK: London, England},
  abstract = {Log data from smartphones have primarily been used in large-scale research designs to draw statistical inferences from hundreds or even thousands of participants. In this article, we argue that more qualitatively oriented designs can also benefit greatly from integrating these rich data sources into studies of smartphones in everyday life. Through an illustrative study, we explore a more nuanced perspective on what can be considered “log data” and how these types of data can be collected and analysed. A qualitative approach to log data analysis offers researchers new opportunities to situate smartphone use within people’s practices, norms, and routines. This is of relevance both to studies focusing on smartphones as cultural objects in everyday life and studies that use such devices as proxies for social behaviour more generally. We argue that log data, for instance in in-depth interviews, may serve as cues to instigate discussion and reflection as well as act as resources for contextualizing and organizing related empirical material. In the discussion, the advantages of a qualitative perspective for research designs are assessed in relation to issues of validity. Further perspectives on the promises of log data from various devices are proposed.},
}

@inproceedings{padmanabha2018_mitigating_the_latency_accuracy_tradeoff_in_moile_data_analytics_systems,
    author = {Padmanabha Iyer, Anand and Erran Li, Li and Chowdhury, Mosharaf and Stoica, Ion},
    title = {Mitigating the Latency-Accuracy Trade-off in Mobile Data Analytics Systems},
    year = {2018},
    isbn = {9781450359030},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3241539.3241581},
    doi = {10.1145/3241539.3241581},
    abstract = {
      An increasing amount of mobile analytics is performed on data that is procured in a real-time fashion to make real-time decisions. Such tasks include simple reporting on streams to sophisticated model building. However, the practicality of these analyses are impeded in several domains because they are faced with a fundamental trade-off between data collection latency and analysis accuracy. In this paper, we first study this trade-off in the context of a specific domain, Cellular Radio Access Networks (RAN). We find that the trade-off can be resolved using two broad, general techniques: intelligent data grouping and task formulations that leverage domain characteristics. Based on this, we present CellScope, a system that applies a domain specific formulation and application of Multi-task Learning (MTL) to RAN performance analysis. It uses three techniques: feature engineering to transform raw data into effective features, a PCA inspired similarity metric to group data from geographically nearby base stations sharing performance commonalities, and a hybrid online-offline model for efficient model updates. Our evaluation shows that CellScope's accuracy improvements over direct application of ML range from 2.5\texttimes{} to 4.4\texttimes{} while reducing the model update overhead by up to 4.8\texttimes{}. We have also used CellScope to analyze an LTE network of over 2 million subscribers, where it reduced troubleshooting efforts by several magnitudes. We then apply the underlying techniques in CellScope to another domain specific problem, mobile phone energy bug diagnosis, and show that the techniques are general.
    },
    booktitle = {Proceedings of the 24th Annual International Conference on Mobile Computing and Networking},
    pages = {513–528},
    numpages = {16},
    keywords = {mobile systems, data analytics, cellular networks},
    location = {New Delhi, India},
    series = {MobiCom '18}
}

@article{palvia2001socio,
  title={A socio-technical framework for quality assessment of computer information systems},
  author={Palvia, Shailendra C and Sharma, Ravi S and Conrath, David W},
  journal={Industrial Management \& Data Systems},
  year={2001},
  publisher={MCB UP Ltd},
  key_point_1 = {They use an Ishikawa diagram related to software quality},
  key_point_2 = {The paper makes 5 initial claims which still seem relevant today},
  key_point_3 = {They use a definition of Quality from ISO9000},
}

@inproceedings{petre2009_insights_from_expert_software_design_practice,
    author = {Petre, Marian},
    title = {Insights from Expert Software Design Practice},
    year = {2009},
    isbn = {9781605580012},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/1595696.1595731},
    doi = {10.1145/1595696.1595731},
    abstract = {Software is a designed artifact. In other design disciplines, such as architecture,
    there is a well-established tradition of design studies which inform not only the
    discipline itself but also tool design, processes, and collaborative work. The 'challenge'
    of this paper is to consider software from such a 'design studies' perspective. This
    paper will present a series of observations from empirical studies of expert software
    designers, and will draw on examples from actual professional practice. It will consider
    what experts' mental imagery, software visualisations, and sketches suggest about
    software design thinking. It will also discuss some of the deliberate practices experts
    use to promote innovation. Finally, it will open discussion on the tensions between
    observed software design practices and received methodology in software engineering.},
    booktitle = {Proceedings of the 7th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations of Software Engineering},
    pages = {233–242},
    numpages = {10},
    keywords = {design, empirical studies, software design processes, expertise},
    location = {Amsterdam, The Netherlands},
    series = {ESEC/FSE '09},
}

@article{petsas2017measurement,
  title={Measurement, modeling, and analysis of the mobile app ecosystem},
  author={Petsas, Thanasis and Papadogiannakis, Antonis and Polychronakis, Michalis and Markatos, Evangelos P and Karagiannis, Thomas},
  journal={ACM Transactions on Modeling and Performance Evaluation of Computing Systems (TOMPECS)},
  doi = {https://doi.org/10.1145/2993419},
  volume={2},
  number={2},
  pages={1--33},
  year={2017},
  publisher={ACM New York, NY, USA},
  abstract={
    Mobile applications (apps) have been gaining popularity due to the advances in mobile technologies and the large increase in the number of mobile users. Consequently, several app distribution platforms, which provide a new way for developing, downloading, and updating software applications in modern mobile devices, have recently emerged. To better understand the download patterns, popularity trends, and development strategies in this rapidly evolving mobile app ecosystem, we systematically monitored and analyzed four popular third-party Android app marketplaces. Our study focuses on measuring, analyzing, and modeling the app popularity distribution and explores how pricing and revenue strategies affect app popularity and developers’ income.

    Our results indicate that unlike web and peer-to-peer file sharing workloads, the app popularity distribution deviates from commonly observed Zipf-like models. We verify that these deviations can be mainly attributed to a new download pattern, which we refer to as the clustering effect. We validate the existence of this effect by revealing a strong temporal affinity of user downloads to app categories. Based on these observations, we propose a new formal clustering model for the distribution of app downloads and demonstrate that it closely fits measured data. Moreover, we observe that paid apps follow a different popularity distribution than free apps and show how free apps with an ad-based revenue strategy may result in higher financial benefits than paid apps. We believe that this study can be useful to appstore designers for improving content delivery and recommendation systems, as well as to app developers for selecting proper pricing policies to increase their income.},
}

@article{parasuraman_complacency_and_bias_in_human_use_of_automation,
    author = {Raja Parasuraman and Dietrich H. Manzey},
    title ={Complacency and Bias in Human Use of Automation: An Attentional Integration},
    journal = {Human Factors},
    volume = {52},
    number = {3},
    pages = {381-410},
    year = {2010},
    doi = {10.1177/0018720810376055},
    note ={PMID: 21077562},
    URL = {https://doi.org/10.1177/0018720810376055},
    eprint = {https://doi.org/10.1177/0018720810376055},
    abstract = { Objective: Our aim was to review empirical studies of complacency and bias in human interaction with automated and decision support systems and provide an integrated theoretical model for their explanation.Background: Automation-related complacency and automation bias have typically been considered separately and independently.Methods: Studies on complacency and automation bias were analyzed with respect to the cognitive processes involved.Results: Automation complacency occurs under conditions of multiple-task load, when manual tasks compete with the automated task for the operator’s attention. Automation complacency is found in both naive and expert participants and cannot be overcome with simple practice. Automation bias results in making both omission and commission errors when decision aids are imperfect.Automation bias occurs in both naive and expert participants, cannot be prevented by training or instructions, and can affect decision making in individuals as well as in teams.While automation bias has been conceived of as a special case of decision bias, our analysis suggests that it also depends on attentional processes similar to those involved in automation-related complacency.Conclusion: Complacency and automation bias represent different manifestations of overlapping automation-induced phenomena, with attention playing a central role. An integrated model of complacency and automation bias shows that they result from the dynamic interaction of personal, situational, and automation-related characteristics.Application: The integrated model and attentional synthesis provides a heuristic framework for further research on complacency and automation bias and design options for mitigating such effects in automated and decision support systems. }
}

 @article{pfleeger2000_risky_business, 
   title={Risky business: what we have yet to learn about risk management},
   volume={53},
   ISSN={0164-1212},
   DOI={https://doi.org/10.1016/S0164-1212(00)00017-0},
   abstractNote={
     This paper examines the way in which software practitioners are taught to perform risk management, and compares it with risk management in other fields. We find that there are three major problems with risk management: false precision, bad science, and the confusion of facts with values. All of these problems can lead to bad decisions, all in the guise of more objective decision-making. But we can learn from these problems and improve the way we do risk management.}, 
   number={3},
   journal={Journal of Systems and Software},
   author={Pfleeger, Shari Lawrence},
   year={2000},
   pages={265–273},
   url = {http://www.sciencedirect.com/science/article/pii/S0164121200000170},
   quotes = {
     `We must personalize the quantified risk to make it meaningful.',
   },
}

@article{randell1970_software_engineering_techniques_nato_dijkstra,
  title={Software Engineering Techniques: Report of a conference sponsored by the NATO Science Committee, Rome, Italy, 27th-31st October 1969},
  author={Randell, B and Buxton, JN},
  year={1970},
  publisher={Newcastle University},
  url = {http://homepages.cs.ncl.ac.uk/brian.randell/NATO/nato1969.PDF},
}

@article{ransdell1993educational_software_evaluation_research_validities,
  title={Educational software evaluation research: Balancing internal, external, and ecological validity},
  author={Ransdell, Sarah},
  journal={Behavior Research Methods, Instruments, \& Computers},
  volume={25},
  number={2},
  pages={228--232},
  year={1993},
  publisher={Springer},
  doi = {https://doi.org/10.3758/BF03204502},
  abstract = {The difficulties inherent in the evaluation of educational software are described in terms of the tradeoffs between internal, external, and ecological validity. Larger issues in evaluation research design and computer-based instruction are highlighted by primary and metaanalytic studies designed to reveal the effects of computer simulations in psychology classrooms and laboratories. The effectiveness of classroom and laboratory computer activities depends on how the inclusion of software, as well as the evaluation process itself, changes the entire instructional process.},
}

@article{rennie1998_freedom_and_responsibility_in_medical_publication,
    author = {Rennie, Drummond},
    title = "{Freedom and Responsibility in Medical Publication: Setting the Balance Right}",
    journal = {JAMA},
    volume = {280},
    number = {3},
    pages = {300-302},
    year = {1998},
    month = {07},
    abstract = "{
      I wish to discuss changes that we might make that would improve the ethical climate of the publication of research by making it both more open and more responsible. While none of the systems I briefly discuss have yet become common practice, they are all being tried in various ways, and they indicate the direction in which we ought to move.In the future, the following 4 systems will, I hope, be routine. First, authorship will be abolished in favor of contribution, and the work done by the contributors will be listed for the readers. Second, peer review will become open, and we will come to talk about anonymous review as a quaint anachronism. Third, scientists will take full responsibility for the aftercare of their papers, and, fourth, as a result, editors will enable and encourage readers to assume the responsibilities of reviewers. Changing our practice would in each case rectify the balance between rights and duties.}",
    issn = {0098-7484},
    doi = {10.1001/jama.280.3.300},
    url = {https://doi.org/10.1001/jama.280.3.300},
    eprint = {https://jamanetwork.com/journals/jama/articlepdf/187765/jpv71038.pdf},
}

@online{reid2012_iso29119_eurostar,
  title={ISO/IEC/IEEE 29119: the new international software testing standards},
  author={Reid, Stuart},
  year={2012},
  organization={Testing Solutions Group},
  address={London, UK},
  publisher={EuroSTAR Conferences},
}

@article{robinson2019_applying_endosymbiosis_theory_tourism_and_its_young_workers,
title = {Applying endosymbiosis theory: Tourism and its young workers},
journal = {Annals of Tourism Research},
volume = {78},
pages = {102751},
year = {2019},
issn = {0160-7383},
doi = {https://doi.org/10.1016/j.annals.2019.102751},
url = {https://www.sciencedirect.com/science/article/pii/S0160738319301082},
author = {Richard N.S. Robinson and Tom Baum and Maria Golubovskaya and David J. Solnet and Victor Callan},
keywords = {Systems theory, Endosymbiosis theory, Interdependency, Employment, Young workers, Tourism organisations},
abstract = {Building on systems theory and its applications in tourism management, we introduce the natural science evolutionary ‘endosymbiosis theory’ to interpret the inter-dependencies of youth employment and tourism. Tourism organisations are located within a tourism industry or a sub-system, which in turn is bounded within a broader socio-economic ecosystem. We mobilise three classifications of symbiosis - mutualism, commensalism and parasitism to: a) test the analytic utility of this theoretical approach as a means to unpack the young worker and tourism employment relationships, and b) unify hitherto disparate literatures on the youth-tourism employment relationship. In particular, we model the explanatory value of endosymbiosis theory, navigating the ethicalities and moralities of the social sciences, in progressing our understanding of the tourism-young worker intersection.}
}

@article{rowley2002_using_case_studies_in_research,
  title={Using case studies in research},
  author={Rowley, Jennifer},
  journal={Management research news},
  year={2002},
  pages = {16--27},
  volume = {25},
  number = {1},
  publisher={MCB UP Ltd},
  doi = {10.1108/01409170210782990},
  abstract = {
    Draws heavily on previous established research in an attempt to distil the key aspects of case study research in such a way as to encourage new researchers to grapple with and apply these. Explains when case study can be used, research design, data collection and data analysis, offering suggestions for drawing on the evidence in writing a report or dissertation. Briefly reviews alternative perspectives on the subject.
  }
}


@article{runeson_2008_guidelines_for_conducting_and_reporting_case_study_research_in_sw_eng,
	title = {Guidelines for conducting and reporting case study research in software engineering},
	volume = {14},
	issn = {1573-7616},
	url = {https://doi.org/10.1007/s10664-008-9102-8},
	doi = {10.1007/s10664-008-9102-8},
	abstract = {Case study is a suitable research methodology for software engineering research since it studies contemporary phenomena in its natural context. However, the understanding of what constitutes a case study varies, and hence the quality of the resulting studies. This paper aims at providing an introduction to case study methodology and guidelines for researchers conducting case studies and readers studying reports of such studies. The content is based on the authors’ own experience from conducting and reading case studies. The terminology and guidelines are compiled from different methodology handbooks in other research domains, in particular social science and information systems, and adapted to the needs in software engineering. We present recommended practices for software engineering case studies as well as empirically derived and evaluated checklists for researchers and readers of case study research.},
	pages = {131},
	number = {2},
	journaltitle = {Empirical Software Engineering},
	journal = {Empirical Software Engineering},
	shortjournal = {Empirical Software Engineering},
	author = {Runeson, Per and Höst, Martin},
	date = {2008-12-19},
	year = {2008},
}

@article{saidani2022_tracking_bad_updates_in_mobile_apps_a_search_based_approach,
  title={Tracking Bad Updates in Mobile Apps: A Search-based Approach},
  author={Saidani, Islem and Ouni, Ali and Ahasanuzzaman, Md and Hassan, Safwat and Mkaouer, Mohamed Wiem and Hassan, Ahmed E},
  journal={Empirical Software Engineering},
  pages={1--41},
  year={2022},
  volume = 27, 
  issue = 4,
}

@ARTICLE{saltzer2020_the_origin_of_the_mit_license,
  author={Saltzer, Jerome H.}, 
  journal={IEEE Annals of the History of Computing},
  title={The Origin of the “MIT License”},  
  year={2020},
  volume={42},
  number={4}, 
  pages={94-98},
  abstract={
    Discusses the origin and history of the “MIT License.” This license has become a popular way of releasing copyrighted computer programs for others to use without requiring signatures or a license fee. The quoted words refer to a group of software licenses that have common ancestry and guiding principles but varying wordings. There has been some recent discussion about the origin of the “MIT License,” but that discussion has been inconclusive because authoritative historical documentation has been missing or hard to find. This note provides some of that history and documentation. Being composed 35 years after the events involved, it relies primarily on my often-flaky recollections but it also offers some relevant supporting documentation found in my files and in online archives.},
  keywords={}, 
  doi={10.1109/MAHC.2020.3020234}, 
  ISSN={1934-1547}, 
  month={Oct},
  publisher = {IEEE},
}

@article{savolainen2006_information_use_as_gap_bridging_the_viewpoint_of_sensemaking_methodology,
    author = {Savolainen, Reijo},
    title = {Information use as gap-bridging: The viewpoint of sense-making methodology},
    journal = {Journal of the American Society for Information Science and Technology},
    volume = {57},
    number = {8},
    pages = {1116-1125},
    doi = {https://doi.org/10.1002/asi.20400},
    url = {https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/asi.20400},
    eprint = {https://asistdl.onlinelibrary.wiley.com/doi/pdf/10.1002/asi.20400},
    abstract = {
       The conceptual issues of information use are discussed by reviewing the major ideas of sense-making methodology developed by Brenda Dervin. Sense-making methodology approaches the phenomena of information use by drawing on the metaphor of gap-bridging. The nature of this metaphor is explored by utilizing the ideas of metaphor analysis suggested by Lakoff and Johnson. First, the source domain of the metaphor is characterized by utilizing the graphical illustrations of sense-making metaphors. Second, the target domain of the metaphor is analyzed by scrutinizing Dervin's key writings on information seeking and use. The metaphor of gap-bridging does not suggest a substantive conception of information use; the metaphor gives methodological and heuristic guidance to posit contextual questions as to how people interpret information to make sense of it. Specifically, these questions focus on the ways in which cognitive, affective, and other elements useful for the sense-making process are constructed and shaped to bridge the gap. Ultimately, the key question of information use studies is how people design information in context.
    },
    year = {2006}
}


@comment{Not yet cited the following},
@article{schelter2018_automating_large_scale_data_quality_verification,
author = {Schelter, Sebastian and Lange, Dustin and Schmidt, Philipp and Celikel, Meltem and Biessmann, Felix and Grafberger, Andreas},
title = {Automating Large-Scale Data Quality Verification},
year = {2018},
issue_date = {August 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3229863.3229867},
doi = {10.14778/3229863.3229867},
abstract = {Modern companies and institutions rely on data to guide every single business process and decision. Missing or incorrect information seriously compromises any decision process downstream. Therefore, a crucial, but tedious task for everyone involved in data processing is to verify the quality of their data. We present a system for automating the verification of data quality at scale, which meets the requirements of production use cases. Our system provides a declarative API, which combines common quality constraints with user-defined validation code, and thereby enables 'unit tests' for data. We efficiently execute the resulting constraint validation workload by translating it to aggregation queries on Apache Spark. Our platform supports the incremental validation of data quality on growing datasets, and leverages machine learning, e.g., for enhancing constraint suggestions, for estimating the 'predictability' of a column, and for detecting anomalies in historic data quality time series. We discuss our design decisions, describe the resulting system architecture, and present an experimental evaluation on various datasets.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1781–1794},
numpages = {14}
}

@article{schneble2018_cambridge_analytica_affair,
  title={The Cambridge Analytica affair and Internet-mediated research},
  author={Schneble, Christophe Olivier and Elger, Bernice Simone and Shaw, David},
  journal={EMBO reports},
  volume={19},
  number={8},
  pages={e46579},
  year={2018}
}

@ARTICLE{seaman1999_qualitative_methods_in_esse,  
    author={Seaman, C.B.},  
    journal={IEEE Transactions on Software Engineering}, 
    title={Qualitative methods in empirical studies of software engineering},   
    year={1999}, 
    volume={25},  
    number={4},  
    pages={557-572},
    abstract={
      While empirical studies in software engineering are beginning to gain recognition in the research community, this subarea is also entering a new level of maturity by beginning to address the human aspects of software development. This added focus has added a new layer of complexity to an already challenging area of research. Along with new research questions, new research methods are needed to study nontechnical aspects of software engineering. In many other disciplines, qualitative research methods have been developed and are commonly used to handle the complexity of issues involving human behaviour. The paper presents several qualitative methods for data collection and analysis and describes them in terms of how they might be incorporated into empirical studies of software engineering, in particular how they might be combined with quantitative methods. To illustrate this use of qualitative methods, examples from real software engineering studies are used throughout.
    },  
    keywords={},  
    doi={10.1109/32.799955}, 
    ISSN={1939-3520},
    month={July},
}

@article{sen2008_determinants_for_foss_license,
  author = { Ravi   Sen  and  Chandrasekar   Subramaniam  and  Matthew L.   Nelson },
  title = {Determinants of the Choice of Open Source Software License},
  journal = {Journal of Management Information Systems},
  volume = {25},
  number = {3},
  pages = {207-240},
  year  = {2008},
  publisher = {Routledge},
  doi = {10.2753/MIS0742-1222250306},
  URL = {https://doi.org/10.2753/MIS0742-1222250306},
  eprint = {https://doi.org/10.2753/MIS0742-1222250306},
  abstract = { 
    In this paper, we examine how the motivations and attitudes of open source software (OSS) developers affect their preference among the three common OSS license types—Strong-Copyleft, Weak-Copyleft, and Non-Copyleft. Despite the importance of the license type and developers to OSS projects, there is little understanding in open source literature of the license choice from a developer's perspective. The results from our empirical study of OSS developers reveal that the intrinsic motivation of challenge (problem solving) is associated with the developers' preference for licenses with moderate restrictions, while the extrinsic motivation of status (through peer recognition) is associated with developers' preference for licenses with least restrictions. We also find that when choosing an OSS license, a developer's attitude toward the software redistribution rights conflicts with his or her attitude toward preserving the social benefits of open source. A major implication of our findings is that OSS managers who want to attract a limited number of highly skilled programmers to their open source project should choose a restrictive OSS license. Similarly, managers of software projects for social programs could attract more developers by choosing a restrictive OSS license.},
}

@article{shahin2019empirical_study_architecting_cd,
  title={An empirical study of architecting for continuous delivery and deployment},
  author={Shahin, Mojtaba and Zahedi, Mansooreh and Babar, Muhammad Ali and Zhu, Liming},
  journal={Empirical Software Engineering},
  volume={24},
  number={3},
  pages={1061--1108},
  year={2019},
  publisher={Springer},
  doi={10.1007/s10664-018-9651-4},
  abstract = {Recently, many software organizations have been adopting Continuous Delivery and Continuous Deployment (CD) practices to develop and deliver quality software more frequently and reliably. Whilst an increasing amount of the literature covers different aspects of CD, little is known about the role of software architecture in CD and how an application should be (re-) architected to enable and support CD. We have conducted a mixed-methods empirical study that collected data through in-depth, semi-structured interviews with 21 industrial practitioners from 19 organizations, and a survey of 91 professional software practitioners. Based on a systematic and rigorous analysis of the gathered qualitative and quantitative data, we present a conceptual framework to support the process of (re-) architecting for CD. We provide evidence-based insights about practicing CD within monolithic systems and characterize the principle of “small and independent deployment units” as an alternative to the monoliths. Our framework supplements the architecting process in a CD context through introducing the quality attributes (e.g., resilience) that require more attention and demonstrating the strategies (e.g., prioritizing operations concerns) to design operations-friendly architectures. We discuss the key insights (e.g., monoliths and CD are not intrinsically oxymoronic) gained from our study and draw implications for research and practice.},
}

@article{shan2016finding,
  title={Finding resume and restart errors in Android applications},
  author={Shan, Zhiyong and Azim, Tanzirul and Neamtiu, Iulian},
  journal={ACM SIGPLAN Notices},
  volume={51},
  number={10},
  pages={864--880},
  year={2016},
  publisher={ACM New York, NY, USA},
  doi={https://doi.org/10.1145/3022671.2984011},
  abstract={Smartphone apps create and handle a large variety of ``instance'' data that has to persist across runs, such as the current navigation route, workout results, antivirus settings, or game state. Due to the nature of the smartphone platform, an app can be paused, sent into background, or killed at any time. If the instance data is not saved and restored between runs, in addition to data loss, partially-saved or corrupted data can crash the app upon resume or restart. While smartphone platforms offer API support for data-saving and data-retrieving operations, the use of this API is ad-hoc: left to the programmer, rather than enforced by the compiler. We have observed that several categories of bugs---including data loss, failure to resume/restart or resuming/restarting in the wrong state---are due to incorrect handling of instance data and are easily triggered by just pressing the `Home' or `Back' buttons. To help address this problem, we have constructed a tool chain for Android (the KREfinder static analysis and the KREreproducer input generator) that helps find and reproduce such incorrect handling. We have evaluated our approach by running the static analysis on 324 apps, of which 49 were further analyzed manually. Results indicate that our approach is (i) effective, as it has discovered 49 bugs, including in popular Android apps, and (ii) efficient, completing on average in 61 seconds per app. More generally, our approach helps determine whether an app saves too much or too little state.},
}

@article{shaw1989_comparing_conceptual_structures__consensus_conflict_correspondence_and_contrast,
    title = {Comparing conceptual structures: consensus, conflict, correspondence and contrast},
    journal = {Knowledge Acquisition},
    volume = {1},
    number = {4},
    pages = {341-363},
    year = {1989},
    issn = {1042-8143},
    doi = {https://doi.org/10.1016/S1042-8143(89)80010-X},
    url = {https://www.sciencedirect.com/science/article/pii/S104281438980010X},
    author = {Mildred L.G. Shaw and Brian R. Gaines},
    abstract = {
        One problem of eliciting knowledge from several experts is that experts may share only parts of their terminologies and conceptual systems. Experts may use the same term for different concepts, use different terms for the same concept, use the same term for the same concept, or use different terms and have different concepts. Moreover, clients who use an expert system have even less likelihood of sharing terms and concepts with the experts who produced it. This paper outlines a methodology for eliciting and recognizing such individual differences. It can then be used to focus discussion between experts on those differences between them which require resolution, enabling them to classify them in terms of differing terminologies, levels of abstraction, disagreements, and so on. The methodology promotes the full exploration of the conceptual framework of a domain of expertise by encouraging experts to operate in a “brain-storming” mode as a group, using differing viewpoints to develop a rich framework. It reduces social pressures forcing an invalid consensus by providing objective analysis of separately elicited conceptual systems.
    }
}

@article{shore2004_fail_fast_software_debugging,
  title={Fail fast [software debugging]},
  author={Shore, Jim},
  journal={IEEE Software},
  volume={21},
  number={5},
  pages={21--25},
  year={2004},
  publisher={IEEE},
  abstract={
    The most annoying aspect of software development is debugging. We don't mind the kinds of bugs that yield to a few minutes inspection. The bugs we hate are the ones that show up only after hours of successful operation, under unusual circumstances, or whose stack traces lead to dead ends. Fortunately, there's a simple technique that dramatically reduces the number of these bugs in our software. It won't reduce the overall number of bugs, at least not at first, but it'll make most defects much easier to find. The technique is to build our software to "fail fast".
  },  
  keywords={}, 
  doi={10.1109/MS.2004.1331296}, 
  ISSN={1937-4194},
  month={Sep.},
}

@article{singer2002_ethical_issues_in_empirical_studies_of_software_engineering,  
  author={Singer, J. and Vinson, N.G.},
  journal={IEEE Transactions on Software Engineering},  
  title={Ethical issues in empirical studies of software engineering}, 
  year={2002},  
  volume={28},
  number={12}, 
  pages={1171-1180}, 
  abstract={
    The popularity of empirical methods in software engineering research is on the rise. Surveys, experiments, metrics, case studies, and field studies are examples of empirical methods used to investigate both software engineering processes and products. The increased application of empirical methods has also brought about an increase in discussions about adapting these methods to the peculiarities of software engineering. In contrast, the ethical issues raised by empirical methods have received little, if any, attention in the software engineering literature. This article is intended to introduce the ethical issues raised by empirical research to the software engineering research community and to stimulate discussion of how best to deal with these ethical issues. Through a review of the ethical codes of several fields that commonly employ humans and artifacts as research subjects, we have identified major ethical issues relevant to empirical studies of software engineering. These issues are illustrated with real empirical studies of software engineering.
  },  
  keywords={},
  doi={10.1109/TSE.2002.1158289}, 
  ISSN={1939-3520}, 
  month={Dec},
}

@article{sirianni1979_caesars_decision_to_cross_the_rubicon,
 ISSN = {07702817, 22959076},
 URL = {http://www.jstor.org/stable/41651462},
 online_copy = {https://www.persee.fr/doc/antiq_0770-2817_1979_num_48_2_1955},
 author = {Frank A. Sirianni},
 journal = {L'Antiquité Classique},
 number = {2},
 pages = {636--638},
 publisher = {L'Antiquité Classique},
 title = {CAESAR'S DECISION TO CROSS THE RUBICON},
 volume = {48},
 year = {1979}
}



@article{slany2014tinkering,
  title={Tinkering with Pocket Code, a Scratch-like programming app for your smartphone},
  author={Slany, Wolfgang},
  journal={Proceedings of Constructionism},
  year={2014},
  address={Vienna, Austria},
  numpages={10},
  number={3},
  volume={N/A},
}

@article{soiferman2010_compare_and_contrast_inductive_and_deductive_research_approaches,
  title={Compare and Contrast Inductive and Deductive Research Approaches.},
  author={Soiferman, L Karen},
  journal={Online Submission},
  url = {https://eric.ed.gov/?id=ED542066},
  year={2010},
  publisher={ERIC},
  abstract = {
    This discussion paper compares and contrasts "inductive" and "deductive" research approaches as described by Trochim (2006) and Plano Clark and Creswell (2007). It also examines the "exploratory" and "confirmatory" approaches by Onwueghuzie and Leech (2005) with respect to the assumption each holds about the nature of knowledge. The paper starts with an historical overview of the two main types of research commonly used in educational settings. It continues with a discussion of the elements that showcase the differences and similarities between the two major research approaches. The elements discussed include: intent of the research, how literature is used, how intent is focussed, how data are collected, how data are analyzed, the role of the researcher, and how data are validated. In addition, there is a section which addresses the decisions researchers must make in choosing the research methodology that allows them to answer their research question. The focus of the discussion is on how the two types of research methodology can be used effectively in an educational setting. It concludes with a look at how the different methods of research can be used collaboratively to form a more complete picture of a research study.
  }
}

@article{Sokolf6426_2013_first_do_no_harm_revisited,
	author = {Sokol, Daniel K},
	title = {{\textquotedblleft}First do no harm{\textquotedblright} revisited},
	volume = {347},
	elocation-id = {f6426},
	year = {2013},
	doi = {10.1136/bmj.f6426},
	publisher = {BMJ Publishing Group Ltd},
	URL = {https://www.bmj.com/content/347/bmj.f6426},
	eprint = {https://www.bmj.com/content/347/bmj.f6426.full.pdf},
	journal = {BMJ}
}

@ARTICLE{not_cited_yet_soltani2020_search_based_crash_reproduction_and_its_impact_on_debugging,  
    author={Soltani, Mozhan and Panichella, Annibale and van Deursen, Arie}, 
    journal={IEEE Transactions on Software Engineering},
    title={Search-Based Crash Reproduction and Its Impact on Debugging}, 
    year={2020},  
    volume={46},
    number={12}, 
    pages={1294-1317}, 
    abstract={
        Software systems fail. These failures are often reported to issue tracking systems, where they are prioritized and assigned to responsible developers to be investigated. When developers debug software, they need to reproduce the reported failure in order to verify whether their fix actually prevents the failure from happening again. Since manually reproducing each failure could be a complex task, several automated techniques have been proposed to tackle this problem. Despite showing advancements in this area, the proposed techniques showed various types of limitations. In this paper, we present EvoCrash, a new approach to automated crash reproduction based on a novel evolutionary algorithm, called Guided Genetic Algorithm (GGA). We report on our empirical study on using EvoCrash to reproduce 54 real-world crashes, as well as the results of a controlled experiment, involving human participants, to assess the impact of EvoCrash tests in debugging. Based on our results, EvoCrash outperforms state-of-the-art techniques in crash reproduction and uncovers failures that are undetected by classical coverage-based unit test generation tools. In addition, we observed that using EvoCrash helps developers provide fixes more often and take less time when debugging, compared to developers debugging and fixing code without using EvoCrash tests.
    },  
    keywords={},  
    doi={10.1109/TSE.2018.2877664},  
    ISSN={1939-3520},  
    month={Dec},
}

@article{spolaor2018_delta_data_extraction_and_logging_tool_for_android,  
  author={Spolaor, Riccardo and Santo, Elia Dal and Conti, Mauro}, 
  journal={IEEE Transactions on Mobile Computing}, 
  title={DELTA: Data Extraction and Logging Tool for Android},
  year={2018},  
  volume={17}, 
  number={6}, 
  pages={1289-1302},
  abstract={
    In recent years, the use of smartphones has increased exponentially, and so have their capabilities. Together with an increase in processing power, smartphones are now equipped with a variety of sensors and provide an extensive set of API. These capabilities allow us to extract data related to environment, user habits, and operating system itself. This data is extremely valuable in many research fields such as user authentication, intrusion, and information leaks detection. For these reasons, researchers need a solid and reliable logging tool to collect data from mobile devices. In this paper, we first survey the existing logging tools available on the Android platform, comparing their features and their impact on the system. Then, we present DELTA - Data Extraction and Logging Tool for Android, which improves the existing Android logging solutions in terms of flexibility, fine-grained tuning capabilities, extensibility, and available set of logging features. We fully implement DELTA and we run a thorough performance evaluation. The results show that our tool has a low impact on the performance of the system, on battery consumption, and on user experience. Finally, we make the DELTA source code available to the research community.},  
  keywords={},  
  doi={10.1109/TMC.2017.2762692}, 
  ISSN={1558-0660},  
  month={June},
}

@article{Schuenemann2011_guidelines2_0_do_no_net_harm,
    author={Sch{\"u}nemann, Holger J.},
    title={Guidelines 2.0: Do No Net Harm---The Future of Practice Guideline Development in Asthma and Other Diseases},
    journal={Current Allergy and Asthma Reports},
    year={2011},
    month={Mar},
    day={17},
    volume={11},
    number={3},
    pages={261},
    abstract={Decisions are like double-edged swords: they always come with benefits and downsides. That is, any decision in life bears desirable and undesirable consequences, even if the latter only involves the time it takes to make or think about the decision, which can be considered the harm of decision making. Therefore, it is impossible to adhere to the Hippocratic Oath's concept of ``primum non nocere,'' which is frequently interpreted as ``never do harm.'' The guiding principle for health care decision making should be to ensure that there is, in summary, more benefit than harm---in other words, ``to do no net harm'' (``primum non net nocere''). Practice guidelines support decision making and, as a consequence, would require the explicit consideration of both desirable and undesirable consequences, and assigning due considerations depending on the magnitude and importance of the consequences. The Grading of Recommendations Assessment, Development and Evaluation (GRADE) Working Group (http://www.gradeworkinggroup.org) has made these considerations more explicit when developing health care recommendations. This article briefly summarizes the work of the GRADE working group based on examples of its application in the field of allergy and asthma, and provides an outlook for advances in the field of guideline development. These developments focus on funding of guidelines and handling conflict of interest, working with observational and diagnostic test accuracy studies, developing appropriate group processes, and the integration of values and preferences in the formulation of recommendations.},
    issn={1534-6315},
    doi={10.1007/s11882-011-0185-8},
    url={https://doi.org/10.1007/s11882-011-0185-8}
}

@article{stuckenbruck1979_the_matrix_organization,
  title = {The Matrix Organization},
  author = {Stuckenbruck, Linn C.},
  url = {https://www.pmi.org/learning/library/matrix-organization-structure-reason-evolution-1837},
  volume = {10},
  number = {3},
  pages = {21–33},
  journal = {Project Management Quarterly},
  year = {1979},
}

@ARTICLE{su2020_why_my_app_crashes_etc_android_framework_exceptions,  
    author={Su, Ting and Fan, Lingling and Chen, Sen and Liu, Yang and Xu, Lihua and Pu, Geguang and Su, Zhendong},  
    journal={IEEE Transactions on Software Engineering}, 
    title={Why My App Crashes Understanding and Benchmarking Framework-specific Exceptions of Android apps},  
    year={2020}, 
    volume={48},
    number={4},
    pages={1115-1137},  
    abstract={
      Mobile apps have become ubiquitous. Ensuring their correctness and reliability is important. However, many apps still suffer from occasional to frequent crashes, weakening their competitive edge. Large-scale, deep analyses of the characteristics of real-world app crashes can provide useful insights to both developers and researchers. However, such studies are difficult and yet to be carried out --- this work fills this gap. We collected 16,245 and 8,760 unique exceptions from 2,486 open-source and 3,230 commercial Android apps, respectively, and observed that the exceptions thrown from Android framework (termed "framework-specific exceptions") account for the majority. With one-year effort, we (1) extensively investigated these framework-specific exceptions, and (2) further conducted an online survey of 135 professional app developers about how they analyze, test, reproduce and fix these exceptions. Specifically, we aim to understand the framework-specific exceptions from several perspectives: (i) their characteristics (e.g., manifestation locations, fault taxonomy), (ii) the developers' testing practices, (iii) existing bug detection techniques' effectiveness, (iv) their reproducibility and (v) bug fixes. To enable follow-up research (e.g., bug understanding, detection, localization and repairing), we further systematically constructed, DroidDefects, the first comprehensive and largest benchmark of Android app exception bugs. This benchmark contains 33 reproducible exceptions (with test cases, stack traces, faulty and fixed app versions, bug types, etc.), and 3,696 ground-truth exceptions (real faults manifested by automated testing tools), which cover the apps with different complexities and diverse exception types. Based on our findings, we also built two prototype tools: Stoat+, an optimized dynamic testing tool, which quickly uncovered three previously-unknown, fixed crashes in Gmail and Google+; ExLocator, an exception localization tool, which can locate the root causes of specific exception types. Our dataset, benchmark and tools are publicly available on https://github.com/tingsu/droiddefects.
    },
    keywords={},  
    doi={10.1109/TSE.2020.3013438}, 
    ISSN={1939-3520},  
    month={},
}

@ARTICLE {tantithamthavorn2021_actionable_analytics_tell_me_what_to_do,
    author = {C. Tantithamthavorn and J. Jiarpakdee and J. Grundy},
    journal = {IEEE Software},
    title = {Actionable Analytics: Stop Telling Me What It Is; Please Tell Me What To Do},
    year = {2021},
    volume = {38},
    number = {04},
    issn = {1937-4194},
    pages = {115-120},
    keywords = {project management;software development management;decision making;analytical models},
    doi = {10.1109/MS.2021.3072088},
    publisher = {IEEE Computer Society},
    address = {Los Alamitos, CA, USA},
    month = {jul}
}


@article{taylor2016no,
  title={No place to hide? The ethics and analytics of tracking mobility using mobile phone data},
  author={Taylor, Linnet},
  journal={Environment and Planning D: Society and Space},
  abstract={This paper examines the ethical and methodological problems with tracking human mobility using data from mobile phones, focusing on research involving low- and middle-income countries. Such datasets are becoming accessible to an increasingly broad community of researchers and data scientists, with a variety of analytical and policy uses proposed. This paper provides an overview of the state of the art in this area of research, then sets out a new analytical framework for such data sources that focuses on three pressing issues: first, interpretation and disciplinary bias; second, the potential risks to data subjects in low- and middle-income countries and possible ethical responses; and third, the likelihood of ‘function creep’ from benign to less benign uses. Using the case study of a data science challenge involving West African mobile phone data, I argue that human mobility is becoming legible in new, more detailed ways, and that this carries with it the dual risk of rendering certain groups invisible and of misinterpreting what is visible. Thus, this emerging ability to track movement in real time offers both the possibility of improved responses to conflict and forced migration, but also unprecedented power to surveil and control unwanted population movement.},
  volume={34},
  number={2},
  pages={319--336},
  year={2016},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{unterkalmsteiner2012_evaluation_and_measurement_of_spi_a_slr,  
  author={Unterkalmsteiner, Michael and Gorschek, Tony and Islam, A.K.M. Moinul and Cheng, Chow Kian and Permadi, Rahadian Bayu and Feldt, Robert}, 
  journal={IEEE Transactions on Software Engineering}, 
  title={Evaluation and Measurement of Software Process Improvement—A Systematic Literature Review},
  year={2012},  
  volume={38},
  number={2}, 
  pages={398-424},
  abstract={
    BACKGROUND-Software Process Improvement (SPI) is a systematic approach to increase the efficiency and effectiveness of a software development organization and to enhance software products. OBJECTIVE-This paper aims to identify and characterize evaluation strategies and measurements used to assess the impact of different SPI initiatives. METHOD-The systematic literature review includes 148 papers published between 1991 and 2008. The selected papers were classified according to SPI initiative, applied evaluation strategies, and measurement perspectives. Potential confounding factors interfering with the evaluation of the improvement effort were assessed. RESULTS-Seven distinct evaluation strategies were identified, wherein the most common one, “Pre-Post Comparison,” was applied in 49 percent of the inspected papers. Quality was the most measured attribute (62 percent), followed by Cost (41 percent), and Schedule (18 percent). Looking at measurement perspectives, “Project” represents the majority with 66 percent. CONCLUSION-The evaluation validity of SPI initiatives is challenged by the scarce consideration of potential confounding factors, particularly given that “Pre-Post Comparison” was identified as the most common evaluation strategy, and the inaccurate descriptions of the evaluation context. Measurements to assess the short and mid-term impact of SPI initiatives prevail, whereas long-term measurements in terms of customer satisfaction and return on investment tend to be less used.},  
  keywords={},  
  doi={10.1109/TSE.2011.26}, 
  ISSN={1939-3520},
  month={March},
}

@INPROCEEDINGS{vandersype2014_on_lawful_disclosure_of_personal_user_data_what_should_app_developers_do,  
    author={Van Der Sype, Yung Shin and Maalej, Walid},  
    booktitle={2014 IEEE 7th International Workshop on Requirements Engineering and Law (RELAW)}, 
    title={On lawful disclosure of personal user data: What should app developers do?},   
    year={2014},
    volume={},  
    number={},  
    pages={25-34},  
    abstract={
        The proliferation of mobile devices and apps together with the increasing public interest in privacy and data protection matters necessitate a more careful precaution for legal compliance. As apps are becoming more popular, app developers can expect an increased scrutiny of privacy practices in the future. In this paper, we focus on the problem of the disclosure of personal data to third parties and the role of app developers to enhance user privacy and data protection in the app ecosystem. We discuss the EU data protection principles and apply them to the mobile app context. We then derive requirements and guidelines for app developers on how to contribute to the protection of their users' data.
    },  
    keywords={}, 
    doi={10.1109/RELAW.2014.6893479}, 
    ISSN={},  
    month={Aug},
}

@article{vivek2012_r3_repeatability_reproducibility_and_rigor,
    author = {Vitek, Jan and Kalibera, Tomas},
    title = {R3: Repeatability, Reproducibility and Rigor},
    year = {2012},
    issue_date = {April 2012},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {47},
    number = {4a},
    issn = {0362-1340},
    url = {https://doi.org/10.1145/2442776.2442781},
    doi = {10.1145/2442776.2442781},
    abstract = {Computer systems research spans sub-disciplines that include embedded systems, programming languages and compilers, networking, and operating systems. Our contention is that a number of structural factors inhibit quality systems research. We highlight some of the factors we have encountered in our own work and observed in published papers and propose solutions that could both increase the productivity of researchers and the quality of their output.},
    journal = {SIGPLAN Not.},
    month = mar,
    pages = {30–36},
    numpages = {7}
}

@article{wang2011_enabling_public_accountability_and_data_dynamics_etc,  
  author={Wang, Qian and Wang, Cong and Ren, Kui and Lou, Wenjing and Li, Jin}, 
  journal={IEEE Transactions on Parallel and Distributed Systems}, 
  title={Enabling Public Auditability and Data Dynamics for Storage Security in Cloud Computing},  
  year={2011},
  volume={22},
  number={5}, 
  pages={847-859},
  abstract={
    Cloud Computing has been envisioned as the next-generation architecture of IT Enterprise. It moves the application software and databases to the centralized large data centers, where the management of the data and services may not be fully trustworthy. This unique paradigm brings about many new security challenges, which have not been well understood. This work studies the problem of ensuring the integrity of data storage in Cloud Computing. In particular, we consider the task of allowing a third party auditor (TPA), on behalf of the cloud client, to verify the integrity of the dynamic data stored in the cloud. The introduction of TPA eliminates the involvement of the client through the auditing of whether his data stored in the cloud are indeed intact, which can be important in achieving economies of scale for Cloud Computing. The support for data dynamics via the most general forms of data operation, such as block modification, insertion, and deletion, is also a significant step toward practicality, since services in Cloud Computing are not limited to archive or backup data only. While prior works on ensuring remote data integrity often lacks the support of either public auditability or dynamic data operations, this paper achieves both. We first identify the difficulties and potential security problems of direct extensions with fully dynamic data updates from prior works and then show how to construct an elegant verification scheme for the seamless integration of these two salient features in our protocol design. In particular, to achieve efficient data dynamics, we improve the existing proof of storage models by manipulating the classic Merkle Hash Tree construction for block tag authentication. To support efficient handling of multiple auditing tasks, we further explore the technique of bilinear aggregate signature to extend our main result into a multiuser setting, where TPA can perform multiple auditing tasks simultaneously. Extensive security and performance analysis show that the proposed schemes are highly efficient and provably secure.},  
  keywords={},  
  doi={10.1109/TPDS.2010.183},  
  ISSN={1558-2183},  
  month={May},
}

@article{warton2010_poisson_point_process_models_solve_the_pseudo_absence_problem_for_presence_only_data_in_ecology,
     ISSN = {19326157, 19417330},
     URL = {http://www.jstor.org/stable/29765559},
     abstract = {Presence-only data, point locations where a species has been recorded as being present, are often used in modeling the distribution of a species as a function of a set of explanatory variables—whether to map species occurrence, to understand its association with the environment, or to predict its response to environmental change. Currently, ecologists most commonly analyze presence-only data by adding randomly chosen "pseudo-absences" to the data such that it can be analyzed using logistic regression, an approach which has weaknesses in model specification, in interpretation, and in implementation. To address these issues, we propose Poisson point process modeling of the intensity of presences. We also derive a link between the proposed approach and logistic regression—specifically, we show that as the number of pseudo-absences increases (in a regular or uniform random arrangement), logistic regression slope parameters and their standard errors converge to those of the corresponding Poisson point process model. We discuss the practical implications of these results. In particular, point process modeling offers a framework for choice of the number and location of pseudo-absences, both of which are currently chosen by ad hoc and sometimes ineffective methods in ecology, a point which we illustrate by example.},
     author = {David I. Warton and Leah C. Shepherd},
     journal = {The Annals of Applied Statistics},
     number = {3},
     pages = {1383--1402},
     publisher = {Institute of Mathematical Statistics},
     title = {POISSON POINT PROCESS MODELS SOLVE THE "PSEUDO-ABSENCE PROBLEM" FOR PRESENCE-ONLY DATA IN ECOLOGY},
     urldate = {2022-07-01},
     volume = {4},
     year = {2010}
}



@article{weider1998_software_fault_prevention_in_coding_and_RCA,
  title={A software fault prevention approach in coding and root cause analysis},
  author={Weider, D Yu},
  journal={Bell Labs Technical Journal},
  volume={3},
  number={2},
  pages={3--21},
  year={1998},
  publisher={Nokia Bell Labs}
}

@techreport{wichmann2007software,
  title={Software Support for Metrology Best Practice Guide No. 1},
  author={Wichmann, Brian and Parkin, Graeme and Barker, Robin},
  journal={Validation of Software in Measurement Systems},
  year={2007},
  institution={{National Physical Laboratory (UK)}},
  url = {http://eprintspublications.npl.co.uk/3922/},
  pages = {1--43},
}

@article{WIEDENBECK1986_beacons_in_computer_program_comprehension,
    title = {Beacons in computer program comprehension},
    journal = {International Journal of Man-Machine Studies},
    volume = {25},
    number = {6},
    pages = {697-709},
    year = {1986},
    issn = {0020-7373},
    doi = {https://doi.org/10.1016/S0020-7373(86)80083-9},
    url = {https://www.sciencedirect.com/science/article/pii/S0020737386800839},
    author = {Susan Wiedenbeck},
    abstract = {
        In programming, beacons are lines of code which serve as typical indicators of a particular structure or operation. This research sought evidence for the existence and use of beacons in comprehension of a sort program. In the first experiment, subjects memorized and later recalled the whole sort program. Experienced programmers, but not novices or intermediates, recalled the beacon lines much better than non-beacon lines. In the second experiment, experienced programmers studied the same program and then were asked to recall several isolated parts of it. They did not know in advance that they would be asked to recall. Subjects recalled the beacon much better than non-beacon parts. They also were more certain that they recalled the beacon correctly. The results of both experiments support the idea that beacons exist as a focal point for study and understanding of programs by experienced programmers.
    }
}

@ARTICLE{winter2022_lets_talk_with_developers_etc_automatic_program_repair,  
    author={Winter, Emily Rowan and Nowack, Vesna and Bowes, David and Counsell, Steve and Hall, Tracy and Haraldsson, Saemundur O and Woodward, John},  
    journal={IEEE Transactions on Software Engineering},   
    title={Let's Talk With Developers, Not About Developers: A Review of Automatic Program Repair Research},   
    year={2022},  
    volume={},  
    number={},  
    pages={1-1},  
    abstract={
        Automatic program repair (APR) offers significant potential for automating some coding tasks. Using APR could reduce the high costs historically associated with fixing code faults and deliver significant benefits to software engineering. Adopting APR could also have profound implications for software developers daily activities, transforming their work practices. To realise the benefits of APR it is vital that we consider how developers feel about APR and the impact APR may have on developers' work. Developing APR tools without consideration of the developer is likely to undermine the success of APR deployment. In this paper, we critically review how developers are considered in APR research by analysing how human factors are treated in 260 studies from Monperrus's Living Review of APR. Over half of the 260 studies in our review were motivated by a problem faced by developers (e.g., the difficulty associated with fixing faults). Despite these human-oriented motivations, fewer than 7\% of the 260 studies included a human study. We looked in detail at these human studies and found their quality mixed (for example, one human study was based on input from only one developer). Our results suggest that software developers are often talked about in APR studies, but are rarely talked with. A more comprehensive and reliable understanding of developer human factors in relation to APR is needed. Without this understanding, it will be difficult to develop APR tools and techniques which integrate effectively into developers' workflows. We recommend a future research agenda to advance the study of human factors in APR.
    },  
    keywords={},  
    doi={10.1109/TSE.2022.3152089},  
    ISSN={1939-3520},  
    month={},
}


@article{not_yet_cited_zhan2021_research_on_thirdparty_libraries_in_android_apps_an_SLR_etc,
  author={Zhan, Xian and Liu, Tianming and Fan, Lingling and Li, Li and Chen, Sen and Luo, Xiapu and Liu, Yang},
  journal={IEEE Transactions on Software Engineering}, 
  title={Research on Third-Party Libraries in Android Apps: A Taxonomy and Systematic Literature Review},  
  year={2021},  
  volume={},  
  number={},  
  pages={1--32},  
  abstract={
    Third-party libraries (TPLs) have been widely used in mobile apps, which play an essential part in the entire Android ecosystem. However, TPL is a double-edged sword. On the one hand, it can ease the development of mobile apps. On the other hand, it also brings security risks such as privacy leaks or increased attack surfaces (e.g., by introducing over-privileged permissions) to mobile apps. Although there are already many studies for characterizing third-party libraries, including automated detection, security and privacy analysis of TPLs, TPL attributes analysis, etc., what strikes us odd is that there is no systematic study to summarize those studies' endeavors. To this end, we conduct the first systematic literature review on Android TPL-related research. Following a well-defined systematic literature review protocol, we collected 74 primary research papers closely related to Android third-party library from 2012 to 2020. After carefully examining these studies, we designed a taxonomy of TPL-related research studies and conducted a systematic study to summarize current solutions, limitations, challenges and possible implications of new research directions related to third-party library analysis. We hope that these contributions can give readers a clear overview of existing TPL-related studies and inspire them to go beyond the current status quo by advancing the discipline with innovative approaches.
  },
  keywords={},
  doi={10.1109/TSE.2021.3114381}, 
  ISSN={1939-3520}, 
  month={},
}

@ARTICLE{zeller2002_simplifying_and_isolating_fault_inducing_input, 
    author={Zeller, A. and Hildebrandt, R.}, 
    journal={IEEE Transactions on Software Engineering}, 
    title={Simplifying and isolating failure-inducing input},
    year={2002}, 
    volume={28},
    number={2}, 
    pages={183-200}, 
    abstract={
        Given some test case, a program fails. Which circumstances of the test case are responsible for the particular failure? The delta debugging algorithm generalizes and simplifies the failing test case to a minimal test case that still produces the failure. It also isolates the difference between a passing and a failing test case. In a case study, the Mozilla Web browser crashed after 95 user actions. Our prototype implementation automatically simplified the input to three relevant user actions. Likewise, it simplified 896 lines of HTML to the single line that caused the failure. The case study required 139 automated test runs or 35 minutes on a 500 MHz PC.
    }, 
    keywords={}, 
    doi={10.1109/32.988498}, 
    ISSN={1939-3520}, 
    month={Feb},
}


@article{zeng2019studying_logging_practices_fdroid,
  title={Studying the characteristics of logging practices in mobile apps: a case study on f-droid},
  author={Zeng, Yi and Chen, Jinfu and Shang, Weiyi and Chen, Tse-Hsun Peter},
  journal={Empirical Software Engineering},
  volume={24},
  number={6},
  pages={3394--3434},
  year={2019},
  publisher={Springer}
}

@article{zou2016_uilog,
  title={Uilog: Improving log-based fault diagnosis by log analysis},
  author={Zou, De-Qing and Qin, Hao and Jin, Hai},
  journal={Journal of computer science and technology},
  volume={31},
  number={5},
  pages={1038--1052},
  year={2016},
  publisher={Springer},
  doi = {10.1007/s11390-016-1678-7},
  url = {https://doi.org/10.1007/s11390-016-1678-7},
  abstract = {In modern computer systems, system event logs have always been the primary source for checking system status. As computer systems become more and more complex, the interaction between software and hardware increases frequently. The components will generate enormous log information, including running reports and fault information. The sheer quantity of data is a great challenge for analysis relying on the manual method. In this paper, we implement a management and analysis system of log information, which can assist system administrators to understand the real-time status of the entire system, classify logs into different fault types, and determine the root cause of the faults. In addition, we improve the existing fault correlation analysis method based on the results of system log classification. We apply the system in a cloud computing environment for evaluation. The results show that our system can classify fault logs automatically and effectively. With the proposed system, administrators can easily detect the root cause of faults.},
  some_intersting_citations_on_log_design = {https://scholar.google.com/scholar?cites=6588326960360905008&as_sdt=2005&sciodt=0,5&hl=en},
}

@audio{harford2021_more_or_less_same_data_opposite_results_can_we_trust_research,
  entrysubtype   = {podcast},
  title = {Same data, opposite results. Can we trust research?},
  url = {https://www.bbc.co.uk/programmes/p0b2hfhh},
  alternate_url = {https://podcasts.apple.com/gb/podcast/same-data-opposite-results-can-we-trust-research/id267300884?i=1000541021853},
  author = {Tim Harford and Martin Schweinsberg},
  booktitle = {More or Less: Behind the Stats},
  publisher = {{BBC}},
  day = {7},
  month = {11},
  year = {2021},
  abstract = {
    When Professor Martin Schweinsberg found that he was consistently reaching different conclusions to his peers, even with the same data, he wondered if he was incompetent. So he set up an experiment.
    What he found out emphasises the importance of the analyst, but calls into question the level of trust we can put into research.
    Features an excerpt from TED Talks
  },
  notes = {
    The audio was also transcribed using Amazon's transcription service.
  }
}

@book{aurini2016_how_to_of_qualitative_research,
  title={The How To of Qualitative Research: Strategies for Executing High Quality Projects},
  author={Aurini, J.D. and Heath, M. and Howells, S.},
  url={https://books.google.co.uk/books?id=q-EODAAAQBAJ},
  year={2016},
  publisher={SAGE Publications},
  isbn = {978-1-4462-6709-7},
  address = {Los Angeles, USA},
}


@book{benioff_trailblazer_2019,
  title = {Trailblazer: The Power of Business as the Greatest Platform for Change},
  author = {Benioff, Marc and Langley, Monica},
  year = {2019},
  publisher = {Simon and Schuster UK},
  address = {London, UK},
  ISBN = {9781984825193},
}

@book{bounegru2021_the_data_journalism_handbook,
    author = {},
    editor = {Liliana Bounegru and Jonathan Gray},
    doi = {10.1515/9789048542079},
    url = {https://doi.org/10.1515/9789048542079},
    title = {The Data Journalism Handbook: Towards A Critical Data Practice},
    year = {2021},
    publisher = {Amsterdam University Press},
    ISBN = {9789048542079},
    address = {Amsterdam, Netherlands},
}


@book{chung2000_non_functional_requirements_in_software_engineering,
  title={Non-functional requirements in software engineering},
  author={Chung, Lawrence and Nixon, Brian A and Yu, Eric and Mylopoulos, John},
  volume={5},
  year={2000},
  publisher={Kluwer Academic Publishers},
  isbn = {0780792386667},
  address = {Norwell, MA, USA},
}

@book{clough2012_students_guide_to_methodology,
  title={A Student's Guide to Methodology},
  edition = {Third edition},
  author={Clough, Peter and Nutbrown, Cathy},
  year={2012},
  publisher={Sage},
  isbn = {978-1-4462-0861-8},
  pages = {288},
}

@book{davenport2010analytics_at_work,
  title={Analytics at work: Smarter decisions, better results},
  author={Davenport, Thomas H and Harris, Jeanne G and Morison, Robert},
  year={2010},
  publisher={Harvard Business Press},
  address={Boston, MA, USA.},
}

@book{davenport2017competing_on_analytics,
  title={Competing on analytics: Updated, with a new introduction: The new science of winning},
  author={Davenport, Thomas and Harris, Jeanne},
  year={2017},
  publisher={Harvard Business Press},
  address={Boston, MA, USA.},
}

@book{evans2004_achieving_software_quality_through_teamwork,
  title={Achieving software quality through teamwork},
  author={Evans, Isabel},
  year={2004},
  publisher={Artech House},
  pages = {324},
  ISBN = {9781580536622},
  address = {Norwood, MA 02062, USA.}
}

@book{flick2018_an_introduction_to_qualitative_research_sixth_ed,
  title = {An Introduction to Qualitative Research},
  edition = {6},
  author = {Uwe Flick},
  publisher = {SAGE Publishing},
  year = {2018},
  pages = {696},
  isbn = {9781526445650},
  address = {London, England},
}

@book{goldratt2017_necessary_but_not_sufficient,
  title={Necessary but not sufficient: a theory of constraints business novel},
  author={{Eliyahu M. Goldratt with Eli Schragenheim and Carol A. Ptak}},
  year={2000},
  publisher={Routledge},
  address = {Great Barrington, MA, USA},
  isbn = {978-0884271703},
}

@book{gray2012_the_data_journalism_handbook,
  title={The data journalism handbook: How journalists can use data to improve the news},
  author={Gray, Jonathan and Chambers, Lucy and Bounegru, Liliana},
  year={2012},
  publisher={"O'Reilly Media, Inc."},
  address = {Sebastopol, CA, USA},
}

@book{gunther2013truth_about_better_decision_making,
  title={The Truth about Better Decision-making (collection)},
  author={Gunther, Robert E},
  year={2008},
  publisher={Pearson Education Limited},
  ISBN = {978-0-273-71806-2},
  address = {Harlow, England},
}

@book{harford2021_the_data_detective,
  title = {The Data Detective: Ten Easy Rules to Make Sense of Statistics},
  author = {Tim Harford},
  publisher = {Riverhead Books},
  isbn = {978-0593084595},

  interesting_amazon_review = {
  https://www.amazon.co.uk/gp/product/0593084594/ref=ox_sc_act_title_2?smid=AHRB2OK2Q2YCL&psc=1
  Trey Shipp, 2 February 2021
  Darrell Huff's classic book, How to Lie with Statistics, warns us not to get misled by statistics. In this book, Tim Harford tells us how to see the Truth with statistics. He gives us ten rules of thumb for thinking about reported numbers. But the real reason for reading the book is for all of the stories Harford tells to illustrate each rule. It's like the greatest hits from the BBC radio show he has hosted for years.

  Here is a simple summary of the ten rules:

    1. When considering new information, pay attention to how it makes you feel. Your emotions can influence you to dismiss accurate statistics that you do not like and to embrace false statistics that you do like
    2. Sometimes your personal experience (a worm's eye view) conflicts with a bird's-eye view statistic. For example, the subway may be only half full on average during the day but packed every time you ride it (during rush hour). Both perspectives help you understand the truth.
    3. Make sure you understand what is being counted. When counting beans, the definition of a bean matters.
    4. Look for information that can put a statistic into context, like the trend, the scale, or how it compares to other situations.
    5. Try to learn where the statistics came from (the backstory) – and what other data might have vanished into obscurity.
    6. Ask who is missing from the data, and would our conclusions differ if they were included.
    7. Ask tough questions about algorithms and the big datasets that drive them, recognizing that without intelligent openness they cannot be trusted.
    8. Pay more attention to the bedrock of official statistics – and the sometimes heroic statisticians who protect it.
    9. Look under the surface of any beautiful graph or chart. Don't let the beauty mislead you.
    10. Keep an open mind, asking how we might be mistaken and whether the facts have changed.

  Those ten tips sound boring, but Harford's stories are not. Most of them show the power of useful statistics. As Harford says: "Good statistics are not smoke and mirrors; in fact, they help us see more clearly. Good statistics are like a telescope for an astronomer, a microscope for a bacteriologist, or an X-ray for a radiologist."
  }
}

@book{harty_aymer_playbook_2016,
	edition = {2},
	title = {The {Mobile} {Analytics} {Playbook}},
	isbn = {978-0-9970694-0-2},
	url = {http://www.themobileanalyticsplaybook.com/},
	urldate = {2017-10-02},
	publisher = {HP Enterprises},
	author = {Harty, Julian and Aymer, Antoine},
	month = {Jan},
	year = {2016},
	address = {High Wycombe, UK},
}

@book{hodgkin1992_chance_and_design,
  title = {Chance and Design},
  author = {Alan Hodgkin},
  publisher = {Cambridge University Press},
  pages = {390},
  isbn = {0-521-40099-6},
}

@book{hubbard2014measure,
  title={How to Measure Anything: Finding the Value of Intangibles in Business, Third Edition},
  author={Hubbard, Douglas W},
  year={2014},
  publisher={John Wiley \& Sons},
  isbn = {978-1-118-53927-9},
  address = {Hoboken, New Jersey, USA},
}

@book{hutchins1995_cognition_in_the_wild,
  title={Cognition in the Wild},
  author={Hutchins, Edwin},
  year={1995},
  publisher={MIT press},
  pages = {402},
  isbn = {9780262082310},
  address = {USA},
}

@book{jain1991art,
  title={The Art Of Computer Systems Performance Analysis: Techniques For Experimental Measurement, Simulation, And Modeling},
  author={Jain, Raj},
  year={1991},
  publisher={john wiley \& sons},
  address = {New York, NY, USA.},
  isbn={978-0-471-50336-1}
}

@book{lepore1999_deming_and_goldratt,
  title = {Demin and Goldratt},
  author = {Domenico Lepore and Oded Cohen},
  year = {1999},
  publisher = {North River Press},
  isbn = {0-88427-163-3},
  address = {Great Barrington, MA, USA},
}

@book{marr2015bigdatabook,
  title={Big Data: Using SMART big data, analytics and metrics to make better decisions and improve performance},
  author={Marr, Bernard},
  year={2015},
  publisher={John Wiley \& Sons},
  address = {Chichester, United Kingdom}
}

@book{mill1884system,
  title={A system of logic, ratiocinative and inductive: Being a connected view of the principles of evidence and the methods of scientific investigation},
  author={Mill, John Stuart},
  volume={1},
  edition={8},
  year={1884},
  publisher={Longmans, Green, and Company},
  address={New York, NY, USA.},
  url={ftp://mirrors.xmission.com/gutenberg/2/7/9/4/27942/27942-pdf.pdf}
}

@book{phadke1995_quality_engineering_using_robust_design,
  title={Quality engineering using robust design},
  author={Phadke, Madhan Shridhar},
  year={1995},
  publisher={Prentice Hall PTR},
  address = {New Jersey, USA},
  ISBN = {0-13-745167-9}
}

https://www.springer.com/gb/book/9783319240442
@book{quality_control_in_R_book,
  title = {Quality Control in R},
  ISBN = {978-3-319-24044-2},
  year = {2015},
  author={Emilio L. Cano, Javier M. Moguerza, Mariano Prieto Corcoba},
  publisher = {Springer Nature},
  address = {Switzerland},
}

@book{riedesel2021_software_telemetry,
  title = {Software Telemetry},
  author = {Jamie Riedesel},
  year = {2021},
  publisher = {Manning},
  isbn = {9781617298141},
  address = {Shelter Island, NY, USA},
  url = {https://www.manning.com/books/software-telemetry},
}

// Actually a court filing, recommended to create a book to cite it
https://latex.org/forum/viewtopic.php?t=32485
@book{rodriguez_et_al_v_google_llc_et_al_2020,
  title = {{20-CV-04688}},
  author = {{Rodriguez et al. v. Google LLC et al.}},
  year = {2020},
  publisher = {{UNITED STATES DISTRICT COURT NORTHERN DISTRICT OF CALIFORNIA}},
  address = {California, USA},
}

@book{rochkind1985_advanced_unit_programming,
  title={Advanced UNIX programming},
  author={Rochkind, Marc J},
  journal={Prentice Hall Software Series},
  year={1985},
  isbn={0-13-011800-1},
  publisher = {Prentice-Hall},
  address = {Englewood Cliffs, N.J. 07632},
  pages = {256},
}

@book{rudder2014dataclysm,
  title={Dataclysm: Love, Sex, Race, and Identity--What Our Online Lives Tell Us about Our Offline Selves},
  author={Rudder, Christian},
  year={2014},
  publisher={4th Estate},
  isbn = {978-0-00-749443-9},
  address = {London},
}

@book{rugg2006_gentle_guide_to_research_methods,
  title={A gentle guide to research methods},
  author={Rugg, Gordon and Petre, Marian},
  year={2006},
  publisher={McGraw-Hill Education (UK)},
  isbn = {978-0-335-21927-8},
  pages = {238},
}

@book{scheinkopf1999_thinking_for_a_change,
  title = {Thinking for a Change: Putting the TOC Thinking Processes to Use},
  author = {Lisa J. Scheinkopf},
  year = {1999},
  publisher = {CRC Press},
  address = {Boca Raton, FL, USA},
  isbn = {978-1574441017},
}

@book{sommerville1989_software_engineering,
  title={Software engineering - third edition},
  author={Sommerville, Ian},
  year={1989},
  publisher={Addison-Wesley Publishers Ltd.},
  address = {Wokingham, England},
  isbn={0-201-17568-1},
}

@book{trafford2008_stepping_stones_to_achieving_your_doctorate,
  title={Stepping stones to achieving your doctorate: By focusing on your viva from the start: Focusing on your viva from the start},
  author={Trafford, Vernon and Leshem, Shosh},
  year={2008},
  publisher={McGraw-Hill Education (UK)},
  isbn = {978-0-335225439},
  address = {New York, NY, USA},
}

@book{walz2015_apptentive_the_mobile_marketers_guide_to_app_store_ratings_and_reviews,
  title = {The Mobile Marketer's Guide to App Store Ratings and Reviews},
  author = {Alex Walz and Robi Ganguly},
  year = {2015},
  publisher = {{Apptentive}},
  pages = {55},
}

@book{weinberg1992quality,
  title={Quality software management (Vol. 1) systems thinking},
  author={Weinberg, Gerald M},
  year={1992},
  publisher={Dorset House Publishing Co., Inc.},
  address = {New York, NY, USA},
}

@book{weinberg2006weinberg,
  title={Weinberg on Writing: The Fieldstone Method},
  author={Weinberg, Gerald M},
  abstract = {Most people never publish an article. Of those who do publish an article, most write only one. Most people never publish a report. Of those who do publish a report, most write only one. Most people never publish a book. Of those who do publish a book, most write only one. Most people never publish a script. Of those who do publish a script, most write only one. If you ask them why they don’t write more, they will say they are stuck, or “blocked.” But these are just labels, and explain nothing. Most often, they stop writing because they don’t know how to work with the essential randomness involved in the creative process.},
  year={2006},
  publisher={Dorset House Pub.}
}

@book{yin2018_case_study_research_and_applications_6th_edition,
  publisher = {SAGE Publications, Inc; Sixth edition},
  title = {Case Study Research and Applications - Design and Methods},
  edition = {\nth{6} edition},
  year = {2018},
  pages = {352},
  ISBN = {978-1506336169},
  address = {London},
  author = {Robert K. Yin},
}

@book{zeller2009_why_programs_fail,
  title = {Why Programs Fail 2nd Edition - A Guide to Systematic Debugging},
  edition = {2},
  year = {2009},
  author = {Andreas Zeller},
  ISBN = {9780123745156},
  doi = {10.1016/B978-0-12-374515-6.X0000-7},
  publisher = {{Elsevier Inc.}},
  address = {Burlington, MA 01803, USA},
}

@inbook{banks2010_blog_posts_and_tweets_the_next_frontier_for_grey_literature,
    author = {Marcus Banks},
    x_editor = {Dominic J. Farace and Joachim Schöpfel},
    doi = {10.1515/9783598441493.2.217},
    url = {https://doi.org/10.1515/9783598441493.2.217},
    title = {Chapter 14. Blog Posts and Tweets: The Next Frontier for Grey Literature},
    booktitle = {Grey Literature in Library and Information Studies},
    year = {2010},
    publisher = {K. G. Saur},
    pages = {217--226},
    address = {Berlin, Germany},
}

@inbook{birani2016_hockey_app_for_crash_reporting,
  title = {Hockey app for crash reporting},
  booktitle = {iOS Forensics Cookbook},
  author = {Bhanu Birani and Mayank Birani},
  year = {2016},
  isbn = {9781783988464},
  pages = {85--88},
  publisher = {Packt},
  abstract = {
    Mobile device forensics is a branch of digital forensics that involves the recovery of evidence or data in a digital format from a device without affecting its integrity. With the growing popularity of iOS-based Apple devices, iOS forensics has developed immense importance.
    
    To cater to the need, this book deals with tasks such as the encryption and decryption of files, various ways to integrate techniques withsocial media, and ways to grab the user events and actions on the iOS app. Using practical examples, we’ll start with the analysis keychain and raw disk decryption, social media integration, and getting accustomed to analytics tools. You’ll also learn how to distribute the iOS apps without releasing them to Apple’s App Store. Moving on, the book covers test flights and hockey app integration, the crash reporting system, recovery tools, and their features. By the end of the book, using the aforementioned techniques, you will be able to successfully analyze iOS-based devices forensically.
  }
}

@inbook{flick2004_triangulation_in_qualitative_research,
  editor = {Uwe Flick and Ernst von Kardorff and Ines Steinke},
  title = {A Companion to Qualitative Research},
  chapter={Triangulation in qualitative research},
  author={Flick, Uwe},
  journal={A companion to qualitative research},
  pages={178--183},
  year={2004},
  publisher={{Sage Publications}},
  address = {London, England},
  isbn = {0761973753},
}

@inbook{hu2018_a_tale_of_two_cities_how_webview_introduces_bugs_to_android_applications,
    author = {Hu, Jiajun and Wei, Lili and Liu, Yepang and Cheung, Shing-Chi and Huang, Huaxun},
    title = {A Tale of Two Cities: How WebView Induces Bugs to Android Applications},
    year = {2018},
    isbn = {9781450359375},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3238147.3238180},
    abstract = {
      WebView is a widely used Android component that augments a native app with web browser capabilities. It eases the interactions between an app’s native code and web code. However, the interaction mechanism of WebView induces new types of bugs in Android apps. Understanding the characteristics and manifestation of these WebView-induced bugs (ωBugs for short) facilitates the correct usages of WebViews in Android apps. This motivates us to conduct the first empirical study on ωBugs based on those found in popular open-source Android apps. Our study identified the major root causes and consequences of ωBugs and made interesting observations that can be leveraged for detecting and diagnosing ωBugs. Based on the empirical study, we further propose an automated testing technique ωDroid to effectively expose ωBugs in Android apps. In our experiments, ωDroid successfully discovered 30 unique and previously-unknown ωBugs when applied to 146 open-source Android apps. We reported the 30 ωBugs to the corresponding app developers. Out of these 30 ωBugs, 14 were confirmed and 7 of them were fixed. This shows that ωDroid can effectively detect ωBugs that are of the developers’ concern.
    },
    booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
    pages = {702–713},
    numpages = {12}
}

@inbook{li2020_experience_or_aging_why_does_android_stop_responding_and_what_can_we_do_about_it,
    author = {Li, Mingliang and Lin, Hao and Liu, Cai and Li, Zhenhua and Qian, Feng and Liu, Yunhao and Sun, Nian and Xu, Tianyin},
    title = {Experience: Aging or Glitching? Why Does Android Stop Responding and What Can We Do about It?},
    year = {2020},
    isbn = {9781450370851},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3372224.3380897},
    abstract = {Almost every Android user has unsatisfying experiences regarding responsiveness, in particular Application Not Responding (ANR) and System Not Responding (SNR) that directly disrupt user experience. Unfortunately, the community have limited understanding of the prevalence, characteristics, and root causes of unresponsiveness. In this paper, we make an in-depth study of ANR and SNR at scale based on fine-grained system-level traces crowdsourced from 30,000 Android systems. We find that ANR and SNR occur prevalently on all the studied 15 hardware models, and better hardware does not seem to relieve the problem. Moreover, as Android evolves from version 7.0 to 9.0, there are fewer ANR events but more SNR events. Most importantly, we uncover multifold root causes of ANR and SNR and pinpoint the largest inefficiency which roots in Android's flawed implementation of Write Amplification Mitigation (WAM). We design a practical approach to eliminating this largest root cause; after large-scale deployment, it reduces almost all (&gt;99\%) ANR and SNR caused by WAM while only decreasing 3\% of the data write speed. In addition, we document important lessons we have learned from this study, and have also released our measurement code/data to the research community.},
    booktitle = {Proceedings of the 26th Annual International Conference on Mobile Computing and Networking},
    articleno = {20},
    numpages = {11},
    extracts = {
        We invited the active users in Xiaomi’s smartphone community to participate in our measurement study by installing Android-MOD on their phones. Over 30,000 users opted in and collected data for us for three weeks, involving 15 different models of Android phones. All the data are collected with informed consent of opt-in users, and no personally identifiable information (PII) was collected.
    },
    thoughts = {
        This is a helpful paper that demonstrates the power of crowdsourcing and on gathering reliability related data from Android device logs to measure and then improve the reliability. They addressed a common cause of ANRs and SNRs where the OS writes to a memory card over time where the Android OS algorithm was causing problems some of the time on some device models. The authors were able to convince Google to implement a joint improvement to Android. The reliability is much better for the users who received this patch. What's not known is the extent their work, and the fix that Google subsequently applied, has on the Android userbase afterwards (after 2018/2019). In theory it should be possible to see the difference for popular apps that run on Android devices that pre and post date the authors' work. 
        
        The authors claim they reduced the ANRs caused by the WAM by more than 99\% and the overall frequency of ANRs by 32\%.
        
        Their focus was on the behaviour of Android and how to improve that behaviour in terms of responsiveness, etc. They did not investigate the app-specific causes of ANRs (which comprised 26\% "app-specific defects (26\%) [p. 1]."
    },
    keywords = {Android; Responsiveness; Application Not Responding (ANR); System Not Responding (SNR); Write Amplification Mitigation (WAM).},
}

@inbook{MacLean2015_pro_android_5_book,
  author="MacLean, Dave and Komatineni, Satya and Allen, Grant",
  title="Deploying Your Application: Google Play Store and Beyond",
  bookTitle="Pro Android 5",
  year="2015",
  publisher="Apress",
  address="Berkeley, CA",
  pages="677--696",
  abstract="Creating a great application that people will love is one thing, but you also need an easy way for people to find and download it. Google created the Play Store for this purpose. From an icon right on the device, users can click straight into the Play Store to browse, search, review, and download applications. Users can also access Play Store over the Internet to do those same things, although the downloading is not to the computer but rather apps are sent directly to the user's device. Many applications are free; for those that are not, the Play Store provides payment mechanisms for easy purchasing.",
  isbn="978-1-4302-4681-7",
  doi="10.1007/978-1-4302-4681-7_30",
  url="https://doi.org/10.1007/978-1-4302-4681-7_30",
  quotes = {
    If you really want to know how a user got to a crash, you’ll want to implement one of the mobile analytics packages into your app. These will generate event records as a user steps through your application, and will also report crashes. The breadcrumbs (event records) will let you know the steps a user took up to the point of the crash. This capability is separate from the Google Play Store, however.
  }
}

@inbook{yet_to_cite_10.1145/1134285.1134349,
    author = {Nagappan, Nachiappan and Ball, Thomas and Zeller, Andreas},
    title = {Mining Metrics to Predict Component Failures},
    year = {2006},
    isbn = {1595933751},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/1134285.1134349},
    abstract = {What is it that makes software fail? In an empirical study of the post-release defect
    history of five Microsoft software systems, we found that failure-prone software entities
    are statistically correlated with code complexity measures. However, there is no single
    set of complexity metrics that could act as a universally best defect predictor. Using
    principal component analysis on the code metrics, we built regression models that
    accurately predict the likelihood of post-release defects for new entities. The approach
    can easily be generalized to arbitrary projects; in particular, predictors obtained
    from one project can also be significant for new, similar projects.},
    booktitle = {Proceedings of the 28th International Conference on Software Engineering},
    pages = {452–461},
    numpages = {10}
}

@inbook{salkind_encyclopedia_2021_natural_experiment,
    title = {Natural Experiments},
	booktitle = {Encyclopedia of Research Design},
	url = {https://methods.sagepub.com/reference/encyc-of-research-design},
	doi = {10.4135/9781412961288},
	author = {O'Connor, Thomas G.},
	__editor = {Salkind, Neil},
	date = {2021-11-12},
	year = {2012},
	pages = {877--880},
	publisher = {{SAGE Publications, Inc.}},
	address = {Thousand Oaks, California},
    isbn =  9781412961288,
    extracts = {
      A key feature of natural experiments is that they offer insight into causal processes, which is one reason why they have an established role in developmental science.
      
      Natural experiments are, therefore, particularly valuable where traditional nonexperimental designs might not be scientifically inadequate or where experimental designs may be practical or ethical. And, natural experiments are useful scientific tools even where other designs might be judged as capable of testing the hypothesis of interest. That is because of the need for findings to be confirmed not only by multiple studies but also by multiple designs. That is, natural experiments can provide a helpful additional scientific “check” on findings generated from naturalistic or experimental studies. There are many illustrations of the problems in relying on findings from a single design. Researchers are now accustomed to defining an effect or association as robust if it is replicated across samples and measures. Also, a finding should replicate across design. No single research sample and no single research design is satisfactory for testing causal hypotheses or inferring causal mechanisms.
    }
}

@Inbook{Wohlin2003_empirical_research_methods_in_software_engineering,
  author="Wohlin, Claes and H{\"o}st, Martin and Henningsson, Kennet",
  editorz="Conradi, Reidar and Wang, Alf Inge",
  title="Empirical Research Methods in Software Engineering",
  bookTitle="Empirical Methods and Studies in Software Engineering: Experiences from ESERNET",
  year="2003",
  publisher="Springer Berlin Heidelberg",
  address="Berlin, Heidelberg",
  pages="7--23",
  abstract="Software engineering is not only about technical solutions. It is to a large extent also concerned with organizational issues, project management and human behaviour. For a discipline like software engineering, empirical methods are crucial, since they allow for incorporating human behaviour into the research approach taken. Empirical methods are common practice in many other disciplines. This chapter provides a motivation for the use of empirical methods in software engineering research. The main motivation is that it is needed from an engineering perspective to allow for informed and well-grounded decision. The chapter continues with a brief introduction to four research methods: controlled experiments, case studies, surveys and post-mortem analyses. These methods are then put into an improvement context. The four methods are presented with the objective to introduce the reader to the methods to a level that it is possible to select the most suitable method at a specific instance. The methods have in common that they all are concerned with quantitative data. However, several of them are also suitable for qualitative data. Finally, it is concluded that the methods are not competing. On the contrary, the different research methods can preferably be used together to obtain more sources of information that hopefully lead to more informed engineering decisions in software engineering.",
  isbn="978-3-540-45143-3",
  doi="10.1007/978-3-540-45143-3_2",
  url="https://doi.org/10.1007/978-3-540-45143-3_2"
}


@incollection{yet_to_cite_easterbrook2008_selecting_empirical_methods_for_software_engineering_research,
	location = {London},
	title = {Selecting Empirical Methods for Software Engineering Research},
	isbn = {978-1-84800-044-5},
	url = {https://doi.org/10.1007/978-1-84800-044-5_11},
	abstract = {Selecting a research method for empirical software engineering research is problematic because the benefits and challenges to using each method are not yet well catalogued. Therefore, this chapter describes a number of empirical methods available. It examines the goals of each and analyzes the types of questions each best addresses. Theoretical stances behind the methods, practical considerations in the application of the methods and data collection are also briefly reviewed. Taken together, this information provides a suitable basis for both understanding and selecting from the variety of methods applicable to empirical software engineering.},
	pages = {285--311},
	booktitle = {Guide to Advanced Empirical Software Engineering},
	publisher = {Springer London},
	author = {Easterbrook, Steve and Singer, Janice and Storey, Margaret-Anne and Damian, Daniela},
	editor = {Shull, Forrest and Singer, Janice and Sjøberg, Dag I. K.},
	date = {2008},
	doi = {10.1007/978-1-84800-044-5_11}
}

@incollection{murphy_enabling_2019,
	address = {Berkeley, {CA}},
	title = {Enabling Productive Software Development by Improving Information Flow},
	isbn = {978-1-4842-4221-6},
	url = {https://doi.org/10.1007/978-1-4842-4221-6_24},
	abstract = {At its core, software development is an information-intensive knowledge generation and consumption activity. We are interested in how software tools can enable the productive development of software. Our hypothesis has been that software development productivity can be increased by improving the access and flow of information between the humans and tools involved in creating software systems. In this chapter, we review an evolution of technologies we have introduced based on this hypothesis. These technologies are in use by large software development organization and have been shown to improve software developer productivity. The description of these technologies also highlights how productivity can be considered at the individual, team and organizational level.},
	pages = {281--292},
	booktitle = {Rethinking Productivity in Software Engineering},
	publisher = {Apress},
	author = {Murphy, Gail C. and Kersten, Mik and Elves, Robert and Bryan, Nicole},
	editor = {Sadowski, Caitlin and Zimmermann, Thomas},
	year = {2019},
	doi = {10.1007/978-1-4842-4221-6_24},
}


@incollection{prechelt_2007_optimizing_ROI_for_empirical_SE_studies,
	location = {Berlin, Heidelberg},
	title = {Optimizing Return-On-Investment ({ROI}) for Empirical Software Engineering Studies Working Group Results},
	isbn = {978-3-540-71301-2},
	url = {https://doi.org/10.1007/978-3-540-71301-2_18},
	abstract = {Return-on-investment ({ROI}) is a concept from the financial world. In the dynamic view, {ROI} describes the periodically recurring profits (returns) from fixed financial capital (investment). In the static view, {ROI} describes the one-time income or saving (return) realized as a consequence of a one-time expenditure (investment). In this case, if the return does not occur within a short time, later parts of the return may be discounted for interest. For our purposes, we will use the static view and ignore discounting. We will call the return benefit and the investment cost.},
	pages = {54--57},
	booktitle = {Empirical Software Engineering Issues. Critical Assessment and Future Directions: International Workshop, Dagstuhl Castle, Germany, June 26-30, 2006. Revised Papers},
	publisher = {Springer Berlin Heidelberg},
	author = {Prechelt, Lutz},
	editor = {Basili, Victor R. and Rombach, Dieter and Schneider, Kurt and Kitchenham, Barbara and Pfahl, Dietmar and Selby, Richard W.},
	date = {2007},
	year = {2007},
	doi = {10.1007/978-3-540-71301-2_18},
	address = {Dagstuhl Castle, Germany},
}

@InProceedings{10.1007/978-3-319-49094-6_16,
author="Samuel, Triin
and Pfahl, Dietmar",
editor="Abrahamsson, Pekka
and Jedlitschka, Andreas
and Nguyen Duc, Anh
and Felderer, Michael
and Amasaki, Sousuke
and Mikkonen, Tommi",
title="Problems and Solutions in Mobile Application Testing",
booktitle="Product-Focused Software Process Improvement",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="249--267",
abstract="In recent years the amount of literature published about mobile application testing has significantly grown. However, it is unclear to what degree stated problems and proposed solutions are relevant to industry. To shed light on this issue, we conducted a literature survey to provide an overview of what current scientific literature considers problems and potential solutions in mobile application testing, and how often proposed solutions were reportedly evaluated in industry. Then we conducted a case study involving six software companies in Estonia to find out which of the problems are considered relevant by professionals, and which of the proposed solutions are considered novel and applicable. In total, we identified 49 potential problems or challenges in the mobile application testing domain and 39 potential solutions, some of which were implemented software tools while others were just theoretical concepts. Although some of the solutions were reportedly applied in practice, in most cases the literature did not give much information on the actual usage in industry of the proposed solutions. The case study revealed that while the relevance of each identified problem was highly variable from one company to another, there are some key problems that are generally considered vital both by research and industry. Regarding solution proposals, it turned out they are often described too much on the conceptual level or are too unrelated to the most urgent test-related problems of our case companies to be of interest to them.",
isbn="978-3-319-49094-6"
}

@inproceedings{10.1145/3239235.3267436,
    author = {Rodr\'{\i}guez-P\'{e}rez, Gema and Zaidman, Andy and Serebrenik, Alexander and Robles, Gregorio and Gonz\'{a}lez-Barahona, Jes\'{u}s M.},
    title = {What If a Bug Has a Different Origin? Making Sense of Bugs without an Explicit Bug Introducing Change},
    year = {2018},
    isbn = {9781450358231},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/3239235.3267436},
    doi = {10.1145/3239235.3267436},
    abstract = {Background: Many studies in the software research literature on bug fixing are built
    upon the assumption that "a given bug was introduced by the lines of code that were
    modified to fix it", or variations of it. Although this assumption seems very reasonable
    at first glance, there is little empirical evidence supporting it. A careful examination
    surfaces that there are other possible sources for the introduction of bugs such as
    modifications to those lines that happened before the last change an changes external
    to the piece of code being fixed. Goal: We aim at understanding the complex phenomenon
    of bug introduction and bug fix. Method: We design a preliminary approach distinguishing
    between bug introducing commits (BIC) and first failing moments (FFM). We apply this
    approach to Nova and ElasticSearch, two large and well-known open source software
    projects. Results: In our initial results we obtain that at least 24% bug fixes in
    Nova and 10% in ElasticSearch have not been caused by a BIC but by co-evolution, compatibility
    issues or bugs in external API. Merely 26--29% of BICs can be found using the algorithm
    based on the assumption that "a given bug was introduced by the lines of code that
    were modified to fix it". Conclusions: The approach allows also for a better framing
    of the comparison of automatic methods to find bug inducting changes. Our results
    indicate that more attention should be paid to whether a bug has been introduced and,
    when it was introduced.},
    booktitle = {Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
    articleno = {52},
    numpages = {4},
    keywords = {SZZ algorithm, bug-introducing change, empirical study},
    address = {Oulu, Finland},
    series = {ESEM '18}
}

@inproceedings{abreu2007_on_the_accuracy_of_spectrum_based_fault_localization,
  author={R. {Abreu} and P. {Zoeteweij} and A. J. C. {van Gemund}},
  booktitle={Testing: Academic and Industrial Conference Practice and Research Techniques - MUTATION (TAICPART-MUTATION 2007)},
  title={On the Accuracy of Spectrum-based Fault Localization},
  year={2007},
  volume={},
  number={},
  pages={89-98},
  abstract={
    Spectrum-based fault localization shortens the test- diagnose-repair cycle by reducing the debugging effort. As a light-weight automated diagnosis technique it can easily be integrated with existing testing schemes. However, as no model of the system is taken into account, its diagnostic accuracy is inherently limited. Using the Siemens Set benchmark, we investigate this diagnostic accuracy as a function of several parameters (such as quality and quantity of the program spectra collected during the execution of the system), some of which directly relate to test design. Our results indicate that the superior performance of a particular similarity coefficient, used to analyze the program spectra, is largely independent of test design. Furthermore, near- optimal diagnostic accuracy (exonerating about 80\% of the blocks of code on average) is already obtained for low-quality error observations and limited numbers of test cases. The influence of the number of test cases is of primary importance for continuous (embedded) processing applications, where only limited observation horizons can be maintained.},
  keywords={program debugging;program diagnostics;program testing;spectrum-based fault localization;test-diagnose-repair cycle;light-weight automated diagnosis technique;low-quality error observations;test data analysis;Software testing;Automatic testing;Fault diagnosis;Debugging;Computer industry;Benchmark testing;Failure analysis;Fault detection;Fault location;Mathematics},  
  doi={10.1109/TAIC.PART.2007.13},
  ISSN={},
  month={Sep},
  address = {Windsor, UK},
  publisher = {IEEE},
  organization = {IEEE},
}

@inproceedings{adams2016modern,
  title={Modern release engineering in a nutshell--why researchers should care},
  author={Adams, Bram and McIntosh, Shane},
  booktitle={2016 IEEE 23rd international conference on software analysis, evolution, and reengineering (SANER)},
  volume={5},
  pages={78--90},
  year={2016},
  organization={IEEE},
  publisher={IEEE},
  address={Suita, Japan},
  relevance_to_phd = {Release Engineering and Mobile App development have parallels.},
  quote_1 = { The main take-home message is that, while release engineering technology has flourished tremendously due to industry, empirical validation of best practices and the impact of the release engineering process on (amongst others) software quality is largely missing and provides major research opportunities.},
  quote_2 = {Even for the pioneers of modern release engineering, newer technologies like mobile apps still pose open challenges. Broader questions include: what will be the long-term effect on user-perceived quality of releases [43, 45], how quickly will technical debt ramp up when release cycles are so short and can end users keep up with a continuous stream of new releases?},
}

@inproceedings{adamsen2015systematic_catrobat,
  title={Systematic execution of {Android} test suites in adverse conditions},
  author={Adamsen, Christoffer Quist and Mezzetti, Gianluca and M{\o}ller, Anders},
  booktitle={Proceedings of the 2015 International Symposium on Software Testing and Analysis},
  pages={83--93},
  year={2015},
  publisher = {{ACM}},
  address = {Baltimore MD, USA},
}

@article{Amland_2000_rbt_financial_case_study, 
   title={Risk-based testing:: Risk analysis fundamentals and metrics for software testing including a financial application case study},
   volume={53},
   ISSN={0164-1212},
   DOI={https://doi.org/10.1016/S0164-1212(00)00019-4},
   abstractNote={
     The idea of risk-based testing is to focus testing and spend more time on critical functions. By combining the focused process with metrics it is possible to manage the test process by intelligent assessment and to communicate the expected consequences of decisions taken. This paper discusses an approach to risk-based testing and how risk-based testing was carried out in a large project in a financial institution. The paper concludes with how practical risk-based testing experience should inform theory and provide advice on organizational requirements that are necessary to achieve success.},
   number={3},
   journal={Journal of Systems and Software},
   author={Amland, Ståle},
   year={2000},
   pages={287–295}
}


@inproceedings{an2015_challenges_and_issues_of_mining_crash_reports,
  author={L. {An} and F. {Khomh}},  
  booktitle={2015 IEEE 1st International Workshop on Software Analytics (SWAN)},   
  title={Challenges and Issues of Mining Crash Reports},   
  year={2015},  
  volume={},  
  number={},  
  pages={5-8},  
  abstract={Automatic crash reporting tools built in many software systems allow software practitioners to understand the origin of field crashes and help them prioritise field crashes or bugs, locate erroneous files, and/or predict bugs and crash occurrences in subsequent versions of the software systems. In this paper, after illustrating the structure of crash reports in Mozilla, we discuss some techniques for mining information from crash reports, and highlight the challenges and issues of these techniques. Our aim is to raise the awareness of the research community about issues that may bias research results obtained from crash reports and provide some guidelines to address certain challenges related to mining crash reports.},  
  keywords={data mining;program debugging;program testing;crash report mining;crash reporting tool;software systems;Computer bugs;Data mining;Databases;Algorithm design and analysis;Software systems;Crash report;bug report;mining softwarerepositories.},  
  doi={10.1109/SWAN.2015.7070480},  
  ISSN={},  
  month={March},
}

@inproceedings{andrews1998_testing_using_log_file_analysis,
  title={Testing using log file analysis: tools, methods, and issues},
  author={Andrews, James H},
  booktitle={Proceedings 13th IEEE International Conference on Automated Software Engineering (Cat. No. 98EX239)},
  pages={157--166},
  year={1998},
  organization={IEEE},
  publisher={IEEE},
  address={Honolulu, HI, USA}
}

@inproceedings{avellis_harty_yu_towards_mobile_twin_peaks,
author = {Avellis, Giovanna and Harty, Julian and Yu, Yijun},
title = {Towards Mobile Twin Peaks for App Development},
year = {2017},
isbn = {9781538626696},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MOBILESoft.2017.10},
doi = {10.1109/MOBILESoft.2017.10},
booktitle = {Proceedings of the 4th International Conference on Mobile Software Engineering and Systems},
pages = {189–193},
numpages = {5},
keywords = {mobile analytics, ethnographic studies, app development, twin peaks, information retrieval},
address = {Buenos Aires, Argentina},
series = {MOBILESoft ’17}
}

@inproceedings{bagnato2020_challenges_and_benefits_from_using_software_analytics_in_softeam,
  title = {Challenges and Benefits from Using Software Analytics in Softeam},
  author = {Bagnato, Alessandra and Abherv{\'e}, Antonin and Mart{\'i}nez\-Fern{\'a}ndez, Silverio and Franch, Xavier},
  pages = {512},
  booktitle = {6th International Workshop on Rapid Continuous Software Engineering (RCoSE)},
  year = {2020},
  organization={IEEE},
  publisher = {IEEE},
  address = {Seoul, South Korea},  
}

@inproceedings{balagtas2009methodology,
  title={A methodology and framework to simplify usability analysis of mobile applications},
  author={Balagtas-Fernandez, Florence and Hussmann, Heinrich},
  booktitle={2009 IEEE/ACM International Conference on Automated Software Engineering},
  pages={520--524},
  year={2009},
  organization={IEEE},
  abstract = {Usability analysis is an important step in software development in order to improve certain aspects of the system. However, it is often a challenge especially when it comes to evaluating applications running on mobile devices because of the restrictions posed by the device and the lack of supporting tools and software available to collect the necessary usability data. This paper proposes a methodology and framework to aid developers in preparing the mobile system for usability analysis. The focus is on the simplification of the developer's task in preparing the system for evaluation and the processing of the collected usability data by automating some of the tasks involved in the process.},
}

@inproceedings{bishop1993variation,
  title={The variation of software survival time for different operational input profiles (or why you can wait a long time for a big bug to fail)},
  author={Bishop, Peter G},
  booktitle={FTCS-23 The Twenty-Third International Symposium on Fault-Tolerant Computing},
  pages={98--107},
  year={1993},
  organization={IEEE},
  publisher={IEEE},
  address={Toulouse, France},
}

@inproceedings{bohme2017_where_is_the_bug_and_how_is_it_fixed,
    author = {B\"{o}hme, Marcel and Soremekun, Ezekiel O. and Chattopadhyay, Sudipta and Ugherughe, Emamurho and Zeller, Andreas},
    title = {Where is the Bug and How is It Fixed? An Experiment with Practitioners},
    year = {2017},
    isbn = {9781450351058},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/3106237.3106255},
    doi = {10.1145/3106237.3106255},
    abstract = { Research has produced many approaches to automatically locate, explain, and repair
    software bugs. But do these approaches relate to the way practitioners actually locate,
    understand, and fix bugs? To help answer this question, we have collected a dataset
    named DBGBENCH --- the correct fault locations, bug diagnoses, and software patches
    of 27 real errors in open-source C projects that were consolidated from hundreds of
    debugging sessions of professional software engineers. Moreover, we shed light on
    the entire debugging process, from constructing a hypothesis to submitting a patch,
    and how debugging time, difficulty, and strategies vary across practitioners and types
    of errors. Most notably, DBGBENCH can serve as reality check for novel automated debugging
    and repair techniques. },
    booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
    pages = {117–128},
    numpages = {12},
    keywords = {Debugging in practice, User studies, User as tool benchmark, Evaluation},
    address = {Paderborn, Germany},
    series = {ESEC/FSE 2017}
}

@inproceedings{bohmer2011falling_asleep_with_angry_birds,
  title={Falling asleep with Angry Birds, Facebook and Kindle: a large scale study on mobile application usage},
  author={B{\"o}hmer, Matthias and Hecht, Brent and Sch{\"o}ning, Johannes and Kr{\"u}ger, Antonio and Bauer, Gernot},
  booktitle={Proceedings of the 13th international conference on Human computer interaction with mobile devices and services},
  pages={47--56},
  year={2011},
  address={Stockholm, Sweden},
  publisher={ACM},
}

@inproceedings{briand2007_a_critical_analysis_of_empirical_research_in_software_testing,
  author={Briand, Lionel C.},  
  booktitle={First International Symposium on Empirical Software Engineering and Measurement (ESEM 2007)}, 
  title={A Critical Analysis of Empirical Research in Software Testing},
  year={2007},
  volume={},  
  number={1},  
  pages={1-8},  
  abstract={
    In the foreseeable future, software testing will remain one of the best tools we have at our disposal to ensure software dependability. Empirical studies are crucial to software testing research in order to compare and improve software testing techniques and practices. In fact, there is no other way to assess the cost-effectiveness of testing techniques, since all of them are, to various extents, based on heuristics and simplifying assumptions. However, when empirically studying the cost and fault- detection rates of a testing technique, a number of validity issues arise. Further, there are many ways in which empirical studies can be performed, ranging from simulations to controlled experiments with human subjects. What are the strengths and drawbacks of the various approaches? What is the best option under which circumstances? This paper presents a critical analysis of empirical research in software testing and will attempt to highlight and clarify the issues above in a structured and practical manner.
  },  
  keywords={},
  doi={10.1109/ESEM.2007.40},
  ISSN={1949-3789}, 
  month={Sep.},
  publisher = {IEEE},
  address = {Madrid, Spain},
}

@inproceedings{budgen1993_case_tools_masters_or_servants,
  author={D. {Budgen} and M. {Marashi} and A. {Reeves}},
  booktitle={1993 Software Engineering Environments},
  title={CASE tools: Masters or servants?},
  publisher = {IEEE},
  year={1993},
  volume={},
  number={},
  pages={156-165},
  abstract={Much of the recent research into the use of CASE tools for specification and design of software systems has focused on the integration of such tools with one another and with related tools. However, much less attention has been directed towards considering how well these tools integrate with current practices. In particular, for software design there is good reason to believe that "opportunistic" design practices are widely employed by software designers, and that few industrial designers use either design methods or CASE tools. In this paper we draw upon some of our own research experiences in a related area to suggest some reasons why this might be so, and to describe some ideas that we are currently exploring to improve our understanding of these reasons. Our conclusion is that the organization and form of the user interface for CASE tools needs to be influenced by the designer's cognitive processes concerning the design, as well as by the need for a consistent form of presentation.},
  keywords={computer aided software engineering;software tools;user interfaces;software systems specification;software systems design;opportunistic design practices;consistent presentation format;CASE tools;current practices;industrial designers;user interface;cognitive processes;Computer aided software engineering;Software design;Design methodology;User interfaces;Navigation;Software tools;Computer science;Software systems;Computer industry;Process design},
  doi={10.1109/SEE.1993.388412},
  ISSN={},
  month={July},
  address = {Reading, United Kingdom},
}

@inproceedings{buse_analytics_2010,
	title = {Analytics for Software Development},
	url = {http://research.microsoft.com/apps/pubs/default.aspx?id=136974},
	address={Santa Fe, New Mexico, USA},
	abstract = {{\textless}p{\textgreater}Despite large volumes of data and many types of metrics, software projects continue to be diffcult to predict and risky to conduct. In this paper we propose software analytics which holds out the promise of helping the managers of software projects turn their plentiful information resources, produced readily by current tools, into insights they can act on. We discuss how analytics works, why it's a good fit for software engineering, and the research problems that must be overcome in order to realize its promise.{\textless}/p{\textgreater}},
	booktitle = {Proceedings of the {FSE}/{SDP} Workshop on the Future of Software Engineering Research ({FoSER})},
	publisher = {Association for Computing Machinery, Inc.},
	author = {Buse, Raymond P. L. and Zimmermann, Thomas},
	date = {2010-11},
	year = {2010},
	pages = {77--80},
	file = {foser-2010-buse.pdf:/Users/julianharty/Zotero/storage/6RW75BVR/foser-2010-buse.pdf:application/pdf}
}

@inproceedings{buse2012_information_needs_for_software_development_analytics,  
  author={Buse, Raymond P. L. and Zimmermann, Thomas},  
  booktitle={2012 34th International Conference on Software Engineering (ICSE)},   
  title={Information needs for software development analytics},   
  year={2012},  
  volume={},  
  number={},  
  pages={987-996},  
  abstract={
    Software development is a data rich activity with many sophisticated metrics. Yet engineers often lack the tools and techniques necessary to leverage these potentially powerful information resources toward decision making. In this paper, we present the data and analysis needs of professional software engineers, which we identified among 110 developers and managers in a survey. We asked about their decision making process, their needs for artifacts and indicators, and scenarios in which they would use analytics. The survey responses lead us to propose several guidelines for analytics tools in software development including: Engineers do not necessarily have much expertise in data analysis; thus tools should be easy to use, fast, and produce concise output. Engineers have diverse analysis needs and consider most indicators to be important; thus tools should at the same time support many different types of artifacts and many indicators. In addition, engineers want to drill down into data based on time, organizational structure, and system architecture.},  
  keywords={}, 
  doi={10.1109/ICSE.2012.6227122},
  ISSN={1558-1225},
  month={June},
  organization = {IEEE},
  publisher = {IEEE},
  address = {Zurich, Switzerland},
}

@inproceedings{cai2019_large_scale_study_of_android_incompatibilities,
  author = {Cai, Haipeng and Zhang, Ziyi and Li, Li and Fu, Xiaoqin},
  title = {A Large-Scale Study of Application Incompatibilities in Android},
  year = {2019},
  isbn = {9781450362245},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3293882.3330564},
  doi = {10.1145/3293882.3330564},
  abstract = {
    The rapid expansion of the Android ecosystem is accompanied by continuing diversification of platforms and devices, resulting in increasing incompatibility issues which damage user experiences and impede app development productivity. In this paper, we conducted a large-scale, longitudinal study of compatibility issues in 62,894 benign apps developed in the past eight years, to understand the symptoms and causes of these issues. We further investigated the incompatibilities that are actually exercised at runtime through the system logs and execution traces of 15,045 apps. Our study revealed that, among others, (1) compatibility issues were prevalent and persistent at both installation and run time, with greater prevalence of run-time incompatibilities, (2) there were no certain Android versions that consistently saw more or less app incompatibilities than others, (3) installation-time incompatibilities were strongly correlated with the minSdkVersion specified in apps, while run-time incompatibilities were most significantly correlated with the underlying platform’s API level, and (4) installation-time incompatibilities were mostly due to apps’ use of architecture-incompatible native libraries, while run-time incompatibilities were mostly due to API changes during SDK evolution. We offered further insights into app incompatibilities, as well as recommendations on dealing with the issues for bother developers and end users of Android apps.},
booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysi (ISSTA 2019)},
  pages = {216–227},
  numpages = {12},
  keywords = {installation failure, run-time failure, Android, compatibility},
  address = {Beijing, China},
  series = {ISSTA 2019}
}

@inproceedings{canfora2013_automating_UX_experience_testing_on_smartphones,
  author={G. {Canfora} and F. {Mercaldo} and C. A. {Visaggio} and M. {DAngelo} and A. {Furno} and C. {Manganelli}},
  booktitle={2013 IEEE Sixth International Conference on Software Testing, Verification and Validation},
  title={A Case Study of Automating User Experience-Oriented Performance Testing on Smartphones},
  year={2013},
  volume={},
  number={},
  pages={66-69},
  abstract={
    We have developed a platform named Advanced Test Environment (ATE) for supporting the design and the automatic execution of UX tests for applications running on Android smartphones. The platform collects objective metrics used to estimate the UX. In this paper, we investigate the extent that the metrics captured by ATE are able to approximate the results that are obtained from UX testing with real human users. Our findings suggest that ATE produces UX estimations that are comparable to those reported by human users. We have also compared ATE with three widespread benchmark tools that are commonly used in the industry, and the results show that ATE outperforms these tools.},
  keywords={automatic testing;Linux;program testing;smart phones;software performance evaluation;advanced test environment;automatic UX test execution;UX test design;Android smartphones;objective metrics;ATE;UX testing;UX estimations;user experience-oriented performance testing automation;Conferences;Software testing;user experience;mobile applications;software testing;usability;smartphone;android},  
  doi={10.1109/ICST.2013.16},
  ISSN={2159-4848},
  month={March},
  publisher = {IEEE},
  address = {Luxembourg, Luxembourg},
}

@inproceedings{not_yet_cited_chen2014_information_leakage_through_mobile_analytics_services,
    author = {Chen, Terence and Ullah, Imdad and Kaafar, Mohamed Ali and Boreli, Roksana},
    title = {Information Leakage through Mobile Analytics Services},
    year = {2014},
    isbn = {9781450327428},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2565585.2565593},
    doi = {10.1145/2565585.2565593},
    abstract = {In this paper we investigate the risk of privacy leakage through mobile analytics services and demonstrate the ease with which an external adversary can extract individual's profile and mobile applications usage information, through two major mobile analytics services, i.e. Google Mobile App Analytics and Flurry. We also demonstrate that it is possible to exploit the vulnerability of analytics services, to influence the ads served to users' devices, by manipulating the profiles constructed by these services. Both attacks can be performed without the necessity of having an attacker controlled app on user's mobile device. Finally, we discuss potential countermeasures (from the perspectives of different parties) that may be utilized to mitigate the risk of individual's personal information leakage.},
    booktitle = {Proceedings of the 15th Workshop on Mobile Computing Systems and Applications},
    articleno = {15},
    numpages = {6},
    location = {Santa Barbara, California},
    series = {HotMobile '14}
}

@inproceedings{chen2014qoe,
  title={{QoE Doctor}: Diagnosing mobile app {QoE} with automated {UI} control and cross-layer analysis},
  author={Chen, Qi Alfred and Luo, Haokun and Rosen, Sanae and Mao, Z Morley and Iyer, Karthik and Hui, Jie and Sontineni, Kranthi and Lau, Kevin},
  booktitle={Proceedings of the 2014 Conference on Internet Measurement Conference},
  pages={151--164},
  year={2014},
  publisher = {ACM},
  address = {Vancouver BC, Canada},
  doi={https://doi.org/10.1145/2663716.2663726},
  abstract={
    Smartphones have become increasingly prevalent and important in our daily lives. To meet users' expectations about the Quality of Experience (QoE) of mobile applications (apps), it is essential to obtain a comprehensive understanding of app QoE and identify the critical factors that affect it. However, effectively and systematically studying the QoE of popular mobile apps such as Facebook and YouTube still remains a challenging task, largely due to a lack of a controlled and reproducible measurement methodology, and limited insight into the complex multi-layer dynamics of the system and network stacks.

    In this paper, we propose QoE Doctor, a tool that supports accurate, systematic, and repeatable measurements and analysis of mobile app QoE. QoE Doctor uses UI automation techniques to replay QoE-related user behavior, and measures the user-perceived latency directly from UI changes. To better understand and analyze QoE problems involving complex multi-layer interactions, QoE Doctor supports analysis across the application, transport, network, and cellular radio link layers to help identify the root causes. We implement QoE Doctor on Android, and systematically quantify various factors that impact app QoE, including the cellular radio link layer technology, carrier rate-limiting mechanisms, app design choices and user-side configuration options.},
}

@inproceedings{cobb2020_ux_s_with_online_status_indicators,
  title={User Experiences with Online Status Indicators},
  author={Cobb, Camille and Simko, Lucy and Kohno, Tadayoshi and Hiniker, Alexis},
  booktitle={Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
  publisher = {ACM},
  address = {Honolulu HI, USA},
  pages={1--12},
  year={2020},
  doi={https://doi.org/10.1145/3313831.3376240},
  abstract={Online status indicators (OSIs) improve online communication by helping users convey and assess availability, but they also let users infer potentially sensitive information about one another. We surveyed 200 smartphone users to understand the extent to which users are aware of information shared via OSIs and the extent to which this shapes their behavior. Despite familiarity with OSIs, participants misunderstand many aspects of OSIs, and they describe carefully curating and seeking to control their self-presentation via OSIs. Some users further report leveraging OSI-conveyed information for problematic and malicious purposes. Drawing on existing constructs of app dependence (i.e., when users contort their behavior to meet an app's demands) and app enablement (i.e., when apps enable users to engage in behaviors they feel good about), we demonstrate that current OSI design patterns promote app dependence, and we call for a shift toward OSI designs that are more enabling for users.},
}

@inproceedings{corner2018micromobile,
  note={Presents excellent approaches to getting a wide variety of people involved in micro research experiments inexpensively.},
  title={MicroMobile: Leveraging mobile advertising for large-scale experimentation},
  author={Corner, Mark D and Levine, Brian N},
  booktitle={Proceedings of the 16th Annual International Conference on Mobile Systems, Applications, and Services},
  pages={310--322},
  year={2018},
  organization={ACM}
}

@inproceedings{cramer2010_research_in_the_large_app_stores,
    author = {Cramer, Henriette and Rost, Mattias and Belloni, Nicolas and Bentley, Frank and Chincholle, Didier},
    title = {Research in the Large. Using App Stores, Markets, and Other Wide Distribution Channels in Ubicomp Research},
    year = {2010},
    isbn = {9781450302838},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/1864431.1864501},
    doi = {10.1145/1864431.1864501},
    booktitle = {Proceedings of the 12th ACM International Conference Adjunct Papers on Ubiquitous Computing - Adjunct},
    pages = {511–514},
    numpages = {4},
    keywords = {mass evaluation methods, mobile ecosystem, app stores, distribution channels, mobile interaction},
    address = {Copenhagen, Denmark},
    series = {UbiComp ’10 Adjunct}
}


@inproceedings{cummings2004automation,
  title={Automation bias in intelligent time critical decision support systems},
  author={Cummings, Mary},
  booktitle={AIAA 1st Intelligent Systems Technical Conference},
  pages={6},
  year={2004},
  publisher = {American Institute of Aeronautics and Astronautics},
  doi = {10.2514/6.2004-6313},
  address = {Chicago, Illinois, USA}
}

@INPROCEEDINGS{not_yet_cited_dang2019_aiops_real_world_challenges,  
  author={Dang, Yingnong and Lin, Qingwei and Huang, Peng},  
  booktitle={2019 IEEE/ACM 41st International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)},   
  title={AIOps: Real-World Challenges and Research Innovations},   year={2019},
  volume={},
  number={},  
  pages={4-5},  
  abstract={
    AIOps is about empowering software and service engineers (e.g., developers, program managers, support engineers, site reliability engineers) to efficiently and effectively build and operate online services and Apps at scale with artificial intelligence (AI) and machine learning (ML) techniques. AIOps can help achieve higher service quality and customer satisfaction, engineering productivity boost, and cost reduction. In this technical briefing, we summarize the real-world challenges on building AIOps solutions based on our practice and experience in Microsoft, propose a roadmap of AIOps related research directions, and share a few successful AIOps solutions we have built for Microsoft service products.},
  keywords={},  
  doi={10.1109/ICSE-Companion.2019.00023},
  ISSN={2574-1934},
  month={May},
  comments = {
    This is a short yet useful paper that helps provide context for my research. It mentions software analytics can be viewed as AIOps innovations, it also argues for AIOps to reduce tedious and repetitive operations such as collecting information from various sources (including mobile analytics services) and repeatedly investigation [similar] issues.
  }
}

@inproceedings{deng2017_is_mutation_analysis_effective_at_testing_android_apps,
  author={L. {Deng} and J. {Offutt} and D. {Samudio}},
  booktitle={2017 IEEE International Conference on Software Quality, Reliability and Security (QRS)},
  title={Is Mutation Analysis Effective at Testing Android Apps?},
  year={2017},
  volume={},
  number={},
  pages={86-93},
  abstract={
    Not only is Android the most widely used mobile operating system, more apps have been released and downloaded for Android than for any other OS. However, quality is an ongoing problem, with many apps being released with faults, sometimes serious faults. Because the structure of mobile app software differs from other types of software, testing is difficult and traditional methods do not work. Thus we need different approaches to test mobile apps. In this paper, we identify challenges in testing Android apps, and categorize common faults according to fault studies. Then, we present a way to apply mutation testing to Android apps. Additionally, this paper presents results from two empirical studies on fault detection effectiveness using open-source Android applications: one for Android mutation testing, and another for four existing Android testing techniques. The studies use naturally occurring faults as well as crowdsourced faults introduced by experienced Android developers. Our results indicate that Android mutation testing is effective at detecting faults.},  
  keywords={Android (operating system);crowdsourcing;fault diagnosis;mobile computing;program diagnostics;program testing;public domain software;software reliability;mutation analysis;Android apps testing;mobile operating system;OS;mobile app software structure;fault detection;open-source Android applications;Android mutation testing;Androids;Humanoid robots;Testing;Crowdsourcing;XML;Wireless fidelity;Fault detection;Android;Software Testing;Mutation Testing;Empirical Evaluation;Crowdsourcing},  
  doi={10.1109/QRS.2017.19},  
  ISSN={},  
  month={July},
  publisher = {IEEE},
  address = {Prague, Czech Republic},
}

@inproceedings{diallo2015_correctness_and_relative_correctness,
  title={Correctness and relative correctness},
  author={Diallo, Nafi and Ghardallou, Wided and Mili, Ali},
  booktitle={2015 IEEE/ACM 37th IEEE International Conference on Software Engineering},
  volume={2},
  pages={591--594},
  year={2015},
  organization={IEEE},
  publisher={IEEE},
  address = {Florence, Italy},
}

@inproceedings{diouf2019_web_scraping_state_of_the_art_and_areas_of_application,
  author={Diouf, Rabiyatou and Sarr, Edouard Ngor and Sall, Ousmane and Birregah, Babiga and Bousso, Mamadou and Mbaye, Sény Ndiaye},  
  booktitle={2019 IEEE International Conference on Big Data (Big Data)},  
  title={Web Scraping: State-of-the-Art and Areas of Application},   
  year={2019},  
  volume={}, 
  number={},  
  pages={6040-6042},  
  abstract={
  Main objective of Web Scraping is to extract information from one or many websites and process it into simple structures such as spreadsheets, database or CSV file. However, in addition to be a very complicated task, Web Scraping is resource and time consuming, mainly when it is carried out manually. Previous studies have developed several automated solutions. The purpose of this article is to revisit the different existing Web Scraping approaches, categories, and tools, but also its areas of application.
  },  
  keywords={},  
  doi={10.1109/BigData47090.2019.9005594}, 
  ISSN={},  
  month={Dec},
  publisher = {IEEE},
  address = {Los Angeles, CA, USA},
}


@inproceedings{drouhard2016_typology_of_hackathon_events,
  title={A typology of hackathon events},
  author={Drouhard, Margaret and Tanweer, Anissa and Fiore-Gartland, Brittany},
  booktitle={Hacking at Time-Bound Events Workshop at Computer Supported Cooperative Work},
  volume={2016},
  pages={1--4},
  year={2016}
}

@inproceedings{erdogdu2015_privacy_utility_tradeoff_under_continual_observation,
  author={M. A. {Erdogdu} and N. {Fawaz}},
  booktitle={2015 IEEE International Symposium on Information Theory (ISIT)},
  title={Privacy-utility trade-off under continual observation},
  publisher = {IEEE},
  address = {Hong Kong},
  year={2015},
  volume={},
  number={},
  pages={1801-1805},
  abstract={
    In the online setting, a user continuously releases a time-series that is correlated with his private data, to a service provider to derive some utility. Due to correlations, the continual observation of the time-series puts the user at risk of inference attacks against his private data. To protect the user's privacy, the time-series is randomized prior to its release according to a probabilistic privacy mapping. This mapping should be designed in a way that balances privacy and utility requirements over time. First, we formalize the framework for the design of utility-aware privacy mappings for time-series, under both online and batch models. We introduce two threat models, for which we respectively show that under the log-loss cost function, the information leakage can be modeled by the mutual or directed information between the randomized time-series and the private data. Second, we prove that the design of the privacy mapping can be cast as a convex optimization. We provide a sequential online scheme that allows to design privacy mappings at scale, that accounts for privacy risk from the history of released data and future releases to come. Third, we prove the equivalence of the optimal mappings under the batch and the online models, in the case of a Hidden Markov Model. Evaluations on real-world time-series data show that smart-meter data can be randomized to prevent disaggregation of per-device energy consumption, while maintaining the utility of the randomized series.},  
  keywords={convex programming;data privacy;time series;privacy-utility trade-off;user privacy protection;probabilistic privacy mapping;utility-aware privacy mapping;threat models;log-loss cost function;information leakage;randomized time-series;convex optimization;hidden Markov model;Privacy;Hidden Markov models;Data privacy;Aggregates;Data models;Distortion;Adaptation models},  
  doi={10.1109/ISIT.2015.7282766},
  ISSN={2157-8117},
  month={June},
}

@inproceedings{evans2020stuck,
  title={Stuck in Limbo with Magical Solutions: The Testers’ Lived Experiences of Tools and Automation},
  author={Evans, Isabel and Porter, Chris and Micallef, Mark and Harty, Julian},
  booktitle={Proceedings of the 15th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications},
  pages={195--202},
  year={2020},
  address = {Valletta, Malta},
  organization={SCITEPRESS-Science and Technology Publications},
  url = {http://oro.open.ac.uk/69980/},
  publisher = {{SciTePress Digital Library}},
  doi = {10.5220/0009091801950202},
}

@inproceedings{evans2020_test_tools_an_illusion_of_usability,  
  author={I. {Evans} and C. {Porter} and M. {Micallef} and J. {Harty}},
  booktitle={2020 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)},
  title={Test Tools: an illusion of usability?},
  year={2020},
  volume={},
  number={},
  pages={392-397},
  abstract={
    Software testing is vital, yet expensive and time-consuming. This essential part of the software development process includes testers performing many repeated actions in test execution and management. Use of automation and tools could reduce costs and timescale, while providing consistency by removing human error during repetitive activities. Challenges for successful tools and automation adoption have been identified both in academic research and in industry practice, including technical, managerial, skills-related and usability issues. We set out to investigate what usability improvements would aid successful tool adoption, and discovered that usability, while a necessary attribute, is not sufficient to ensure success, and the belief in usability as a sufficient cure for automation shelfware might be an illusory phenomenon which disguises potential difficulties when using tools longer term. This illusion of usability includes a belief that UI attractiveness is sufficient for tool usability, a belief that testers come from a narrow group of personas, and a belief that skill levels and requirements for tools are static. This may lead to frustration for testers, and therefore reluctance to use tools and automation. We summarise our findings and outline our proposed next research steps.},  
  keywords={human factors;program testing;software engineering;user interfaces;test tools;software testing;software development process;test execution;human error;automation shelfware;tool usability;UI attractiveness;Tools;Usability;Automation;Testing;Industries;ISO Standards;software testing;test tools;automation;usability;quality in use;user experience},  
  doi={10.1109/ICSTW50294.2020.00070},
  ISSN={},
  month={Oct},
  publisher = {IEEE},
  address = {Porto, Portugal},
}

@inproceedings{evans2021_scared_frustrated_and_quietly_proud,
    author = {Evans, Isabel and Porter, Chris and Micallef, Mark},
    title = {Scared, Frustrated and Quietly Proud: Testers’ Lived Experience of Tools and Automation},
    year = {2021},
    isbn = {9781450387576},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3452853.3452872},
    doi = {10.1145/3452853.3452872},
    abstract = {
      Software testing is vital, expensive, time-consuming yet a necessary part of software development. Testers perform repeated actions during testing, where automation and tools could reduce costs, timescale and human error. However, challenges to tools adoption have been identified in academic research and industry, which are blockers to success with automation. In attempting to find whether testers were experiencing tool usability shortcomings, we followed an exploratory research path, collecting stories from over 100 test practitioners. We discovered a richer, more complex story than we expected. We realised that usability – while necessary – is not sufficient to enable success, and that other human factors challenge successful automation projects. In answering privately to questions about their experiences of tools and automation, testers expressed themselves in language that was more emotional and linked to their lived experience (LX) than we expected. We uncovered frustrations and fear, as well as pride. In this paper we present our findings so far about TX: The testers’ lived experience of tools and automation, and we suggest steps for future research.
    },
    booktitle = {European Conference on Cognitive Ergonomics 2021},
    articleno = {16},
    numpages = {7},
    keywords = {lived experience, user experience, software testing, test automation, human factors, test tools, usability},
    location = {Siena, Italy},
    series = {ECCE 2021}
}

@inproceedings{fan2018large,
  title={Large-scale analysis of framework-specific exceptions in Android apps},
  author={Fan, Lingling and Su, Ting and Chen, Sen and Meng, Guozhu and Liu, Yang and Xu, Lihua and Pu, Geguang and Su, Zhendong},
  booktitle={2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE)},
  pages={408--419},
  year={2018},
  organization={IEEE},
  doi={10.1145/3180155.3180222},
  abstract={Mobile apps have become ubiquitous. For app developers, it is a key priority to ensure their apps' correctness and reliability. However, many apps still suffer from occasional to frequent crashes, weakening their competitive edge. Large-scale, deep analyses of the characteristics of real-world app crashes can provide useful insights to guide developers, or help improve testing and analysis tools. However, such studies do not exist - this paper fills this gap. Over a four-month long effort, we have collected 16,245 unique exception traces from 2,486 open-source Android apps, and observed that framework-specific exceptions account for the majority of these crashes. We then extensively investigated the 8,243 framework-specific exceptions (which took six person-months): (1) identifying their characteristics (e.g., manifestation locations, common fault categories), (2) evaluating their manifestation via state-of-the-art bug detection techniques, and (3) reviewing their fixes. Besides the insights they provide, these findings motivate and enable follow-up research on mobile apps, such as bug detection, fault localization and patch generation. In addition, to demonstrate the utility of our findings, we have optimized Stoat, a dynamic testing tool, and implemented ExLocator, an exception localization tool, for Android apps. Stoat is able to quickly uncover three previously-unknown, confirmed/fixed crashes in Gmail and Google+; ExLocator is capable of precisely locating the root causes of identified exceptions in real-world apps. Our substantial dataset is made publicly available to share with and benefit the community.},
}

@inproceedings{ferre2017_extending_mobile_app_analytics_for_usability_test_logging,
author="Ferre, Xavier
and Villalba, Elena
and Julio, H{\'e}ctor
and Zhu, Hongming",
editor="Bernhaupt, Regina
and Dalvi, Girish
and Joshi, Anirudha
and K. Balkrishan, Devanuj
and O'Neill, Jacki
and Winckler, Marco",
title="Extending Mobile App Analytics for Usability Test Logging",
booktitle="Human-Computer Interaction -- INTERACT 2017",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="114--131",
abstract="Mobile application development is characterized by reduced development cycles and high time-to-market pressure. Usability evaluation in mobile applications calls for the application of cost-effective methods, specially adapted to such constraints. We propose extending the Google Analytics for Mobile Applications basic service to store specific low-level user actions of interest for usability evaluation purposes. The solution can serve both for lab usability testing, automating quantitative data gathering, and for logging real use after application release. It is based on identification of relevant user tasks and the detailed events worth gathering, instrumentation of specific code for data gathering, and subsequent data extraction for calculating relevant usability--related variables. We validated our application in a real usability test by comparing the automatically gathered data with the information gathered by the human observer. Results shows both measurements are statistically exchangeable, opening promising new ways to perform usability testing cost-effectively and at greater scale.",
isbn="978-3-319-67687-6"
}

@inproceedings{flajolet2007_hyper_log_log,
  TITLE = {{HyperLogLog: the analysis of a near-optimal cardinality estimation algorithm}},
  AUTHOR = {Flajolet, Philippe and Fusy, {\'E}ric and Gandouet, Olivier and Meunier, Fr{\'e}d{\'e}ric},
  URL = {https://hal.inria.fr/hal-00406166},
  BOOKTITLE = {{AofA: Analysis of Algorithms}},
  ADDRESS = {Juan les Pins, France},
  EDITOR = {Jacquet, Philippe},
  PUBLISHER = {{Discrete Mathematics and Theoretical Computer Science}},
  SERIES = {DMTCS Proceedings},
  VOLUME = {DMTCS Proceedings vol. AH, 2007 Conference on Analysis of Algorithms (AofA 07)},
  PAGES = {137-156},
  YEAR = {2007},
  MONTH = Jun,
  KEYWORDS = {cardinality estimation ; Probabilistic algorithm},
  PDF = {https://hal.inria.fr/hal-00406166v2/file/dmAH0110.pdf},
  HAL_ID = {hal-00406166},
  HAL_VERSION = {v2},
}

@inproceedings{heule2013_hyper_log_log_in_practice,
    author = {Heule, Stefan and Nunkesser, Marc and Hall, Alexander},
    title = {HyperLogLog in Practice: Algorithmic Engineering of a State of the Art Cardinality Estimation Algorithm},
    year = {2013},
    isbn = {9781450315975},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/2452376.2452456},
    doi = {10.1145/2452376.2452456},
    abstract = {Cardinality estimation has a wide range of applications and is of particular importance in database systems. Various algorithms have been proposed in the past, and the HyperLogLog algorithm is one of them. In this paper, we present a series of improvements to this algorithm that reduce its memory requirements and significantly increase its accuracy for an important range of cardinalities. We have implemented our proposed algorithm for a system at Google and evaluated it empirically, comparing it to the original HyperLogLog algorithm. Like HyperLogLog, our improved algorithm parallelizes perfectly and computes the cardinality estimate in a single pass.},
    booktitle = {Proceedings of the 16th International Conference on Extending Database Technology},
    pages = {683–692},
    numpages = {10},
    address = {Genoa, Italy},
    series = {EDBT '13}
}

@inproceedings{khalid2014_prioritizing_the_devices_to_test_your_app_on_casestudy_android_games,
    author = {Khalid, Hammad and Nagappan, Meiyappan and Shihab, Emad and Hassan, Ahmed E.},
    title = {Prioritizing the Devices to Test Your App on: A Case Study of Android Game Apps},
    year = {2014},
    isbn = {9781450330565},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/2635868.2635909},
    doi = {10.1145/2635868.2635909},
    abstract = { Star ratings that are given by the users of mobile apps directly impact the revenue
    of its developers. At the same time, for popular platforms like Android, these apps
    must run on hundreds of devices increasing the chance for device-specific problems.
    Device-specific problems could impact the rating assigned to an app, given the varying
    capabilities of devices (e.g., hardware and software). To fix device-specific problems
    developers must test their apps on a large number of Android devices, which is costly
    and inefficient. Therefore, to help developers pick which devices to test their apps
    on, we propose using the devices that are mentioned in user reviews. We mine the user
    reviews of 99 free game apps and find that, apps receive user reviews from a large
    number of devices: between 38 to 132 unique devices. However, most of the reviews
    (80\%) originate from a small subset of devices (on average, 33\%). Furthermore, we
    find that developers of new game apps with no reviews can use the review data of similar
    game apps to select the devices that they should focus on first. Finally, among the
    set of devices that generate the most reviews for an app, we find that some devices
    tend to generate worse ratings than others. Our findings indicate that focusing on
    the devices with the most reviews (in particular the ones with negative ratings),
    developers can effectively prioritize their limited Quality Assurance (QA) efforts,
    since these devices have the greatest impact on ratings. },
    booktitle = {Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
    pages = {610–620},
    numpages = {11},
    keywords = {Mobile apps, Android fragmentation, Device prioritization},
    address = {Hong Kong, China},
    series = {FSE 2014}
}

@inproceedings{kinshuman2009_debugging_in_the_very_large,
    author = {Glerum, Kirk and Kinshumann, Kinshuman and Greenberg, Steve and Aul, Gabriel and Orgovan, Vince and Nichols, Greg and Grant, David and Loihle, Gretchen and Hunt, Galen},
    title = {Debugging in the (Very) Large: Ten Years of Implementation and Experience},
    year = {2009},
    isbn = {9781605587523},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/1629575.1629586},
    doi = {10.1145/1629575.1629586},
    abstract = {Windows Error Reporting (WER) is a distributed system that automates the processing
    of error reports coming from an installed base of a billion machines. WER has collected
    billions of error reports in ten years of operation. It collects error data automatically
    and classifies errors into buckets, which are used to prioritize developer effort
    and report fixes to users. WER uses a progressive approach to data collection, which
    minimizes overhead for most reports yet allows developers to collect detailed information
    when needed. WER takes advantage of its scale to use error statistics as a tool in
    debugging; this allows developers to isolate bugs that could not be found at smaller
    scale. WER has been designed for large scale: one pair of database servers can record
    all the errors that occur on all Windows computers worldwide.},
    booktitle = {Proceedings of the ACM SIGOPS 22nd Symposium on Operating Systems Principles},
    pages = {103–116},
    numpages = {14},
    keywords = {blue screen of death, statistics-based debugging., error reports, bucketing, classifying, minidump, labeling},
    address = {Big Sky, Montana, USA},
    series = {SOSP '09}
}

@inproceedings{kong2019_mining_android_crash_fixes,
  author = {Kong, Pingfan and Li, Li and Gao, Jun and Bissyand\'{e}, Tegawend\'{e} F. and Klein, Jacques},
  title = {Mining Android Crash Fixes in the Absence of Issue- and Change-Tracking Systems},
  year = {2019},
  isbn = {9781450362245},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3293882.3330572},
  doi = {10.1145/3293882.3330572},
  abstract = {
    Android apps are prone to crash. This often arises from the misuse of Android framework APIs, making it harder to debug since official Android documentation does not discuss thoroughly potential exceptions.Recently, the program repair community has also started to investigate the possibility to fix crashes automatically. Current results, however, apply to limited example cases. In both scenarios of repair, the main issue is the need for more example data to drive the fix processes due to the high cost in time and effort needed to collect and identify fix examples. We propose in this work a scalable approach, CraftDroid, to mine crash fixes by leveraging a set of 28 thousand carefully reconstructed app lineages from app markets, without the need for the app source code or issue reports. We developed a replicative testing approach that locates fixes among app versions which output different runtime logs with the exact same test inputs. Overall, we have mined 104 relevant crash fixes, further abstracted 17 fine-grained fix templates that are demonstrated to be effective for patching crashed apks. Finally, we release ReCBench, a benchmark consisting of 200 crashed apks and the crash replication scripts, which the community can explore for evaluating generated crash-inducing bug patches.},
  booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis (ISSTA 2019)},
  pages = {78–89},
  numpages = {12},
  keywords = {debugging, mining software repository, Android, crash, testing},
  address = {Beijing, China},
  series = {ISSTA 2019}
}


@inproceedings{harty2020_fast_abstract_data_dynamics_for_testing_systems,
  author={Harty, Julian},
  booktitle={2020 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)}, 
  title={Fast Abstract: Data Dynamics for Testing Systems}, 
  year={2020},
  volume={},
  number={},
  pages={491-492},
  abstract={Data is the lifeblood of business systems, and accordingly having useful data for testing is both an interesting research challenge and a headache for many involved in projects in industry. There are few good answers of where and how to source such data cost- and time- effectively. In the author's experience across various industries globally the state of practice is poor. The poor practices potentially compromises both the projects and potentially the reputation and revenues of businesses, especially given fines based on a percentage of turnover. This paper aims to stimulate discussion and research into ways to first understand and then devise safe yet potent data sets for testing systems where the data are appropriate to the needs and context of the software project.},
  keywords={},
  doi={10.1109/ICSTW50294.2020.9374728},
  ISSN={},
  month={Oct},
  address = {Porto, Portugal},
  publisher = {IEEE},
  organization = {IEEE},
  }


@inproceedings{febrero2017_software_reliability_as_user_perception,
  author={F. {Febrero} and M. A. {Moraga} and C. {Calero}},
  booktitle={2017 IEEE International Conference on Software Quality, Reliability and Security (QRS)},
  title={Software Reliability as User Perception: Application of the Fuzzy Analytic Hierarchy Process to Software Reliability Analysis},
  year={2017},
  volume={},
  number={},
  pages={224-231},
  abstract={
    Software Quality is a multidimensional concept for which Reliability is considered as a key attribute. Notwithstanding, due to its conceptual complexity, there is no common agreement on what Software Reliability is, thus different stakeholders use a variety of Software Reliability views. With the aim to improve our understanding of what Software Reliability means for industrial stakeholders as well as that of contributing to enhance the industrial applicability of Software Quality Models we propose approaching Software Reliability analysis by using structural model based on representative International Standards, which are industry-oriented. As analysis method, since interested on stakeholders' vision of Reliability we will apply the Fuzzy Analytic Hierarchical Process (FAHP) which is designed to manage human assessment, always characterized by a certain degree of vagueness and subjectivity. The rationality of the proposed model and feasibility of the analysis method is proved by the application on a very large industrial system which provides empirical evidence on the conceptual descriptiveness capturing stakeholders' views and industrial applicability in an efficient manner.},  
  keywords={analytic hierarchy process;fuzzy set theory;human factors;software reliability;user perception;fuzzy analytic hierarchy process;software reliability analysis;FAHP;Software reliability;Software;Stakeholders;Analytical models;Standards;Industries;Empirical Software Engineering;Fuzzy Analytical Hierarchical Process;Quality Analysis and Evaluation;Software Reliability},
  ISSN={},
  month={July},
  publisher = {IEEE},
  address = {Prague, Czech Republic},
  doi = {10.1109/QRS.2017.33},
}


@INPROCEEDINGS{francese2017_mobile_app_development_and_management_results_from_a_quantitative_investigation,  
    author={Francese, Rita and Gravino, Carmine and Risi, Michele and Scanniello, Giuseppe and Tortora, Genoveffa},  
    booktitle={2017 IEEE/ACM 4th International Conference on Mobile Software Engineering and Systems (MOBILESoft)}, 
    title={Mobile App Development and Management: Results from a Qualitative Investigation}, 
    year={2017},  
    volume={}, 
    number={}, 
    pages={133-143},  
    abstract={
        We conducted a qualitative study to investigate the main aspects related to the development and management of applications (or apps) for smart and mobile devices. This investigation is composed of two main steps and its context is the software industry. In the first step, we interviewed software managers with experience in the context of app development and management. This part of our study can be intended as explorative because we used its outcomes to plan and execute the second step of our study, namely a survey with software professionals. From this survey, we obtained a number of findings that we can summarize as follows: (i) app development is mostly done by junior developers; (ii) agile methodologies and cross-platform development frameworks are largely adopted even if there are no approaches and frameworks considered the best; (iii) support for testing is considered inadequate; (iv) fragmentation of software and hardware is perceived an important concern; and (v) app development is considered different from the development of web/desktop applications. Based on our findings, we highlight areas that require more attention from the research and the industry.
    },  
    keywords={},
    doi={10.1109/MOBILESoft.2017.33},  
    ISSN={}, 
    month={May},
    rationale = {
         This paper is interesting in terms of providing background context e.g. that app development is often performed by relatively junior people and that maintenance is frequently not performed at all, not least because it's outside their commercial contract. 
    },
    extracts = {
        D. Maintenance We observed that 39 respondents never performed maintenance (Q9.1). In their opinion, the specific difficulties in maintaining apps (Q9.2*) are related to: (i) the comprehension of source code written by others; (ii) the absence of documentation; and (iii) the interpretation of bug reports from users (e.g., one of the respondent wrote: a lot of time “App does not work” is the only received information). Interestingly, 44 respondents declared that maintenance activities are not managed by an agreement with the customer as usually done with other kinds of software (Q9.3). This somewhat confirms interview outcomes.
        We also observed the presence of a high level of specialization and expertise even if development is mostly done with junior developers (less than 5 years of industrial experience). This might have education implications since special skills would be beneficial for students thinking about working on app development.
    }
}

@inproceedings{frankl1997choosing_testing_for_reliability,
  title={Choosing a testing method to deliver reliability},
  author={Frankl, Phyllis and Hamlet, Dick and Littlewood, Bev and Strigini, Lorenzo},
  booktitle={Proceedings of the 19th international conference on Software engineering},
  address = {Boston, Massachusetts, USA},
  publisher = {ACM},
  pages={68--78},
  year={1997}
}

@inproceedings{fu2013_why_people_hate_your_app_making_sense_of_user_feedback_in_a_mobile_app_store,
    author = {Fu, Bin and Lin, Jialiu and Li, Lei and Faloutsos, Christos and Hong, Jason and Sadeh, Norman},
    title = {Why People Hate Your App: Making Sense of User Feedback in a Mobile App Store},
    year = {2013},
    isbn = {9781450321747},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2487575.2488202},
    doi = {10.1145/2487575.2488202},
    abstract = {
        User review is a crucial component of open mobile app markets such as the Google Play Store. How do we automatically summarize millions of user reviews and make sense out of them? Unfortunately, beyond simple summaries such as histograms of user ratings, there are few analytic tools that can provide insights into user reviews. In this paper, we propose Wiscom, a system that can analyze tens of millions user ratings and comments in mobile app markets at three different levels of detail. Our system is able to (a) discover inconsistencies in reviews; (b) identify reasons why users like or dislike a given app, and provide an interactive, zoomable view of how users' reviews evolve over time; and (c) provide valuable insights into the entire app market, identifying users' major concerns and preferences of different types of apps. Results using our techniques are reported on a 32GB dataset consisting of over 13 million user reviews of 171,493 Android apps in the Google Play Store. We discuss how the techniques presented herein can be deployed to help a mobile app market operator such as Google as well as individual app developers and end-users.
    },
    booktitle = {Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
    pages = {1276–1284},
    numpages = {9},
    keywords = {topic model, sentiment analysis, user rating and comments, text mining, mobile app market},
    location = {Chicago, Illinois, USA},
    series = {KDD '13},
}

@inproceedings{gao2014mobile,
  title={Mobile Testing-as-a-Service (MTaaS)--Infrastructures, Issues, Solutions and Needs},
  author={Gao, Jerry and Tsai, Wei-Tek and Paul, Ray and Bai, Xiaoying and Uehara, Tadahiro},
  booktitle={2014 IEEE 15th International Symposium on High-Assurance Systems Engineering},
  pages={158--167},
  year={2014},
  organization={IEEE},
  publisher = {IEEE},
  address = {Miami Beach, FL, USA},
  abstract = {With the rapid advance of mobile computing technology and wireless networking, there is a significant increase of mobile subscriptions. This drives a strong demand for development and validation of mobile APPs and SaaS applications on mobile web. This paper is written to offer informative and insightful discussion about mobile testing-as-a-service (MTaaS), including its basic concepts, motivations, distinct features and requirements, test environments, and different approaches. Moreover, it presents a test process in MTaaS and three different approaches. Furthermore, the paper proposes one mobile test cloud infrastructure for mobile TaaS, and discusses the required mobile test frameworks and environments. Finally, the paper addresses existing issues, challenges, and emergent needs.},
  convo={yes},
}

@inproceedings{geiger2018_a_graph_based_dataset_of_commit_history_of_realworld_android_apps,
    author = {Geiger, Franz-Xaver and Malavolta, Ivano and Pascarella, Luca and Palomba, Fabio and Di Nucci, Dario and Bacchelli, Alberto},
    title = {A Graph-Based Dataset of Commit History of Real-World Android Apps},
    year = {2018},
    isbn = {9781450357166},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/3196398.3196460},
    doi = {10.1145/3196398.3196460},
    abstract = {Obtaining a good dataset to conduct empirical studies on the engineering of Android apps is an open challenge. To start tackling this challenge, we present AndroidTimeMachine, the first, self-contained, publicly available dataset weaving spread-out data sources about real-world, open-source Android apps. Encoded as a graph-based database, AndroidTimeMachine concerns 8,431 real open-source Android apps and contains: (i) metadata about the apps' GitHub projects, (ii) Git repositories with full commit history and (iii) metadata extracted from the Google Play store, such as app ratings and permissions.},
    booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
    pages = {30–33},
    numpages = {4},
    keywords = {mining software repositories, dataset, Android},
    address = {Gothenburg, Sweden},
    series = {MSR '18}
}

@inproceedings{gray1986_why_do_computers_stop_and_what_can_be_done_about_it,
  author    = {Jim Gray},
  title     = {Why Do Computers Stop and What Can Be Done About It?},
  booktitle = {Fifth Symposium on Reliability in Distributed Software and Database
               Systems, {SRDS} 1986, Los Angeles, California, USA, January 13-15,
               1986, Proceedings},
  pages     = {3--12},
  publisher = {{IEEE} Computer Society},
  year      = {1986},
  month     = {Jan},
  address    = {Los Angeles, California, USA},
  timestamp = {Mon, 06 Nov 2017 16:35:11 +0100},
  biburl    = {https://dblp.org/rec/conf/srds/Gray86.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  exclusion_rationale = {
    Ancient work far removed from mobile analytics. It might be more relevant to cite: Why Do Internet Services Fail, and What Can Be Done About It? or even write something similar for why do mobile apps fail, etc.?
    
    BTW the technical report available online is dated 1985, I wonder if the 1986 paper is identical in terms of the contents?
  },
  inclusion_rationale = {
   It's the likely source of the concept of Heisenbug which I want to mention in the related work chapter.
  },
}

@inproceedings{greenheld2018_automating_developers_responses_to_app_reviews,
  author={G. {Greenheld} and B. T. R. {Savarimuthu} and S. A. {Licorish}},
  booktitle={2018 25th Australasian Software Engineering Conference (ASWEC)}, 
  title={Automating Developers' Responses to App Reviews}, 
  year={2018},
  volume={},
  number={},
  pages={66-70},
  publisher = {IEEE},
  address = {Adelaide, SA, Australia},
  abstract={App reviews are important as they contain valuable information for improving the quality of such software systems. To this end, most users providing app reviews expect a response; to the extent that prior research has shown that when developers respond to app reviews these responses improve app ratings and users' satisfaction. However, unfortunately, user reviews largely go unanswered for most apps due to the high prevalence of reviews. This challenge may be addressed by creating a system that automatically generates responses having learned from the responses already posted by developers for a given app. These generated responses may then be modified by developers if required. This work presents a system that recommends socially-acceptable responses based on principles adopted from three domains: information retrieval, social norms and userinterface design. We then evaluate the newly developed system against Google Play's de facto review response system, which requires developers to write responses manually. The evaluation of the two systems involved measuring participants' feedback on three aspects - usability, cognitive load and performance. The goal of the evaluation was to investigate whether users prefer the new system over the existing system. Our outcomes show that there were statistically significant differences between the two systems on all three aspects evaluated, with users preferring the newly proposed system over Google Play's response system. In particular, the proposed system has the potential to reduce the overall workload of developers considerably.},
  keywords={formal specification;information retrieval;Internet;mobile computing;user interfaces;software systems;app reviews;app ratings;user reviews;socially-acceptable responses;automating developers;Google Play response system;Google Play de facto review response system;Google;Task analysis;User interfaces;Information retrieval;Information science;Atmospheric measurements;Particle measurements;app review;response recommendation;social norms;user interface design;information retrieval},
  doi={10.1109/ASWEC.2018.00017},
  ISSN={2377-5408},
  month={Nov},
}

@inproceedings{godinho2016open,
  title={Open Device Lab: An Analysis of Available Devices in the Gaming Market.},
  abstract={Device Lab is a worldwide network of laboratories providing a community pool of internet connected devices. Quality assurance across real devices is needed to solve device-specific problems. The community proposal is to enable anyone a free-to-use lab to test their work. Building on the gaming market categories we carried out an analysis of community devices that can serve this market to evaluate their potential for collaboration. Our findings indicate that the majority of devices can serve the mobile game segment. An important alternative to resolve device-specific problems and improve gaming experience.},
  author={Godinho, Raquel Paiva and Contreras-Espinosa, Ruth S},
  booktitle={2016 8th International Conference on Games and Virtual Worlds for Serious Applications (VS-GAMES)},
  pages={1--4},
  year={2016},
  organization={IEEE}
}

@inproceedings{ghardallou2016debugging_without_testing,
  title={Debugging without testing},
  author={W. {Ghardallou} and N. {Diallo} and A. {Mili} and M. F. {Frias}},  
  booktitle={2016 IEEE International Conference on Software Testing, Verification and Validation (ICST)},
  pages={113--123},
  volume={},
  number={},
  year={2016},
  organization={IEEE},
  publisher = {IEEE},
  address = {Chicago, IL, USA.},
  abstract={It is so inconceivable to debug a program without testing it that these two words are used nearly interchangeably. Yet we argue that using the concept of relative correctness we can indeed remove a fault from a program and prove that the fault has been removed, by proving that the new program is more correct than the original. This is a departure from the traditional roles of proving and testing methods, whereby static proof methods are applied to a correct program to prove its correctness, and dynamic testing methods are applied to an incorrect program to expose its faults.},
  keywords={program debugging;program debugging;static proof methods;dynamic testing methods;Sufficient conditions;Mathematics;Conferences;Software testing;Debugging;Scalability;debugging;testing;correctness;relative correctness;faults;fault removal},  
  doi={10.1109/ICST.2016.12},  
  ISSN={},  
  month={April},
}

@inproceedings{gousios2012_ghtorrent_githubs_data_from_a_firehose,
  author={Gousios, Georgios and Spinellis, Diomidis},
  booktitle={2012 9th IEEE Working Conference on Mining Software Repositories (MSR)}, 
  title={GHTorrent: Github's data from a firehose}, 
  year={2012},
  volume={},
  number={},
  pages={12-21},
  abstract={A common requirement of many empirical software engineering studies is the acquisition and curation of data from software repositories. During the last few years, GitHub has emerged as a popular project hosting, mirroring and collaboration platform. GitHub provides an extensive REST API, which enables researchers to retrieve both the commits to the projects' repositories and events generated through user actions on project resources. GHTorrent aims to create a scalable off line mirror of GitHub's event streams and persistent data, and offer it to the research community as a service. In this paper, we present the project's design and initial implementation and demonstrate how the provided datasets can be queried and processed.},
  keywords={},
  doi={10.1109/MSR.2012.6224294},
  ISSN={2160-1860},
  month={June},
  publisher = {IEEE},
  address = {Zurich, Switzerland},
}

@comment{This is a provisional entry, hand crafted until the conference makes the paper available}
@inproceedings{guilardi_are_apps_ready_for_new_android_releases,
  title = {Are apps ready for new Android releases?},
  author = {Guilardi, Demetrio and Nicacio, Jalves and Napoleao, Bianca  and  Petrillo, Fabio},
  url = {https://conf.researchr.org/details/mobilesoft-2020/mobilesoft-2020-technical-papers/8/Are-apps-ready-for-new-Android-releases-},
  year = {2020},
  pages = {66 - 76},
  doi = {10.1145/3387905.3388598},
  abstract = {Context: Android operating system always brings new releases and updates to improve security, increase performance and bring a better user experience. When Google announces a new release, a whole chain of changes is triggered in cascade, causing many compatibility issues. Objective: This study focus at performing a quantitative and qualitative analysis on the state of apps readiness for new Android releases over time. Method: We performed an empirical study to map apps readiness to different Android versions. We developed a Repository Mining Tool to analyse 8420 open-source repositories, detecting 2118 Android projects and when they were adapted to different Android versions along their lifetimes. Results: Our results show that Android apps have became “less ready” over time. We found that 76.45\% of the analysed apps were ready for Android Lollipop 5.0 (API level 21) release, in October 2014. Though only 5.46\% were ready for Android 10 (API level 29), in September 2019. In addition, our results show that when apps are adapted to an Android version, 59.41\% perform the adaptation until the new Android release month, 95\% are adapted twelve months after the release, and 99.16\% are adapted two years later. Conclusion: Our findings reveal implications that affect not only the Android or mobile development research field and developers, they also reveal implications that points to Google’s policies and Android final users as well.},
  booktitle = {Proceedings of the 7th International Conference on Mobile Software Engineering and Systems},
  publisher = {ACM},
  address = {Virtual},
}

@inproceedings{Hamdi2021empirical,
  author={Hamdi, Oumayma and Ouni, Ali and AlOmar, Eman Abdullah and Ó Cinnéide, Mel and Mkaouer, Mohamed Wiem},  
  alternative_author={Hamdi, Oumayma and Ouni, Ali and AlOmar, Eman Abdullah and {\'O} Cinn{\'e}ide, Mel and Mkaouer, Mohamed Wiem},
  booktitle={2021 IEEE/ACM 8th International Conference on Mobile Software Engineering and Systems (MobileSoft)}, 
  title={An Empirical Study on the Impact of Refactoring on Quality Metrics in Android Applications},  
  year={2021}, 
  volume={},  
  number={},  
  pages={28-39},  
  abstract={
    Mobile applications must continuously evolve, sometimes under such time pressure that poor design or implementation choices are made, which inevitably result in structural software quality problems. Refactoring is the widely-accepted approach to ameliorating such quality problems. While the impact of refactoring on software quality has been widely studied in object-oriented software, its impact is still unclear in the context of mobile apps. This paper reports on the first empirical study that aims to address this gap. We conduct a large empirical study that analyses the evolution history of 300 open-source Android apps exhibiting a total of 42,181 refactoring operations. We analyze the impact of these refactoring operations on 10 common quality metrics using a causal inference method based on the Difference-in-Differences (DiD) model. Our results indicate that when refactoring affects the metrics it generally improves them. In many cases refactoring has no significant impact on the metrics, whereas one metric (LCOM) deteriorates overall as a result of refactoring. These findings provide practical insights into the current practice of refactoring in the context of Android app development.},  
  keywords={}, 
  doi={10.1109/MobileSoft52590.2021.00010}, 
  ISSN={}, 
  month={May},
  publisher = {IEEE},
  address = {Madrid, Spain},
  comments = {
    Their work investigates the code and how it evolves through refactoring.
    
    "Refactoring is the most common approach to improve the internal structure of software systems without affecting their external behavior" is contradicted in their later statement: 
    "Even though refactoring aims at improving code structure, this expectation might not be always met in real settings as refactoring changes are often performed quickly to meet users requirements, fix defects or adapt to environment changes in the highly volatile mobile market" If the developers are fixing defects they are often changing the external behaviour e.g. to prevent a crash.
    
    There appears to be a similar confusion in: 
    "Given that Android apps should evolve quickly to add new user requirements, fix bugs or adapt to new technological changes, such refactorings may increase technical debt and thus cause developers to invest additional maintenance effort in the future in order to fix quality issues in their apps." where code changes to support new requirements, fix bugs, etc. are described as refactorings.  
    
    Their group of qualities are all internal qualities of the code, rather than quality-in-use metrics. The developers of the apps in the app-centric case studies may have performed some refactoring as defined in this research, the amount is unknown (but could be calculated for the opensource codebases), as such refactoring was not mentioned or observed in the app-centric case studies I'll assume it was not explicitly performed and that it wasn't performed more than before or after the active case study period.
    
    Disconcertingly they didn't include Kiwix-Android despite the app meeting their published selection criteria.
    "For this purpose, we performed a custom search on GitHub by targeting all Java repositories in which the readme.md file contains a link to a Google Play Store page", the link to the app in Google Play has been on the page for at least 3 years, see: https://github.com/kiwix/kiwix-android/blame/develop/README.md
  }
}


@inproceedings{harty_better_android_apps_using_android_vitals,
    author = {Harty, Julian and M\"{u}ller, Matthias},
    title = {Better Android Apps Using Android Vitals},
    year = {2019},
    isbn = {9781450368582},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/3340496.3342761},
    doi = {10.1145/3340496.3342761},
    booktitle = {Proceedings of the 3rd ACM SIGSOFT International Workshop on App Market Analytics},
    pages = {26–32},
    numpages = {7},
    keywords = {Opensource, App development, Feedback and reputation, Android Vitals, Quality of apps},
    address = {Tallinn, Estonia},
    series = {WAMA 2019}
}

@inproceedings{harty2020_designing_engineering_onboarding,
    author = {Harty, Julian},
    title = {Designing Engineering Onboarding for 60+ Nationalities},
    year = {2020},
    isbn = {9781450370936},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/3372787.3390504},
    doi = {10.1145/3372787.3390504},
    abstract = {A large international engineering office in Germany needed to double in size in 12 months. We designed an onboarding programme within 3 months to help it do so efficaciously. We wanted to optimize for: fast iterations in the programme rollout, to keep the 'flywheel spinning' by reducing drag on current staff, rapid acceleration where new hires contributed quickly, and smooth integration where new hires adapted to the engineering, company, and country cultures.To reduce drag we onboarded in cohorts and involved existing practitioners in the design and discussion. To encourage contributions quickly we built contributions into the sessions, we also streamlined IT Support. To help new hires adopt the culture we encouraged help and mentoring within and across cohorts.For fast iterations, we incorporated existing islands of onboarding, involved local technical staff in design and delivery of hands-on training, and applied analytics to help improve the practice. And we launched early to bootstrap our learning and evaluation.Our approach worked; new hires were able to make meaningful contributions within a week and they scored the onboarding programme positively (8.5 NPS).},
    booktitle = {Proceedings of the 15th International Conference on Global Software Engineering},
    pages = {76–80},
    numpages = {5},
    keywords = {mob programming, collaboration in software development, onboarding, collaborative learning},
    address = {Seoul, Republic of Korea},
    series = {ICGSE '20}
}

@inproceedings{harty_google_play_console_insightful_development_using_android_vitals_and_pre_launch_reports,
author = {Harty, Julian},
title = {Google Play Console: Insightful Development Using Android Vitals and Pre-Launch Reports},
year = {2019},
publisher = {IEEE Press},
booktitle = {Proceedings of the 6th International Conference on Mobile Software Engineering and Systems},
pages = {62–65},
numpages = {4},
keywords = {software quality, Android vitals, mobile applications, Google dev console, pre-launch report, kiwix, metrics},
address = {Montreal, Quebec, Canada},
series = {MOBILESoft ’19}
}

@inproceedings{harty2020_how_can_software_testing_be_improved_by_analytics_to_deliver_better_apps,
  author = {Harty, Julian},
  booktitle={2020 IEEE 13th International Conference on Software Testing, Validation and Verification (ICST)},
  title={How Can Software Testing be Improved by Analytics to Deliver Better Apps?},
  year={2020},
  volume={},
  number={},
  pages={418-420}, 
  abstract={Many consider software testing to be necessary yet given the nature of testing and practical project constraints it cannot be comprehensive or complete. The resulting software has bugs including those that affect some users. Analytics of usage of apps may help illuminate testing that has been performed on existing releases and also inspire improvements to future testing. The Android ecosystem provides unusually rich analytics tools for developers of apps released in Google Play so my research focuses on this ecosystem to evaluate several analytics tools including Google Play Console, Android Vitals, which are integrated into the platform and the operating system, together with additional mobile analytics offerings from Google and Microsoft.},  
  keywords={mobile computing;program debugging;program testing;software tools;Android Vitals;software testing;mobile analytics;Google Play Console;Android ecosystem;future testing;Software;Androids;Humanoid robots;Tools;Google;Software testing;Android;Android Vitals;Apps;Crashlytics;Firebase;Mobile;Software Analytics;Software Testing},  
  doi={10.1109/ICST46399.2020.00052},  
  ISSN={2159-4848},
  month={Oct},
  address = {Porto, Portugal},
  publisher = {IEEE},
}

@inproceedings{harty_improving_app_quality_despite_flawed_mobile_analytics,
author = {Harty, Julian},
title = {Improving App Quality Despite Flawed Mobile Analytics},
year = {2020},
publisher = {IEEE Press},
booktitle = {Proceedings of the 7th International Conference on Mobile Software Engineering and Systems},
pages = {},
numpages = {2},
address = {Seoul, South Korea},
series = {MOBILESoft ’20}
}

@inproceedings{hecht2015approach,
  title={An approach to detect Android antipatterns},
  author={Hecht, Geoffrey},
  booktitle={2015 IEEE/ACM 37th IEEE International Conference on Software Engineering},
  volume={2},
  pages={766--768},
  year={2015},
  organization={IEEE},
  publisher = {IEEE},
  address = {Florence, Italy},
  convo={yes},
}

@inproceedings{hirsch2019approach_catrobat,
  title={An Approach to Test Classification in Big Android Applications},
  author={Hirsch, Thomas and Schindler, Christian and M{\"u}ller, Matthias and Schranz, Thomas and Slany, Wolfgang},
  booktitle={2019 IEEE 19th International Conference on Software Quality, Reliability and Security Companion (QRS-C)},
  pages={300--308},
  year={2019},
  organization={IEEE},
  publisher={IEEE},
  address={Sofia, Bulgaria}
}

@inproceedings{hsu2018_how_agile_impacts_a_software_corporation,  
  author={H. {Hsu} and Y. {Lin}},  
  booktitle={2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC)},   
  title={How Agile Impacts a Software Corporation: An Empirical Study},   
  year={2018},  
  volume={02},  
  number={},  
  pages={20-25},  
  abstract={Agile methods advocating early and frequent delivery, incremental and iterative development, and adaptive planning gradually becomes a popular software development methodology around the world. Many top software companies adopt agile methods and develop lots of remarkable software products. Although many researchers have studied agile methods using various approaches, whether and how agile methods help developers establish better software is still controversial. In this paper, we study the business data, development, and human resources records provided by Titansoft, a Singapore IT service corporation which is well-known in its successful Agile experiences. From analysis of the data, three primary impacts brought by agile adoption are revealed: (1) A temporary business stagnation; (2) Improvement in product quality; (3) Positive evolution in corporate culture. To the best of our knowledge, this paper is one of the few studies discussing how agile methods influence a software company in software development, business, and corporate culture aspects throughout a relatively long duration (more than 5 years).},  
  keywords={DP industry;project management;software development management;software engineering;software prototyping;software quality;incremental development;iterative development;popular software development methodology;software companies;agile methods;remarkable software products;agile adoption;Software;Companies;Programming;Collaboration;Electronic mail;Agile development, Data analysis, Coporate culture},  
  doi={10.1109/COMPSAC.2018.10197},  
  ISSN={0730-3157},  
  month={July},
}

@inproceedings{hu_tealeaf_cxmobile,  
  author={ {Yanke Hu}},  
  booktitle={2016 24th International Conference on Geoinformatics},   
  title={Tealeaf CxMobile - replaying real time customer experience},   
  year={2016},  
  volume={},  
  number={},  
  pages={1-4},
  address={Galway, Ireland},
  publisher={IEEE},
  abstract={With the rapid evolvement of mobile and cloud technology, improving and optimizing business mobile apps' customer experience has unprecedented importance. Mobile developers are expecting a tool that can help them know how customers are interacting with their apps, and how their apps react to the customers' behaviors. If there is any imperfection of the mobile app interface design or bugs being thrown out, they would want to know that as early as possible. This helps a lot to accelerate conversation rate and avoid business losses. There are numerous raw data analytics tools on the market to help those online businesses make improvements, but no tool can tell these businesses why and how their mobile customers are struggling. This paper introduces IBM Tealeaf CxMobile [1], which provides extensive analytics functions and expansive visibilities into the mobile customer experience of native mobile applications. IBM Tealeaf CxMobile is using the Modularization SDK methodology [2], which is essentially encapsulating core function modules into SDKs and centralize the backend reporting. With its unique Native Mobile Replay technology, IBM Tealeaf CxMobile can replay a user's journey through a native iOS or Android app in the way of “what you see is what you get” rather than mere data analysis. This paper will explain the principles of Tealeaf Native Replay and how it works on iOS and Android platforms. We also will give an example of integrating the Geo-Location logging functions into IBM Tealeaf CxMobile.},  
  
  keywords={Android (operating system);business data processing;customer services;data handling;iOS (operating system);mobile computing;optimisation;program debugging;real time customer experience;mobile technology;cloud technology;business mobile apps;customer experience;mobile developers;customer behaviors;mobile app interface design;raw data analytics tools;online businesses;IBM Tealeaf CxMobile;modularization SDK methodology;native iOS;Android app;geo-location logging functions;Business;Mobile communication;Airports;Mobile;Customer;Geo-Location;Enterprise;Analytics;Growth Hacking},  
  
  doi={10.1109/GEOINFORMATICS.2016.7578968},  
  ISSN={2161-0258},  
  month={Aug},
}

@inproceedings{huang2019_up_to_crash_3rdparty_libraries_on_android,
  author={Huang, Jie and Borges, Nataniel and Bugiel, Sven and Backes, Michael}, 
  booktitle={2019 IEEE European Symposium on Security and Privacy (EuroS P)}, 
  title={Up-To-Crash: Evaluating Third-Party Library Updatability on Android}, 
  year={2019}, 
  volume={}, 
  number={}, 
  publisher = {IEEE},
  address = {Stockholm, Sweden},
  pages={15-30}, 
  abstract={
    Buggy and flawed third-party libraries increase their host app's attack surface and put the users' privacy at risk. To avert this risk, libraries have to be kept updated to their newest versions by the app developers that integrate them into their projects. Recent researches revealed that the prevalence of outdated third-party libraries in Android apps is indeed a rampant problem, but also suggested that there is a great opportunity for drop-in replacements of outdated libraries, which would not even require cooperation by the app developers to update the libraries. However, all those conclusions are based on static app analysis, which can only provide an abstract view. In this work, we extend the updatability analysis to the runtime of apps. We implement a solution to update third-party libraries with drop-in replacements by their newer versions. To verify the feasibility of this developer-independent update mechanism, we dynamically test 3,000 real world apps for 3 popular libraries (78 library versions) for runtime failures stemming from incompatible library updates. To investigate the updatability of libraries in-depth, exploration enhanced dynamic testing is adopted to monitor the runtime behaviors of 15 apps before and after library updating. From our test, we find that the prior reported updatability rate is under real conditions overestimated by a factor of 1.57-2.06. Through root cause analysis, we find that the underlying problems prohibiting easy updates are intricate, such as deprecated functions, changed data structures, or entangled dependencies between different libraries and even the host app. We think our results not only put a more realistic light on the library updatability problem in Android, but also provide valuable insights for future solutions that provide automatic library updates or that try to support the app developers in better maintaining their external dependencies.},
  keywords={}, 
  doi={10.1109/EuroSP.2019.00012},
  ISSN={}, 
  month={June},
}


@inproceedings{jha2019_characterizing_android_specific_crash_bugs,
author = {Jha, Ajay Kumar and Lee, Sunghee and Lee, Woo Jin},
title = {Characterizing Android-Specific Crash Bugs},
year = {2019},
publisher = {IEEE Press},
abstract = {Android platform provides a unique framework for app development. Failure to comply
with the framework may result in serious bugs. Android platform is also evolving rapidly
and developers extensively use APIs provided by the framework, which may lead to serious
compatibility bugs if developers do not update the released apps frequently. Furthermore,
Android apps run on a wide range of memory-constrained devices, which may cause various
device-specific and memory-related bugs. There are several other Android-specific
issues that developers need to address during app development and maintenance. Failure
to address the issues may result in serious bugs manifested as crashes. In this paper,
we perform an empirical study to investigate and characterize various Android-specific
crash bugs, their prevalence, root causes, and solutions by analyzing 1,862 confirmed
crash reports of 418 open source Android apps. The investigation results can help
app developers in understanding, preventing, and fixing the Android-specific crash
bugs. Moreover, the results can help app developers and researchers in designing effective
bug detection tools for Android apps.},
booktitle = {Proceedings of the 6th International Conference on Mobile Software Engineering and Systems},
pages = {111–122},
numpages = {12},
keywords = {Android apps, crash bug analysis, mining crash bugs, characterizing crash bugs},
address = {Montreal, Quebec, Canada},
series = {MOBILESoft '19}
}

@inproceedings{johnson2013_why_dont_devs_use_static_analysis,
  title={Why don't software developers use static analysis tools to find bugs?},
  author={Johnson, Brittany and Song, Yoonki and Murphy-Hill, Emerson and Bowdidge, Robert},
  booktitle={2013 35th International Conference on Software Engineering (ICSE)},
  pages={672--681},
  year={2013},
  organization={IEEE},
  publisher = {IEEE},
  address = {San Francisco, CA. USA.},
}

@inproceedings{joorabchi2013_real_challenges_in_mobile_app_development,
  author={Joorabchi, Mona Erfani and Mesbah, Ali and Kruchten, Philippe},  
  booktitle={2013 ACM / IEEE International Symposium on Empirical Software Engineering and Measurement}, 
  title={Real Challenges in Mobile App Development}, 
  year={2013},  
  volume={}, 
  number={}, 
  pages={15-24},
  publisher = {IEEE},
  address = {Baltimore, MD, USA},
  abstract={
    Context: Mobile app development is a relatively new phenomenon that is increasing rapidly due to the ubiquity and popularity of smartphones among end-users. Objective: The goal of our study is to gain an understanding of the main challenges developers face in practice when they build apps for different mobile devices. Method: We conducted a qualitative study, following a Grounded Theory approach, in which we interviewed 12 senior mobile developers from 9 different companies, followed by a semi-structured survey, with 188 respondents from the mobile development community. Results: The outcome is an overview of the current challenges faced by mobile developers in practice, such as developing apps across multiple platforms, lack of robust monitoring, analysis, and testing tools, and emulators that are slow or miss many features of mobile devices. Conclusion: Based on our findings of the current practices and challenges, we highlight areas that require more attention from the research and development community.},  
  keywords={},  
  doi={10.1109/ESEM.2013.9},
  ISSN={1949-3789}, 
  month={Oct},
}
  
@inproceedings{kalliamvakou2014_promises_and_perils_of_mining_github,
    author = {Kalliamvakou, Eirini and Gousios, Georgios and Blincoe, Kelly and Singer, Leif and German, Daniel M. and Damian, Daniela},
    title = {The Promises and Perils of Mining GitHub},
    year = {2014},
    isbn = {9781450328630},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/2597073.2597074},
    doi = {10.1145/2597073.2597074},
    abstract = { With over 10 million git repositories, GitHub is becoming one of the most important source of software artifacts on the Internet. Researchers are starting to mine the information stored in GitHub's event logs, trying to understand how its users employ the site to collaborate on software. However, so far there have been no studies describing the quality and properties of the data available from GitHub. We document the results of an empirical study aimed at understanding the characteristics of the repositories in GitHub and how users take advantage of GitHub's main features---namely commits, pull requests, and issues. Our results indicate that, while GitHub is a rich source of data on software development, mining GitHub for research purposes should take various potential perils into consideration. We show, for example, that the majority of the projects are personal and inactive; that GitHub is also being used for free storage and as a Web hosting service; and that almost 40\% of all pull requests do not appear as merged, even though they were. We provide a set of recommendations for software engineering researchers on how to approach the data in GitHub. },
    booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
    pages = {92–101},
    numpages = {10},
    keywords = {Mining software repositories, github, code reviews, git, bias},
    address = {Hyderabad, India},
    series = {MSR 2014}
}  

@inproceedings{kaasila2012_testdroid_etc,
    author = {Kaasila, Jouko and Ferreira, Denzil and Kostakos, Vassilis and Ojala, Timo},
    title = {Testdroid: Automated Remote UI Testing on Android},
    year = {2012},
    isbn = {9781450318150},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2406367.2406402},
    doi = {10.1145/2406367.2406402},
    abstract = {Open mobile platforms such as Android currently suffer from the existence of multiple versions, each with its own peculiarities. This makes the comprehensive testing of interactive applications challenging. In this paper we present Testdroid, an online platform for conducting scripted user interface tests on a variety of physical Android handsets. Testdroid allows developers and researchers to record test scripts, which along with their application are automatically executed on a variety of handsets in parallel. The platform reports the outcome of these tests, enabling developers and researchers to quickly identify platforms where their systems may crash or fail. At the same time the platform allows us to identify more broadly the various problems associated with each handset, as well as frequent programming mistakes.},
    booktitle = {Proceedings of the 11th International Conference on Mobile and Ubiquitous Multimedia},
    articleno = {28},
    numpages = {4},
    keywords = {applications, fragmentation, mobiles, performance, usability, user interface, remote testing},
    location = {Ulm, Germany},
    series = {MUM '12}
}
  
@inproceedings{kidwell2015_toward_fault_taxonomy_application_of_software_analytics,
  title={Toward a learned project-specific fault taxonomy: application of software analytics},
  author={Kidwell, Billy and Hayes, Jane Huffman},
  booktitle={2015 IEEE 1st International Workshop on Software Analytics (SWAN)},
  pages={1--4},
  year={2015},
  organization={IEEE},
  publisher = {IEEE},
  address = {Montreal, QC, Canada},
  doi = {10.1109/SWAN.2015.7070479},
  abstract = {This position paper argues that fault classification provides vital information for software analytics, and that machine learning techniques such as clustering can be applied to learn a project- (or organization-) specific fault taxonomy. Anecdotal evidence of this position is presented as well as possible areas of research for moving toward the posited goal.},
}

@inproceedings{kimbler_app_store_strategies_2010,  
    author={K. {Kimbler}},  
    booktitle={2010 14th International Conference on Intelligence in Next Generation Networks},   
    title={App store strategies for service providers},   
    year={2010},  
    volume={},  
    number={},  
    pages={1-5},  
    abstract={Apple App Store has introduced a substantially different model for digital good distribution to mobile users. Following Apple's spectacular success other smartphone and mobile OS vendors have started similar services. Mobile operators who invest hundreds of millions in 2,5G, 3G and now 4G technologies have a right to participate in the digital economy they enable. Can they quickly bite into the app store business and turn it to their advantage or they will be out of game again? This paper will try to answer these important questions by analysing the app store market and investigating potential app store strategies and scenarios for mobile operators.},  keywords={retailing;app store strategies;service providers;Apple app store;digital good distribution;smartphone;mobile OS vendors;digital economy;app store market;Mobile communication;Business;Smart phones;Mobile handsets;Games;app stores;business models;strategy},  doi={10.1109/ICIN.2010.5640947},
    month={Oct},
    address={Berlin, Germany},
    publisher={IEEE},
}

@inproceedings{lee2014_user_interaction_based_profiling_system_for_android_app_tuning,
    author = {Lee, Seokjun and Yoon, Chanmin and Cha, Hojung},
    title = {User Interaction-Based Profiling System for Android Application Tuning},
    year = {2014},
    isbn = {9781450329682},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/2632048.2636091},
    doi = {10.1145/2632048.2636091},
    abstract = {Quality improvement in mobile applications should be based on the consideration of several factors, such as users' diversity in spatio-temporal usage, as well as the device's resource usage, including battery life. Although application tuning should consider this practical issue, it is difficult to ensure the success of this process during the development stage due to the lack of information about application usage. This paper proposes a user interaction-based profiling system to overcome the limitations of development-level application debugging. In our system, the analysis of both device behavior and energy consumption is possible with fine-grained process-level application monitoring. By providing fine-grained information, including user interaction, system behavior, and power consumption, our system provides meaningful analysis for application tuning. The proposed method does not require the source code of the application and uses a web-based framework so that users can easily provide their usage data. Our case study with a few popular applications demonstrates that the proposed system is practical and useful for application tuning.},
    booktitle = {Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
    pages = {289–299},
    numpages = {11},
    keywords = {mobile application, tuning, debugging, webbased framework, profiling, user interaction},
    address = {Seattle, Washington},
    series = {UbiComp '14}
}

@inproceedings{li2016_an_investigation_into_the_use_of_common_libraries_in_android_apps,  
    author={Li, Li and Bissyandé, Tegawendé F. and Klein, Jacques and Le Traon, Yves},
    booktitle={2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering (SANER)}, 
    title={An Investigation into the Use of Common Libraries in Android Apps}, 
    year={2016}, 
    volume={1}, 
    number={},
    publisher = {IEEE},
    address = {Osaka, Japan},
    pages={403-414}, 
    abstract={
      The packaging model of Android apps requires the entire code necessary for the execution of an app to be shipped into one single apk file. Thus, an analysis of Android apps often visits code which is not part of the functionality delivered by the app. Such code is often contributed by the common libraries which are used pervasively by all apps. Unfortunately, Android analyses, e.g., for piggybacking detection and malware detection, can produce inaccurate results if they do not take into account the case of library code, which constitute noise in app features. Despite some efforts on investigating Android libraries, the momentum of Android research has not yet produced a complete set of common libraries to further support in-depth analysis of Android apps. In this paper, we leverage a dataset of about 1.5 million apps from Google Play to harvest potential common libraries, including advertisement libraries. With several steps of refinements, we finally collect by far the largest set of 1,113 libraries supporting common functionality and 240 libraries for advertisement. We use the dataset to investigates several aspects of Android libraries, including their popularity and their proportion in Android app code. Based on these datasets, we have further performed several empirical investigations to confirm the motivations behind our work.},  
    keywords={},  
    doi={10.1109/SANER.2016.52}, 
    ISSN={},  
    month={March},
}

@inproceedings{li2020_where_shall_we_log,
  title = {Where Shall We Log? Studying and Suggesting Logging Locations in Code Blocks},
  author = {Zhenhao Li and Tse-Hsun (Peter) Chen and Weiyi Shang},
  year = {2020},
  publisher = {IEEE},
  pages = {12},
  booktitle={2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)},
  address = {Virtual Event, Australia},
}

@inproceedings{linares2013_api_change_and_fault_proneness_android,
  title={API change and fault proneness: a threat to the success of Android apps},
  author={Linares-V{\'a}squez, Mario and Bavota, Gabriele and Bernal-C{\'a}rdenas, Carlos and Di Penta, Massimiliano and Oliveto, Rocco and Poshyvanyk, Denys},
  booktitle={Proceedings of the 2013 9th joint meeting on foundations of software engineering},
  pages={477--487},
  year={2013},
  publisher = {ACM},
  address = {Sacramento, CA , USA},
  abstract = {
    During the recent years, the market of mobile software applications (apps) has maintained an impressive upward trajectory. Many small and large software development companies invest considerable resources to target available opportunities. As of today, the markets for such devices feature over 850K+ apps for Android and 900K+ for iOS. Availability, cost, functionality, and usability are just some factors that determine the success or lack of success for a given app. Among the other factors, reliability is an important criteria: users easily get frustrated by repeated failures, crashes, and other bugs; hence, abandoning some apps in favor of others.
    
    This paper reports a study analyzing how the fault- and change-proneness of APIs used by 7,097 (free) Android apps relates to applications' lack of success, estimated from user ratings. Results of this study provide important insights into a crucial issue: making heavy use of fault- and change-prone APIs can negatively impact the success of these apps.},
}

@inproceedings{linares2015_mining_android_app_execution_traces_etc,  
  author={M. {Linares-Vásquez} and M. {White} and C. {Bernal-Cárdenas} and K. {Moran} and D. {Poshyvanyk}},
  booktitle={2015 IEEE/ACM 12th Working Conference on Mining Software Repositories},
  title={Mining Android App Usages for Generating Actionable GUI-Based Execution Scenarios},
  year={2015},
  volume={},
  number={},
  pages={111-122},
  abstract={
    GUI-based models extracted from Android app execution traces, events, or source code can be extremely useful for challenging tasks such as the generation of scenarios or test cases. However, extracting effective models can be an expensive process. Moreover, existing approaches for automatically deriving GUI-based models are not able to generate scenarios that include events which were not observed in execution (nor event) traces. In this paper, we address these and other major challenges in our novel hybrid approach, coined as MONKEYLAB. Our approach is based on the Record→Mine→Generate→Validate framework, which relies on recording app usages that yield execution (event) traces, mining those event traces and generating execution scenarios using statistical language modeling, static and dynamic analyses, and validating the resulting scenarios using an interactive execution of the app on a real device. The framework aims at mining models capable of generating feasible and fully replayable (i.e., actionable) scenarios reflecting either natural user behavior or uncommon usages (e.g., corner cases) for a given app. We evaluated MONKEYLAB in a case study involving several medium-to-large open-source Android apps. Our results demonstrate that MONKEYLAB is able to mine GUI-based models that can be used to generate actionable execution scenarios for both natural and unnatural sequences of events on Google Nexus 7 tablets.},  
  keywords={Graphical user interfaces;Testing;Androids;Humanoid robots;Vocabulary;Analytical models;History;GUI models;mobile apps;mining execution traces and event logs;language models},  
  doi={10.1109/MSR.2015.18},  
  ISSN={2160-1860},  
  month={May},
  publisher = {IEEE},
  address = {Florence, Italy},
}

@inproceedings{linares2017_how_do_developers_test_android_apps,
  author={Linares-Vásquez, Mario and Bernal-Cardenas, Cárlos and Moran, Kevin and Poshyvanyk, Denys},
  booktitle={2017 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
  title={How do Developers Test Android Applications?}, 
  year={2017},
  volume={},
  number={},
  pages={613-622},
  abstract={Enabling fully automated testing of mobile applications has recently become an important topic of study for both researchers and practitioners. A plethora of tools and approaches have been proposed to aid mobile developers both by augmenting manual testing practices and by automating various parts of the testing process. However, current approaches for automated testing fall short in convincing developers about their benefits, leading to a majority of mobile testing being performed manually. With the goal of helping researchers and practitioners - who design approaches supporting mobile testing - to understand developer's needs, we analyzed survey responses from 102 open source contributors to Android projects about their practices when performing testing. The survey focused on questions regarding practices and preferences of developers/testers in-the-wild for (i) designing and generating test cases, (ii) automated testing practices, and (iii) perceptions of quality metrics such as code coverage for determining test quality. Analyzing the information gleaned from this survey, we compile a body of knowledge to help guide researchers and professionals toward tailoring new automated testing approaches to the need of a diverse set of open source developers.},
  keywords={},
  doi={10.1109/ICSME.2017.47},
  ISSN={},
  month={Sep.},
  publisher = {IEEE},
  address = {Shanghai, China},
}

@inproceedings{liu2015measurement,
  title={A measurement-based study on application popularity in android and {iOS} app stores},
  author={Liu, Wei and Zhang, Ge and Chen, Jun and Zou, Yuze and Ding, Wenchao},
  booktitle={Proceedings of the 2015 Workshop on Mobile Big Data},
  publisher = {ACM},
  organization = {ACM},
  address = {Hangzhou, China},
  pages={13--18},
  year={2015},
  doi = {https://doi.org/10.1145/2757384.2757392},
}

@inproceedings{lu2016_PRADA,
  author={X. {Lu} and X. {Liu} and H. {Li} and T. {Xie} and Q. {Mei} and D. {Hao} and G. {Huang} and F. {Feng}},
  booktitle={2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE)},
  title={PRADA: Prioritizing Android Devices for Apps by Mining Large-Scale Usage Data},
  year={2016},
  volume={},
  number={},
  pages={3-13},
  abstract={
    Selecting and prioritizing major device models are critical for mobile app developers to select testbeds and optimize resources such as marketing and quality-assurance resources. The heavily fragmented distribution of Android devices makes it challenging to select a few major device models out of thousands of models available on the market. Currently app developers usually rely on some reported or estimated general market share of device models. However, these estimates can be quite inaccurate, and more problematically, can be irrelevant to the particular app under consideration. To address this issue, we propose PRADA, the first approach to prioritizing Android device models for individual apps, based on mining large-scale usage data. PRADA adapts the concept of operational profiling (popularly used in software reliability engineering) for mobile apps - the usage of an app on a specific device model reflects the importance of that device model for the app. PRADA includes a collaborative filtering technique to predict the usage of an app on different device models, even if the app is entirely new (without its actual usage in the market yet), based on the usage data of a large collection of apps. We empirically demonstrate the effectiveness of PRADA over two popular app categories, i.e., Game and Media, covering over 3.86 million users and 14,000 device models collected through a leading Android management app in China.},  
  keywords={collaborative filtering;data mining;smart phones;PRADA;prioritizing android devices for apps;large-scale usage data mining;operational profiling;mobile apps;collaborative filtering technique;Android management app;Androids;Humanoid robots;Data models;Games;Testing;Data mining;Mobile communication;Mobile apps;Android fragmentation;prioritization;usage data},  
  doi={10.1145/2884781.2884828},
  ISSN={1558-1225},
  month={May},
  publisher = {IEEE},
  address = {Austin, TX, USA},
}

@inproceedings{luhana2018streamlining,
  title={Streamlining mobile app deployment with Jenkins and Fastlane in the case of Catrobat's pocket code},
  author={Luhana, Kirshan Kumar and Schindler, Christian and Slany, Wolfgang},
  booktitle={2018 IEEE International Conference on Innovative Research and Development (ICIRD)},
  pages={1--6},
  year={2018},
  organization={IEEE},
  publisher={IEEE},
  address={Bangkok},
}

@inproceedings{makhdoumi2014_from_information_bottleneck_to_the_privacy_funnel,
  author={A. {Makhdoumi} and S. {Salamatian} and N. {Fawaz} and M. {Médard}},
  booktitle={2014 IEEE Information Theory Workshop (ITW 2014)},
  title={From the Information Bottleneck to the Privacy Funnel},
  year={2014},
  volume={},
  number={},
  pages={501-505},
  abstract={
    We focus on the privacy-utility trade-off encountered by users who wish to disclose some information to an analyst, that is correlated with their private data, in the hope of receiving some utility. We rely on a general privacy statistical inference framework, under which data is transformed before it is disclosed, according to a probabilistic privacy mapping. We show that when the log-loss is introduced in this framework in both the privacy metric and the distortion metric, the privacy leakage and the utility constraint can be reduced to the mutual information between private data and disclosed data, and between non-private data and disclosed data respectively. We justify the relevance and generality of the privacy metric under the log-loss by proving that the inference threat under any bounded cost function can be upperbounded by an explicit function of the mutual information between private data and disclosed data. We then show that the privacy-utility tradeoff under the log-loss can be cast as the non-convex Privacy Funnel optimization, and we leverage its connection to the Information Bottleneck, to provide a greedy algorithm that is locally optimal. We evaluate its performance on the US census dataset. Finally, we characterize the optimal privacy mapping for the Gaussian Privacy Funnel.},
  keywords={data privacy;greedy algorithms;information bottleneck;privacy-utility trade-off;probabilistic privacy mapping;log-loss;privacy leakage;utility constraint;disclosed data;nonprivate data;privacy metric;bounded cost function;privacy-utility tradeoff;nonconvex privacy funnel optimization;greedy algorithm;US census dataset;optimal privacy mapping;Gaussian privacy funnel;Privacy;Data privacy;Optimization;Mutual information;Greedy algorithms;Distortion measurement},
  doi={10.1109/ITW.2014.6970882},
  ISSN={1662-9019},
  month={Nov},
}

@inproceedings{malavolta2020_android_runner,
    author = {Malavolta, Ivano and Grua, Eoin Martino and Lam, Cheng-Yu and de Vries, Randy and Tan, Franky and Zielinski, Eric and Peters, Michael and Kaandorp, Luuk},
    title = {A Framework for the Automatic Execution of Measurement-Based Experiments on Android Devices},
    year = {2020},
    isbn = {9781450381284},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3417113.3422184},
    doi = {10.1145/3417113.3422184},
    abstract = {Conducting measurement-based experiments is fundamental for assessing the quality of Android apps in terms of, e.g., energy consumption, CPU, and memory usage. However, orchestrating such experiments is not trivial as it requires large boilerplate code, careful setup of measurement tools, and the adoption of various empirical best practices scattered across the literature. All together, those factors are slowing down the scientific advancement and harming experiments' replicability in the mobile software engineering area.In this paper we present Android Runner (AR), a framework for automatically executing measurement-based experiments on native and web apps running on Android devices. In AR, an experiment is defined once in a descriptive fashion, and then its execution is fully automatic, customizable, and replicable. AR is implemented in Python and it can be extended with third-party profilers.AR has been used in more than 25 scientific studies primarily targeting performance and energy efficiency.},
    booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering Workshops},
    pages = {61–66},
    numpages = {6},
    location = {Virtual Event, Australia},
    series = {ASE '20}
}

﻿@InProceedings{medina2020_what_do_we_know_about_hackathons_etc_a_SLR,
    author={Medina Angarita, Maria Angelica and Nolte, Alexander},
    editor={Nolte, Alexander
    and Alvarez, Claudio
    and Hishiyama, Reiko
    and Chounta, Irene-Angelica
    and Rodr{\'i}guez-Triana, Mar{\'i}a Jes{\'u}s
    and Inoue, Tomoo},
    title={What Do We Know About Hackathon Outcomes and How to Support Them? -- A Systematic Literature Review},
    booktitle={Collaboration Technologies and Social Computing},
    year={2020},
    publisher={Springer International Publishing},
    address={Cham},
    pages={50-64},
    abstract={Hackathons are time-bounded events where participants gather in teams to develop projects that interest them. Such events have been adopted in various domains to generate innovative solutions, foster learning, build and expand communities and to tackle civic and ecological issues. While research interest has also grown subsequently, most studies focus on singular events in specific domains. A systematic overview of the current state of the art is currently missing. Such an overview is however crucial to further study the hackathon phenomenon, understand its underlying mechanisms and develop support for hackathon organizers, in particular related to the sustainability of hackathon outcomes. This paper fills that gap by reporting on the results of a systematic literature review thus providing an overview of potential hackathon outcomes, design aspects and connections between them that have been addressed in prior work. Our findings also outline gaps in prior work e.g. related to the lack of work focusing on hackathon outcomes other than hackathon projects.},
    isbn={978-3-030-58157-2}
}


@inproceedings{menzies2019take,
  title={Take Control:(On the Unreasonable Effectiveness of Software Analytics)},
  author={Menzies, Tim},
  booktitle={2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)},
  pages={265--266},
  year={2019},
  organization={IEEE},
  publisher={IEEE},
  address={Montreal, QC, Canada},
}

@inproceedings{merdes2006_ubiquitous_RATs_resource_aware_runtime_tests_improve_reliability,
  author = {Merdes, Matthias and Malaka, Rainer and Suliman, Dima and Paech, Barbara and Brenner, Daniel and Atkinson, Colin},
  title = {Ubiquitous RATs: How Resource-Aware Run-Time Tests Can Improve Ubiquitous Software Systems},
  year = {2006},
  isbn = {1595935851},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/1210525.1210538},
  doi = {10.1145/1210525.1210538},
  abstract = {
    In this paper we describe a new approach for increasing the reliability of ubiquitous software systems. This is achieved by executing tests at run-time. The individual software components are consequently accompanied by executable tests. We augment this well-known built-in test (BIT) paradigm by combining it with resource-awareness. Starting from the constraints for such resource-aware tests (RATs) we derive their design and describe a number of strategies for executing such tests under resource constraints as well as the necessary middleware. Our approach is especially beneficial to ubiquitous software systems due to their dynamic nature - which prevents a static verification of their reliability - and their inherent resource limitations.},
  booktitle = {Proceedings of the 6th International Workshop on Software Engineering and Middleware},
  pages = {55–62},
  numpages = {8},
  keywords = {MORABIT, resource-aware test (RAT), built-in test (BIT), ubiquitous software, run-time testing},
  address = {Portland, Oregon},
  series = {SEM '06},
  comments = {I've not cited this paper, it's here for future reference. If and when we use in-app analytics to drive testing this paper will be relevant.}
}

@inproceedings{mili2014_on_faults_and_faulty_programs,
  title={On faults and faulty programs},
  author={Mili, Ali and Frias, Marcelo F and Jaoua, Ali},
  booktitle={International Conference on Relational and Algebraic Methods in Computer Science},
  pages={191--207},
  year={2014},
  organization={Springer},
  publisher = {Springer},
  address = {Marienstatt, Germany},
  abstract = {A fault is an attribute of a program that precludes it from satisfying its specification; while this definition may sound clear-cut, it leaves many details unspecified. An incorrect program may be corrected in many different ways, involving different numbers of modifications. Hence neither the location nor the number of of faults may be defined in a unique manner; this, in turn, sheds a cloud of uncertainty on such concepts as fault density, and fault forecasting. In this paper, we present a more precise definition of a program fault, that has the following properties: it recognizes that the same incorrect behavior may be remedied in more than one way; it recognizes that removing a fault does not necessarily make the program correct, but may make it less incorrect (in a sense to be defined); it characterizes fault removals that make the program less incorrect, as opposed to fault removals that may remedy one aspect of program behavior at the expense of others; it recognizes that isolating a fault in a program is based on implicit assumptions about the remaining program parts; it identifies instances when a fault may be localized in a program with absolute certainty.},
  keywords = {faults, faulty programs, correctness, relative correctness, refinement, contingent fault, definite fault, fault removal, monotonic fault removal},
}

@inproceedings{miluzzo2010research_in_the_app_store_era,
  title={Research in the app store era: Experiences from the cenceme app deployment on the iphone},
  author={Miluzzo, Emiliano and Lane, Nicholas D and Lu, Hong and Campbell, Andrew T},
  booktitle = {UbiComp '10},
  pages = {4},
  year={2010},
  publisher = {ACM},
  address = {Copenhagen, Denmark},
}

@inproceedings{minelli2013_software_analytics_samoa,
  title={Software analytics for mobile applications--insights \& lessons learned},
  author={Minelli, Roberto and Lanza, Michele},
  booktitle={2013 17th European Conference on Software Maintenance and Reengineering},
  pages={144--153},
  year={2013},
  organization={IEEE},
  publisher={IEEE},
  address={Genova, Italy},
}

@inproceedings{moran2016_automatically_drr_android_app_crashes,  
  author={K. {Moran} and M. {Linares-Vásquez} and C. {Bernal-Cárdenas} and C. {Vendome} and D. {Poshyvanyk}},
  booktitle={2016 IEEE International Conference on Software Testing, Verification and Validation (ICST)},
  title={Automatically Discovering, Reporting and Reproducing Android Application Crashes},   
  year={2016},
  volume={},
  number={},
  pages={33-44},
  publisher = {IEEE},
  address = {Chicago, IL. USA.},
  abstract={Mobile developers face unique challenges when detecting and reporting crashes in apps due to their prevailing GUI event-driven nature and additional sources of inputs (e.g., sensor readings). To support developers in these tasks, we introduce a novel, automated approach called CRASHSCOPE. This tool explores a given Android app using systematic input generation, according to several strategies informed by static and dynamic analyses, with the intrinsic goal of triggering crashes. When a crash is detected, CRASHSCOPE generates an augmented crash report containing screenshots, detailed crash reproduction steps, the captured exception stack trace, and a fully replayable script that automatically reproduces the crash on a target device(s). We evaluated CRASHSCOPE's effectiveness in discovering crashes as compared to five state-of-the-art Android input generation tools on 61 applications. The results demonstrate that CRASHSCOPE performs about as well as current tools for detecting crashes and provides more detailed fault information. Additionally, in a study analyzing eight real-world Android app crashes, we found that CRASHSCOPE's reports are easily readable and allow for reliable reproduction of crashes by presenting more explicit information than human written reports.},
  keywords={Android (operating system);graphical user interfaces;mobile computing;program diagnostics;Android application crashes;mobile developers;GUI event-driven nature;CRASHSCOPE;systematic input generation;static analyses;dynamic analyses;crash reproduction steps;exception stack trace;fault information;human written reports;Computer crashes;Graphical user interfaces;Androids;Humanoid robots;Testing;Mobile communication;Systematics;android;crash reports;GUI-testing},
  doi={10.1109/ICST.2016.34},
  ISSN={},
  month={April},
}

@inproceedings{moran2017_crashscope_a_practical_tool_for_automated_testing_of_android_apps,  
  author={Moran, Kevin and Linares-Vasquez, Mario and Bernal-Cardenas, Carlos and Vendome, Christopher and Poshyvanyk, Denys},  
  booktitle={2017 IEEE/ACM 39th International Conference on Software Engineering Companion (ICSE-C)},   
  title={CrashScope: A Practical Tool for Automated Testing of Android Applications},   
  year={2017},  
  volume={},  
  number={},  
  pages={15-18},  
  abstract={
    Unique challenges arise when testing mobile applications due to their prevailing event-driven nature and complex contextual features (e.g. sensors, notifications). Current automated input generation approaches for Android apps are typically not practical for developers to use due to required instrumentation or platform dependence and generally do not effectively exercise contextual features. To better support developers in mobile testing tasks, in this demo we present a novel, automated tool called CRASHSCOPE. This tool explores a given Android app using systematic input generation, according to several strategies informed by static and dynamic analyses, with the intrinsic goal of triggering crashes. When a crash is detected, CRASHSCOPE generates an augmented crash report containing screen shots, detailed crash reproduction steps, the captured exception stack trace, and a fully replay able script that automatically reproduces the crash on a target device(s). Results of preliminary studies show that CRASHSCOPE is able to uncover about as many crashes as other state of the art tools, while providing detailed useful crash reports and test scripts to developers. Website: www.android-dev-tools.com/crashscope-home Video url: https://youtu.be/ii6S1JF6xDw.},
  keywords={},  
  doi={10.1109/ICSE-C.2017.16},  
  ISSN={},
  month={May},
  publisher = {IEEE},
  address = {Buenos Aires, Argentina},
}

@inproceedings{mueller2019_pocketcode,  
  author={Müller, Matthias and Schindler, Christian and Slany, Wolfgang},  
  booktitle={2019 IEEE/ACM 6th International Conference on Mobile Software Engineering and Systems (MOBILESoft)},   
  title={Pocket Code - A Mobile Visual Programming Framework for App Development},   
  year={2019},  
  volume={},  
  number={},  
  pages={140-143},  
  abstract={
    Software development more and more focuses on mobile and connected IoT solutions. Whereas a variety of frameworks is available for professional developers, also the need comes up to deliver apps fast for personal use or rapid prototyping. Innovative ideas, in whatever context, must be realizable without having a vast amount of resources or deep domain-knowledge needed. In this work we present Pocket Code, a free open source mobile visual coding framework for Android and iOS with various hardware extensions, enabling everyone to create powerful apps directly on mobiles in short time. Especially the included visual coding-bricks for Arduino boards and Raspberry Pis also allow more sophisticated programs not only in an educational context to be developed with Pocket Code. In contrast to existing solutions, the presented app does not require any PC setting and is therefore making the development of apps mobile and broadening it to an even wider audience.},
  keywords={},
  doi={10.1109/MOBILESoft.2019.00027},
  ISSN={},
  month={May},
  publisher = {IEEE},
  address = {Montreal, QC, Canada},
}

@InProceedings{munappy2020_data_pipeline_management_in_practice_challenges_and_opportunities,
    author="Munappy, Aiswarya Raj
    and Bosch, Jan
    and Olsson, Helena Homstr{\"o}m",
    editor="Morisio, Maurizio
    and Torchiano, Marco
    and Jedlitschka, Andreas",
    title="Data Pipeline Management in Practice: Challenges and Opportunities",
    booktitle="Product-Focused Software Process Improvement",
    year="2020",
    publisher="Springer International Publishing",
    address="Cham",
    pages="168--184",
    abstract="Data pipelines involve a complex chain of interconnected activities that starts with a data source and ends in a data sink. Data pipelines are important for data-driven organizations since a data pipeline can process data in multiple formats from distributed data sources with minimal human intervention, accelerate data life cycle activities, and enhance productivity in data-driven enterprises. However, there are challenges and opportunities in implementing data pipelines but practical industry experiences are seldom reported. The findings of this study are derived by conducting a qualitative multiple-case study and interviews with the representatives of three companies. The challenges include data quality issues, infrastructure maintenance problems, and organizational barriers. On the other hand, data pipelines are implemented to enable traceability, fault-tolerance, and reduce human errors through maximizing automation thereby producing high-quality data. Based on multiple-case study research with five use cases from three case companies, this paper identifies the key challenges and benefits associated with the implementation and use of data pipelines.",
    isbn="978-3-030-64148-1",
    doi = {10.1007/978-3-030-64148-1_11},
}


@inproceedings{nagappan2016_future_trends_in_sw_eng_for_mobile_apps,  
  author={M. {Nagappan} and E. {Shihab}},
  booktitle={2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering (SANER)},   
  title={Future Trends in Software Engineering Research for Mobile Apps},   
  year={2016},  
  volume={5},  
  number={},  
  pages={21-32},  
  abstract={
    There has been tremendous growth in the use of mobile devices over the last few years. This growth has fueled the development of millions of software applications for these mobile devices often called as 'apps'. Current estimates indicate that there are hundreds of thousands of mobile app developers. As a result, in recent years, there has been an increasing amount of software engineering research conducted on mobile apps to help such mobile app developers. In this paper, we discuss current and future research trends within the framework of the various stages in the software development life-cycle: requirements (including non-functional), design and development, testing, and maintenance. While there are several non-functional requirements, we focus on the topics of energy and security in our paper, since mobile apps are not necessarily built by large companies that can afford to get experts for solving these two topics. For the same reason we also discuss the monetizing aspects of a mobile app at the end of the paper. For each topic of interest, we first present the recent advances done in these stages and then we present the challenges present in current work, followed by the future opportunities and the risks present in pursuing such research.},  
  keywords={Mobile communication;Software engineering;Feature extraction;Software;Smart phones;Google;Mobile apps;Mining app markets},  
  doi={10.1109/SANER.2016.88},
  ISSN={},
  month={March},
  publisher = {IEEE},
  address = {Osaka, Japan},
}

@inproceedings{nayebi2016release,
  title={Release Practices for Mobile Apps--What do Users and Developers Think?},
  author={Nayebi, Maleknaz and Adams, Bram and Ruhe, Guenther},
  booktitle={2016 {IEEE} 23rd international conference on software analysis, evolution, and reengineering {(SANER)}},
  volume={1},
  pages={552--562},
  year={2016},
  organization={IEEE},
  publisher = {IEEE},
  address = {Suita, Japan},
}

@inproceedings{nayebi2017version,
  title={Which version should be released to app store?},
  author={Nayebi, Maleknaz and Farahi, Homayoon and Ruhe, Guenther},
  booktitle={2017 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
  pages={324--333},
  year={2017},
  organization={IEEE},
  publisher = {IEEE},
  address = {Toronto, ON, Canada}
}

@inproceedings{nitze2015_a_survey_on_mobile_users_sq_perceptions_and_expectations,  
  author={Nitze, André and Schmietendorf, Andreas},  
  booktitle={2015 IEEE Eighth International Conference on Software Testing, Verification and Validation Workshops (ICSTW)}, 
  title={A survey on mobile users' software quality perceptions and expectations}, 
  year={2015},
  volume={},
  number={}, 
  pages={1-2},
  abstract={
    Software quality can be looked at from many perspectives. Software quality assurance techniques can improve internal and external quality properties of applications. However, the users' perceptions of and expectations on the resulting software product are pivotal to the products' economic success. In this contribution the results of a survey on quality aspects of mobile applications (apps) with 144 participants are presented. The results are discussed in terms of implications for developers and product managers of mobile application development projects. This work is based on a previous evaluation of mobile quality assurance aspects of mobile application development.},  
  keywords={},
  doi={10.1109/ICSTW.2015.7107417},
  ISSN={}, 
  month={April},
  organization={IEEE},
  publisher = {IEEE},
  address = {Graz, Austria},
}

@inproceedings{olesen2020_10_years_of_hackathons,
  author = {Falk Olesen, Jeanette and Halskov, Kim},
  title = {10 Years of Research With and On Hackathons},
  year = {2020},
  isbn = {9781450369749},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3357236.3395543},
  doi = {10.1145/3357236.3395543},
  abstract = {Hackathon formats have been praised for their potential for promoting innovative thinking and making in a short time-frame. For this reason, hackathons have also been embraced by many researchers who use hackathons as part of their research in various ways. Through an extensive review of 381 publications published during a 10 year time span, we document the multiple ways in which hackathons are embraced and used by researchers The paper contributes to a better understanding of hackathons as part of research by providing a broad overview as a resource for researchers. We identify three main motivations for using hackathons as part of research: 1) Structuring learning, 2) structuring processes, and 3) enabling participation. For each of the motivations, we identify research with hackathons, and research on hackathons as two main categories. Drawing on several examples from the review we discuss benefits and challenges of using hackathons as part of research.},
  booktitle = {Proceedings of the 2020 ACM Designing Interactive Systems Conference},
  pages = {1073–1088},
  numpages = {16},
  keywords = {research method, literature review, hackathons},
  address = {Eindhoven, Netherlands},
  series = {DIS '20}
}

@inproceedings{paakkonen2009_communication_in_testing,
  author    = {Tuula P{\"{a}}{\"{a}}akk{\"{o}}nen and
               Jorma Sajaniemi},
  title     = {Communication in Testing: Improvements for Testing Management},
  booktitle = {Proceedings of the 21st Annual Workshop of the Psychology of Programming
               Interest Group, {PPIG} 2009, Limerick, Ireland, June 24-26, 2009},
  pages     = {12},
  publisher = {Psychology of Programming Interest Group},
  address   = {Limerick, Ireland},
  year      = {2009},
  url       = {http://ppig.org/library/paper/communication-testing-improvements-testing-management},
  timestamp = {Fri, 08 Jun 2018 11:42:28 +0200},
  biburl    = {https://dblp.org/rec/conf/ppig/PaaakkonenS09.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  abstract = {
    Testing in companies with highly competitive environments has many opportunities and challenges. Testing is diversified, both contextually and geographically: there are many testing goals, many ongoing parallel projects, many testing phases and all this needs to be managed so that the full view of testing is visible for management and reporting. Testing is also linked to project communication and psychology: communication needs to be constructive, and testers and test leaders should have good interpersonal skills. It really does matter how a failure or test report is formulated.
    In order to localize testing problems in a large software-intensive company, we conducted a current state analysis via web-based questionnaire among experienced testing practitioners, most having at least 5 years experience in these tasks. The scope of the survey was decided to keep broad with the goal of finding future improvements within tool, process and method development. In this paper, we will concentrate on those survey results that have psychological underlying. Based on the results, we suggest a set of improvements for testing and, especially, test reporting. Implementation of these improvements is still underway, but the findings and suggestions provide insight into psychology of testing.
  },
  pdf = {https://ppig.org/files/2009-PPIG-21st-paakkonen.pdf},
}

@INPROCEEDINGS{panichella2015_how_can_i_improve_my_app_classifying_user_reviews_for_sw_maintenance_and_evolution,  
    author={Panichella, Sebastiano and Di Sorbo, Andrea and Guzman, Emitza and Visaggio, Corrado A. and Canfora, Gerardo and Gall, Harald C.},  
    booktitle={2015 IEEE International Conference on Software Maintenance and Evolution (ICSME)},  
    title={How can i improve my app? Classifying user reviews for software maintenance and evolution},  
    year={2015},  
    volume={},
    number={}, 
    pages={281-290},  
    abstract={
        App Stores, such as Google Play or the Apple Store, allow users to provide feedback on apps by posting review comments and giving star ratings. These platforms constitute a useful electronic mean in which application developers and users can productively exchange information about apps. Previous research showed that users feedback contains usage scenarios, bug reports and feature requests, that can help app developers to accomplish software maintenance and evolution tasks. However, in the case of the most popular apps, the large amount of received feedback, its unstructured nature and varying quality can make the identification of useful user feedback a very challenging task. In this paper we present a taxonomy to classify app reviews into categories relevant to software maintenance and evolution, as well as an approach that merges three techniques: (1) Natural Language Processing, (2) Text Analysis and (3) Sentiment Analysis to automatically classify app reviews into the proposed categories. We show that the combined use of these techniques allows to achieve better results (a precision of 75\% and a recall of 74\%) than results obtained using each technique individually (precision of 70\% and a recall of 67\%).
    },  
    keywords={}, 
    doi={10.1109/ICSM.2015.7332474},
    ISSN={},  
    month={Sep.},
}

@inproceedings{parate2016_RECKON_an_analytics_framework_for_app_developers_HP_AppPulseMobile,
    author = {Parate, Abhinav and Jain, Puneet and Kim, Kyu-Han},
    title = {RECKON: An Analytics Framework for App Developers},
    year = {2016},
    isbn = {9781450342544},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/2980147.2980154},
    doi = {10.1145/2980147.2980154},
    abstract = {This paper argues that there is a need to expand the capabilities of mobile app analytics beyond providing low-level insights about how efficiently a user can execute an action on a mobile app, to deeper insights about how efficiently a user can complete a task using the app (e.g., flight reservation). This paper presents RECKON, a framework for mobile app analytics, that provides insights about end-to-end user experience in completing tasks, without explicit effort from the app developers. RECKON identifies and extracts task-level information from an unlabeled data stream of user actions. Based on the extracted information, RECKON outputs various helpful metrics to the developers. We have implemented and evaluated RECKON with a popular travel assistant app with 122, 243 users for 30 days. Our evaluation results validate RECKON's performance in obtaining critical mobile app insights as well as demonstrate its wide applicability for developers.},
    booktitle = {Proceedings of the 2nd Workshop on Experiences in the Design and Implementation of Smart Objects},
    pages = {23–28},
    numpages = {6},
    keywords = {developer, app analytics, task, user interface},
    address = {New York City, New York},
    series = {SmartObjects '16}
}

@inproceedings{partachi2020_flexme_untangling_commits,
    author = {P\^{a}rundefinedachi, Profir-Petru and Dash, Santanu Kumar and Allamanis, Miltiadis and Barr, Earl T.},
    title = {Flexeme: Untangling Commits Using Lexical Flows},
    year = {2020},
    isbn = {9781450370431},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/3368089.3409693},
    doi = {10.1145/3368089.3409693},
    abstract = {Today, most developers bundle changes into commits that they submit to a shared code
    repository. Tangled commits intermix distinct concerns, such as a bug fix and a new
    feature. They cause issues for developers, reviewers, and researchers alike: they
    restrict the usability of tools such as git bisect, make patch comprehension more
    difficult, and force researchers who mine software repositories to contend with noise.
    We present a novel data structure, the ��-NFG, a multiversion Program Dependency Graph
    augmented with name flows. A ��-NFG directly and simultaneously encodes different
    program versions, thereby capturing commits, and annotates data flow edges with the
    names/lexemes that flow across them. Our technique, Flexeme, builds a ��-NFG from
    commits, then applies Agglomerative Clustering using Graph Similarity to that ��-NFG
    to untangle its commits. At the untangling task on a C# corpus, our implementation,
    Heddle, improves the state-of-the-art on accuracy by 0.14, achieving 0.81, in a fraction
    of the time: Heddle is 32 times faster than the previous state-of-the-art.},
    booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
    pages = {63–74},
    numpages = {12},
    keywords = {graph kernels, clustering, commint untangling},
    address = {Virtual Event, USA},
    series = {ESEC/FSE 2020}
}

@inproceedings{patro2013_capturing_mobile_experience_in_the_wild,
    author = {Patro, Ashish and Rayanchu, Shravan and Griepentrog, Michael and Ma, Yadi and Banerjee, Suman},
    title = {Capturing Mobile Experience in the Wild: A Tale of Two Apps},
    year = {2013},
    isbn = {9781450321013},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/2535372.2535391},
    doi = {10.1145/2535372.2535391},
    abstract = {We present a long term and large scale study of the experience of mobile users through
    two popular but contrasting applications in the wild. To conduct this study, we implemented
    a measurement framework and library, called Insight, which has been deployed on these
    two applications that are available through Apple's App Store and Google's Android
    Market. One of them, Parallel Kingdom (PK), is a popular massively multiplayer online
    role-playing game (MMORPG) which has over a million unique users distributed more
    than 120 countries. The other application, StudyBlue (SB), is an educational application
    with over 160,000 unique users. Our study spans most of the life of the PK game (more
    than 3 years) while our deployment with SB has been running for over a year now. We
    use Insight to collect diverse information about network behavior, application usage
    and footprints, platform statistics, user actions, and various factors affecting application
    revenues.},
    booktitle = {Proceedings of the Ninth ACM Conference on Emerging Networking Experiments and Technologies},
    pages = {199–210},
    numpages = {12},
    keywords = {network performance, application usage, mobile applications, mmorpg},
    address = {Santa Barbara, California, USA},
    series = {CoNEXT '13}
}

@inproceedings{pielot2015attention,
  title={When attention is not scarce-detecting boredom from mobile phone usage},
  author={Pielot, Martin and Dingler, Tilman and Pedro, Jose San and Oliver, Nuria},
  booktitle={Proceedings of the 2015 ACM international joint conference on pervasive and ubiquitous computing},
  pages={825--836},
  organization = {ACM},
  publisher = {ACM},
  address = {Osaka, Japan},
  year={2015}
}

@inproceedings{poeplau2014_execute_this_unsafe_android,
  title={Execute this! analyzing unsafe and malicious dynamic code loading in android applications.},
  author={Poeplau, Sebastian and Fratantonio, Yanick and Bianchi, Antonio and Kruegel, Christopher and Vigna, Giovanni},
  year={2014},
  pages={23-26},
  volume = {14},
  organization = {NDSS},
  publisher = {Internet Society},
  address = {San Diego, CA, USA},
  isbn = {1-891562-35-5},
}

@inproceedings{pokhrel2020_digitaltwin_for_cybersecurity,
  title={Digital Twin for Cybersecurity Incident Prediction: A Multivocal},
  author={Pokhrel, Abhishek and Katta, Vikash and Colomo-Palacios, Ricardo},
  year = {2020},
  booktitle={2nd International Workshop on Software Engineering Research \& Practices for the Internet of Things (SERP4IoT)},
  pages={671--678},
  organization={IEEE},
  publisher = {IEEE},
  address = {Seoul, South Korea},
}

@inproceedings{prochlo2017_strong_privacy_analytics_in_the_crowd_46411,
    author = {Bittau, Andrea and Erlingsson, \'{U}lfar and Maniatis, Petros and Mironov, Ilya and Raghunathan, Ananth and Lie, David and Rudominer, Mitch and Kode, Ushasree and Tinnes, Julien and Seefeld, Bernhard},
    title = {Prochlo: Strong Privacy for Analytics in the Crowd},
    year = {2017},
    isbn = {9781450350853},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/3132747.3132769},
    also_available_at = {https://arxiv.org/abs/1710.00901},
    doi = {10.1145/3132747.3132769},
    abstract = {The large-scale monitoring of computer users' software activities has become commonplace, e.g., for application telemetry, error reporting, or demographic profiling. This paper describes a principled systems architecture---Encode, Shuffle, Analyze (ESA)---for performing such monitoring with high utility while also protecting user privacy. The ESA design, and its Prochlo implementation, are informed by our practical experiences with an existing, large deployment of privacy-preserving software monitoring.With ESA, the privacy of monitored users' data is guaranteed by its processing in a three-step pipeline. First, the data is encoded to control scope, granularity, and randomness. Second, the encoded data is collected in batches subject to a randomized threshold, and blindly shuffled, to break linkability and to ensure that individual data items get "lost in the crowd" of the batch. Third, the anonymous, shuffled data is analyzed by a specific analysis engine that further prevents statistical inference attacks on analysis results.ESA extends existing best-practice methods for sensitive-data analytics, by using cryptography and statistical techniques to make explicit how data is elided and reduced in precision, how only common-enough, anonymous data is analyzed, and how this is done for only specific, permitted purposes. As a result, ESA remains compatible with the established workflows of traditional database analysis.Strong privacy guarantees, including differential privacy, can be established at each processing step to defend against malice or compromise at one or more of those steps. Prochlo develops new techniques to harden those steps, including the Stash Shuffle, a novel scalable and efficient oblivious-shuffling algorithm based on Intel's SGX, and new applications of cryptographic secret sharing and blinding. We describe ESA and Prochlo, as well as experiments that validate their ability to balance utility and privacy.},
    booktitle = {Proceedings of the 26th Symposium on Operating Systems Principles},
    pages = {441–459},
    numpages = {19},
    address = {Shanghai, China},
    series = {SOSP '17},
}

@inproceedings {ravindrath2012_appinsight_mobile_app_performance_in_the_wild,
    author = {Lenin Ravindranath and Jitendra Padhye and Sharad Agarwal and Ratul Mahajan and Ian Obermiller and Shahin Shayandeh},
    title = {{AppInsight}: Mobile App Performance Monitoring in the Wild},
    booktitle = {10th USENIX Symposium on Operating Systems Design and Implementation (OSDI 12)},
    year = {2012},
    isbn = {978-1-931971-96-6},
    address = {Hollywood, CA},
    pages = {107--120},
    url = {https://www.usenix.org/conference/osdi12/technical-sessions/presentation/ravindranath},
    publisher = {USENIX Association},
    month = oct,
}

@inproceedings{ravindrath2014_automatic_and_scalable_fault_detection_for_mobile_apps,
    author = {Ravindranath, Lenin and Nath, Suman and Padhye, Jitendra and Balakrishnan, Hari},
    title = {Automatic and Scalable Fault Detection for Mobile Applications},
    year = {2014},
    isbn = {9781450327930},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2594368.2594377},
    doi = {10.1145/2594368.2594377},
    abstract = {This paper describes the design, implementation, and evaluation of VanarSena, an automated fault finder for mobile applications (``apps''). The techniques in VanarSena are driven by a study of 25 million real-world crash reports of Windows Phone apps reported in 2012. Our analysis indicates that a modest number of root causes are responsible for many observed failures, but that they occur in a wide range of places in an app, requiring a wide coverage of possible execution paths. VanarSena adopts a ``greybox'' testing method, instrumenting the app binary to achieve both coverage and speed. VanarSena runs on cloud servers: the developer uploads the app binary; VanarSena then runs several app ``monkeys'' in parallel to emulate user, network, and sensor data behavior, returning a detailed report of crashes and failures. We have tested VanarSena with 3000 apps from the Windows Phone store, finding that 1108 of them had failures; VanarSena uncovered 2969 distinct bugs in existing apps, including 1227 that were not previously reported. Because we anticipate VanarSena being used in regular regression tests, testing speed is important. VanarSena uses two techniques to improve speed. First, it uses a ``hit testing'' method to quickly emulate an app by identifying which user interface controls map to the same execution handlers in the code. Second, it generates a ProcessingCompleted event to accurately determine when to start the next interaction. These features are key benefits of VanarSena's greybox philosophy.},
    booktitle = {Proceedings of the 12th Annual International Conference on Mobile Systems, Applications, and Services},
    pages = {190–203},
    numpages = {14},
    keywords = {testing, reliability, mobile applications, software engineering},
    location = {Bretton Woods, New Hampshire, USA},
    series = {MobiSys '14}
}

@inproceedings{razaghpanah2018_apps_trackers_privacy_and_regulators_a_global_study_of_the_mobile_tracking_ecosystem,
  title={Apps, trackers, privacy, and regulators: A global study of the mobile tracking ecosystem},
  author={Razaghpanah, Abbas and Nithyanand, Rishab and Vallina-Rodriguez, Narseo and Sundaresan, Srikanth and Allman, Mark and Kreibich, Christian and Gill, Phillipa and others},
  booktitle={The 25th Annual Network and Distributed System Security Symposium (NDSS 2018)},
  year={2018},
  url = {http://hdl.handle.net/20.500.12761/507},
  abstraact = {
    Third-party services form an integral part of the mobile ecosystem: they ease application development and enable features such as analytics, social network integration, and app monetization through ads. However, aided by the general opacity of mobile systems, such services are also largely invisible to users. This has negative consequences for user privacy as third-party services can potentially track users without their consent, even across multiple applications. Using real-world mobile traffic data gathered by the Lumen Privacy Monitor (Lumen), a privacy enhancing app with the ability to analyze network traffic on mobile devices in user space, we present insights into the mobile advertising and tracking ecosystem and its stakeholders. In this study, we develop automated methods to detect third-party advertising and tracking services at the traffic level. Using this technique we identify 2,121 such services, of which 233 were previously unknown to other popular advertising and tracking blacklists. We then uncover the business relationships between the providers of these services and characterize them by their prevalence in the mobile and Web ecosystem. Our analysis of the privacy policies of the largest advertising and tracking service providers shows that sharing harvested data with subsidiaries and third-party affiliates is the norm. Finally, we seek to identify the services likely to be most impacted by privacy regulations such as the European General Data Protection Regulation (GDPR) and ePrivacy directives.
  },
  pages = {1--15},
}

@inproceedings{rekimoto1999_time_machine_computing,
    author = {Rekimoto, Jun},
    title = {Time-Machine Computing: A Time-Centric Approach for the Information Environment},
    year = {1999},
    isbn = {1581130759},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/320719.322582},
    doi = {10.1145/320719.322582},
    abstract = {This paper describes the concept of Time-Machine Computing (TMC), a time-centric approach to organizing information on computers. A system based on Time-Machine Computing allows a user to visit the past and the future states of computers. When a user needs to refer to a document that he/she was working on at some other time, he/she can travel in the time dimension and the system restores the computer state at that time. Since the user's activities on the system are automatically archived, the user's daily workspace is seamlessly integrated into the information archive. The combination of spatial information management of the desktop metaphor and time traveling allows a user to organize and archive information without being bothered by folder hierarchies or the file classification problems that are common in today's desktop environments. TMC also provides a mechanism for linking multiple applications and external information sources by exchanging time information. This paper describes the key features of TMC, a time-machine desktop environment called “TimeScape,” and several time-oriented application integration examples.},
    booktitle = {Proceedings of the 12th Annual ACM Symposium on User Interface Software and Technology},
    pages = {45–54},
    numpages = {10},
    keywords = {desktop environment, information visualization, time-machine computing, document management, time traveling, inter-application communication},
    address = {Asheville, North Carolina, USA},
    series = {UIST '99}
}

@inproceedings{riganelli2019benchmark_android_data_loss_bugs,
  title={A benchmark of data loss bugs for android apps},
  author={Riganelli, Oliviero and Mobilio, Marco and Micucci, Daniela and Mariani, Leonardo},
  booktitle={2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR)},
  pages={582--586},
  year={2019},
  organization={IEEE},
  publisher = {IEEE},
  address = {Montreal, QC, Canada},
}

@inproceedings{sama2009using_jinjector,
  title={Using code instrumentation to enhance testing on J2ME: a lesson learned with JInjector},
  author={Sama, Michele and Harty, Julian},
  booktitle={Proceedings of the 10th workshop on Mobile Computing Systems and Applications},
  pages={1--6},
  year={2009},
  organization = {ACM},
  publisher = {ACM},
  address = {Santa Cruz, CA, USA}, 
  doi = {10.1145/1514411.1514424},
}

@inproceedings{santos2016_investigating_the_adoption_of_agile_practices_by_20_undergrad_students_in_mobile_app_devt,
    author = {Santos, Alan and Kroll, Josiane and Sales, Afonso and Fernandes, Paulo and Wildt, Daniel},
    title = {Investigating the Adoption of Agile Practices in Mobile Application Development},
    year = {2016},
    isbn = {9789897581878},
    publisher = {SCITEPRESS - Science and Technology Publications, Lda},
    address = {Setubal, PRT},
    url = {https://doi.org/10.5220/0005835404900497},
    doi = {10.5220/0005835404900497},
    abstract = {The mobile application development market has been dramatically growing in the last few years as the complexity of its applications and speed of software development process. These changes in the mobile development market require a rethinking on the way the software development should be performed by teams. In order to better understand how agile practices support mobile application development, we applied a questionnaire to 20 undergraduate students. These students have been training in an iOS development course combined with agile practices. Our study aims to identify challenges and to report the students experience on the adoption of agile practices to develop mobile applications. Our findings reveal that agile practices help mobile software development mainly in terms of project management and control and development speed. However, aspects of user interface and user experience, different development platforms, and users expectations still point challenges in developing mobile applications.},
    booktitle = {Proceedings of the 18th International Conference on Enterprise Information Systems},
    pages = {490–497},
    numpages = {8},
    keywords = {Mobile Application, Benefits., Software Development, Challenges, Agile Practices, Software Engineering},
    location = {Rome, Italy},
    series = {ICEIS 2016}
}

@inproceedings{sarro2015_feature_lifecycles_in_appstores,
    author={F. {Sarro} and A. A. {Al-Subaihin} and M. {Harman} and Y. {Jia} and W. {Martin} and Y. {Zhang}},  
    booktitle={2015 IEEE 23rd International Requirements Engineering Conference (RE)},   
    title={Feature lifecycles as they spread, migrate, remain, and die in App Stores},
    publisher = {IEEE},
    address = {Ottawa, ON, Canada},
    year={2015},  
    volume={},  
    number={},  
    pages={76-85},  
    abstract={We introduce a theoretical characterisation of feature lifecycles in app stores, to help app developers to identify trends and to find undiscovered requirements. To illustrate and motivate app feature lifecycle analysis, we use our theory to empirically analyse the migratory and non-migratory behaviours of 4,053 non-free features from two App Stores (Samsung and BlackBerry). The results reveal that, in both stores, intransitive features (those that neither migrate nor die out) exhibit significantly different behaviours with regard to important properties, such as their price. Further correlation analysis also highlights differences between trends relating price, rating, and popularity. Our results indicate that feature lifecycle analysis can yield insights that may also help developers to understand feature behaviours and attribute relationships.},  keywords={formal specification;smart phones;application feature lifecycle analysis;empirical analysis;migratory behaviours;nonmigratory behaviours;nonfree features;Samsung App Stores;BlackBerry App Stores;intransitive features;correlation analysis;price factor;rating factor;popularity factor;feature behaviours;attribute relationships;Feature extraction;Data mining;Databases;Software;HTML;Market research;Natural language processing},  
    doi={10.1109/RE.2015.7320410},  
    ISSN={2332-6441},  
    month={Aug},
    relevant_text = {The App Store marketplace can be thought of as a highly user-participatory cyclic development model that partly involves “requirements for the masses; requirements from the masses”. In this adaptive development space, users express their needs and desires by voting apps up and down and contributing product reviews. Users also tacitly express support for a feature by downloading apps that offer it. Developers may observe this behaviour and respond accordingly by adding popular features, where appropriate, to their own products. In this way, the developers triage the perceived desires of their users and make strategic decisions as to which features to adopt [7 - A case study of post-deployment user feedback triage].
    
    Capturing user reactions helps developers to select and prioritise feature inclusions in the next releases [4 - Analysis of user comments: An approach for software requirements evolution].}
}

@inproceedings{10.1145/2632168.2632169,
    author = {Sasnauskas, Raimondas and Regehr, John},
    title = {Intent Fuzzer: Crafting Intents of Death},
    year = {2014},
    isbn = {9781450329347},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/2632168.2632169},
    doi = {10.1145/2632168.2632169},
    abstract = { We present a fuzzing framework for Intents: the core IPC mechanism for intra- and inter-app communication in Android. Since intents lie at a trust boundary between apps, their correctness is important and thorough testing is warranted. The key challenge is to balance the tension between generating intents that applications expect, permitting deep penetration into application logic, and generating intents that trigger interesting bugs that have not been previously uncovered. Our work strikes this balance using a novel combination of static analysis and random test-case generation. Our intent fuzzer crashed dozens of Google core and top Google Play apps, resulting in app restarts or even in a complete OS reboot. },
    booktitle = {Proceedings of the 2014 Joint International Workshop on Dynamic Analysis (WODA) and Software and System Performance Testing, Debugging, and Analytics (PERTEA)},
    pages = {1–5},
    numpages = {5},
    keywords = {fuzz testing, static analysis, Android IPC, random testing},
    address = {San Jose, CA, USA},
    series = {WODA+PERTEA 2014}
}
@inproceedings{scaffidi2007_toawards_a_calculus_of_confidence,  
    author={Scaffidi, Christopher and Shaw, Mary},  
    booktitle={2007 First International Workshop on the Economics of Software and Computation}, 
    title={Toward a Calculus of Confidence},  
    year={2007},
    volume={}, 
    number={}, 
    pages={7-7},  
    abstract={
      Programmers, and end-user programmers in particular, often have difficulty evaluating software, data, and communication components for reuse in new software systems, which effectively reduces the value programmers derive from those components. End-user programmers are especially ill equipped to exercise the customary high-ceremony means of evaluating software quality. We seek effective ways to use low-ceremony sources of evidence, such as online reviews and reputation data, to make components' quality attributes easier to establish, thereby facilitating more effective selection of components for reuse. Achieving this will require identifying sources of low-ceremony evidence, designing the meta-information required to track the differing sources and levels of credibility of various sources of evidence, and developing a method for combining pieces of disparate information into overall estimates of component value.
    },  
    keywords={}, 
    doi={10.1109/ESC.2007.9},
    ISSN={}, 
    month={May},
    publisher = {IEEE},
    address = {Minneapolis, MN, USA},
}

@inproceedings{scaffidi2007developing,
  title={Developing confidence in software through credentials and low-ceremony evidence},
  author={Scaffidi, Christopher and Shaw, Mary},
  booktitle={International Workshop on Living with Uncertainties},
  year={2007},
  address = {Atlanta, GA, USA},
  numpages = {3},
  publisher = {IEEE/ACM},
  source_of_paper = {http://se.cs.toronto.edu/IWLU/papers/Confidence_Scaffidi.pdf},
  month = "1",
  url = "https://kilthub.cmu.edu/articles/journal_contribution/Developing_Confidence_in_Software_through_Credentials_and_Low-Ceremony_Evidence/6621953",
  doi = "10.1184/R1/6621953.v1",
  abstract = {
    Conventional software specifications and reasoning based on such specifications do not accommodate uncertainty in the specifications, nor do they support the informal, subjective sorts of reasoning that many people use when making decisions about complex systems. We propose a notation for representing specifications in which attributes have different levels of confidence and we discuss ways that uncertain information can contribute usefully to software decisions.
  }
}

@inproceedings{schranz2019_contributors_impact_on_a_foss_project_quality_catrobat,
    author = {Schranz, Thomas and Schindler, Christian and M\"{u}ller, Matthias and Slany, Wolfgang},
    title = {Contributors’ Impact on a FOSS Project’s Quality},
    year = {2019},
    isbn = {9781450368575},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3340495.3342754},
    doi = {10.1145/3340495.3342754},
    abstract = {Engaging contributors in a Free Open Source Software (FOSS) project can be challenging. Finding an appropriate task to start with is a common entrance barrier for newcomers. Poor code quality contributes to difficulties in the onboarding process and limits contributor satisfaction in general. In turn, dissatisfied developers tend to exacerbate problems with system integrity. Poorly designed systems are difficult to maintain and extend. Users can often directly experience these issues as instabilities in system behavior. Thus code quality is a key issue for users and contributors in FOSS. We present a case study on the interactions between code quality and contributor experience in the real-world FOSS project Catrobat. We describe the implications of a refactoring process in terms of code metrics and benefits for developers and users.},
    booktitle = {Proceedings of the 2nd ACM SIGSOFT International Workshop on Software Qualities and Their Dependencies},
    pages = {35–38},
    numpages = {4},
    keywords = {software evolution, contributor experience, FOSS, software quality},
    location = {Tallinn, Estonia},
    series = {SQUADE 2019},
}

@inproceedings{seneviratne2015_a_measurement_study_of_tracking_in_paid_mobile_apps,
    author = {Seneviratne, Suranga and Kolamunna, Harini and Seneviratne, Aruna},
    title = {A Measurement Study of Tracking in Paid Mobile Applications},
    year = {2015},
    isbn = {9781450336239},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/2766498.2766523},
    doi = {10.1145/2766498.2766523},
    abstract = {Smartphone usage is tightly coupled with the use of apps that can be either free or paid. Numerous studies have investigated the tracking libraries associated with free apps. Only a limited number of these have focused on paid apps. As expected, these investigations indicate that tracking is happening to a lesser extent in paid apps, yet there is no conclusive evidence. This paper provides the first large-scale study of paid apps. We analyse top paid apps obtained from four different countries: Australia, Brazil, Germany, and US, and quantify the level of tracking taking place in paid apps in comparison to free apps. Our analysis shows that 60\% of the paid apps are connected to trackers that collect personal information compared to 85\%--95\% in free apps. We further show that approximately 20\% of the paid apps are connected to more than three trackers. With tracking being pervasive in both free and paid apps, we then quantify the aggregated privacy leakages associated with individual users. Using the data of user installed apps of over 300 smartphone users, we show that 50\% of the users are exposed to more than 25 trackers which can result in significant leakages of privacy.},
    booktitle = {Proceedings of the 8th ACM Conference on Security \& Privacy in Wireless and Mobile Networks},
    articleno = {7},
    numpages = {6},
    address = {New York, New York},
    series = {WiSec '15}
}


@inproceedings{shen2017_towards_release_strategy_optimization_for_apps_in_google_play,
    author = {Shen, Sheng and Lu, Xuan and Hu, Ziniu and Liu, Xuanzhe},
    title = {Towards Release Strategy Optimization for Apps in Google Play},
    year = {2017},
    isbn = {9781450353137},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/3131704.3131710},
    doi = {10.1145/3131704.3131710},
    abstract = {In the appstore-centric ecosystem, app developers have an urgent requirement to optimize their release strategy to maximize user adoption of their apps. To address this problem, we introduce an approach to assisting developers to select the proper release opportunity based on the purpose of the update and current condition of the app. Before that, we propose the update interval to characterize release patterns of apps, and find significance of the updates through empirical analysis. We mined the release-history data of 17,820 apps from 33 categories in Google Play, over a period of 105 days. With 41,028 releases identified from these apps, we reveal important characteristics of update intervals and how these factors can influence update effects. We suggest developers to synthetically consider app ranking, rating trend, and update purpose in addition to the timing of releasing an app version. We propose a Multinomial Naive Bayes model to help decide an optimal release opportunity to gain better user adoption.},
    booktitle = {Proceedings of the 9th Asia-Pacific Symposium on Internetware},
    articleno = {1},
    numpages = {10},
    keywords = {update interval, release strategy, app store, Mobile apps},
    address = {Shanghai, China},
    series = {Internetware'17}
}

@inproceedings{shklovski2014_leakiness_and_creepiness_in_app_space_perceptions_of_privacy_and_mobile_app_use,
    author = {Shklovski, Irina and Mainwaring, Scott D. and Sk\'{u}lad\'{o}ttir, Halla Hrund and Borgthorsson, H\"{o}skuldur},
    title = {Leakiness and Creepiness in App Space: Perceptions of Privacy and Mobile App Use},
    year = {2014},
    isbn = {9781450324731},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2556288.2557421},
    doi = {10.1145/2556288.2557421},
    abstract = {Mobile devices are playing an increasingly intimate role in everyday life. However,
    users can be surprised when informed of the data collection and distribution activities
    of apps they install. We report on two studies of smartphone users in western European
    countries, in which users were confronted with app behaviors and their reactions assessed.
    Users felt their personal space had been violated in "creepy" ways. Using Altman's
    notions of personal space and territoriality, and Nissenbaum's theory of contextual
    integrity, we account for these emotional reactions and suggest that they point to
    important underlying issues, even when users continue using apps they find creepy.},
    booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
    pages = {2347–2356},
    numpages = {10},
    keywords = {bodily integrity, mobile devices, creepiness, data privacy, learned helplessness},
    location = {Toronto, Ontario, Canada},
    series = {CHI '14},
    remarks = {
      The differences between users saying they distrust the app store, and their behaviours which imply they DO trust the apps not to misbehave by continuing to use apps even if they feel the data collection is 'creepy'.
    }
}

@inproceedings{sigmund2015_views_on_internal_and_external_validity_in_ESE,  
  author={Siegmund, Janet and Siegmund, Norbert and Apel, Sven}, 
  booktitle={2015 IEEE/ACM 37th IEEE International Conference on Software Engineering},  
  title={Views on Internal and External Validity in Empirical Software Engineering}, 
  year={2015},
  volume={1}, 
  number={},  
  pages={9-19},  
  abstract={
    Empirical methods have grown common in software engineering, but there is no consensus on how to apply them properly. Is practical relevance key? Do internally valid studies have any value? Should we replicate more to address the tradeoff between internal and external validity? We asked the community how empirical research should take place in software engineering, with a focus on the tradeoff between internal and external validity and replication, complemented with a literature review about the status of empirical research in software engineering. We found that the opinions differ considerably, and that there is no consensus in the community when to focus on internal or external validity and how to conduct and review replications.
  },
  keywords={},
  doi={10.1109/ICSE.2015.24},
  ISSN={1558-1225},
  month={May},
}

@inproceedings{silva2016_an_analysis_of_automated_tests_for_mobile_android_apps,  
  author={Bernardo Silva, Davi and Endo, Andre Takeshi and Eler, Marcelo Medeiros and Durelli, Vinicius H. S.}, 
  booktitle={2016 XLII Latin American Computing Conference (CLEI)},  
  title={An analysis of automated tests for mobile Android applications},  
  year={2016}, 
  volume={}, 
  number={}, 
  pages={1-9},  
  abstract={
    Mobile computing has become ubiquitous, thus the amount and complexity of mobile applications have challenged software engineering practices. To overcome the challenges brought by mobile applications, the adoption of repeatable, systematic and mainly automated tests have been researched. In this paper, we look into open source projects in hopes of identifying how automated tests are applied to mobile applications developed for the Android platform. We analyzed the automated tests to identify the frameworks adopted, the relation between the production and the testing code, and how the following challenges have been dealt with: connectivity, rich GUIs, limited resources, sensors, and multiple configurations.
  },
  keywords={},
  doi={10.1109/CLEI.2016.7833334}, 
  ISSN={},
  month={Oct},
}

@inproceedings{Silva:2019:RCS:3339076.3339130,
     author = {Silva, Rodrigo F. G. and Roy, Chanchal K. and Rahman, Mohammad Masudur and Schneider, Kevin A. and Paixao, Klerisson and de Almeida Maia, Marcelo},
     title = {Recommending Comprehensive Solutions for Programming Tasks by Mining Crowd Knowledge},
     booktitle = {Proceedings of the 27th International Conference on Program Comprehension},
     series = {ICPC '19},
     year = {2019},
     address = {Montreal, Quebec, Canada},
     pages = {358--368},
     numpages = {11},
     url = {https://doi.org/10.1109/ICPC.2019.00054},
     doi = {10.1109/ICPC.2019.00054},
     acmid = {3339130},
     publisher = {IEEE Press},
     address = {Piscataway, NJ, USA},
     keywords = {mining crowd knowledge, stack overflow, word embedding},
} 

@inproceedings{spadini2018_pydriller,
  author = {Spadini, Davide and Aniche, Maur\'{\i}cio and Bacchelli, Alberto},
  title = {PyDriller: Python Framework for Mining Software Repositories},
  year = {2018},
  isbn = {9781450355735},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3236024.3264598},
  doi = {10.1145/3236024.3264598},
  abstract = {Software repositories contain historical and valuable information about the overall development of software systems. Mining software repositories (MSR) is nowadays considered one of the most interesting growing fields within software engineering. MSR focuses on extracting and analyzing data available in software repositories to uncover interesting, useful, and actionable information about the system. Even though MSR plays an important role in software engineering research, few tools have been created and made public to support developers in extracting information from Git repository. In this paper, we present PyDriller, a Python Framework that eases the process of mining Git. We compare our tool against the state-of-the-art Python Framework GitPython, demonstrating that PyDriller can achieve the same results with, on average, 50\% less LOC and significantly lower complexity.  URL: https://github.com/ishepard/pydriller  Materials: https://doi.org/10.5281/zenodo.1327363  Pre-print: https://doi.org/10.5281/zenodo.1327411},
  booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages = {908–911},
  numpages = {4},
  keywords = {Mining Software Repositories, Python, Git, GitPython},
  address = {Lake Buena Vista, FL, USA},
  series = {ESEC/FSE 2018}
}

@inproceedings{steglich2019_revisiting_the_mobile_app_ecosystem,
  title={Revisiting the mobile software ecosystems literature},
  author={Steglich, Caio and Marczak, Sabrina and Guerra, Luiz Pedro and Mosmann, Luiz Henrique and Perin, Marcelo and Figueira Filho, Fernando and de Souza, Cleidson},
  booktitle={2019 IEEE/ACM 7th International Workshop on Software Engineering for Systems-of-Systems (SESoS) and 13th Workshop on Distributed Software Development, Software Ecosystems and Systems-of-Systems (WDES)},
  pages={50--57},
  year={2019},
  organization={IEEE},
  publisher={IEEE},
  address={Montreal, QC, Canada},
  doi={10.1109/SESoS/WDES.2019.00015},
  abstract={Software Ecosystems are comprised of a technology platform, business models, internal and external developers, and engaging users. The popularity of smartphones brought along the mobile software ecosystems, such as iOS and Android, which are composed of a platform, a community of users and developers, mobile applications, and online application store, and evangelists that often promote the ecosystem. Given the recent nature of the topic, this paper aims to revisit the state-of-the-art through a systematic literature mapping. We found 63 publications on the topic of mobile software ecosystems that were categorized by year (almost 50\% of the publications are from 2015 and on), by author (a few collaboration clusters were identified), and by the mobile ecosystems characteristics (most publications discuss business or technical aspects) and elements (applications and the platform are the most discussed topics followed by the developers and the users). Our results provide an up-to-date map of the topic for those interested in mobile software ecosystems.}
}

@inproceedings{strahm2018_mobile_app_onboarding,
    author = {Strahm, Brendan and Gray, Colin M. and Vorvoreanu, Mihaela},
    title = {Generating Mobile Application Onboarding Insights Through Minimalist Instruction},
    year = {2018},
    isbn = {9781450351980},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/3196709.3196727},
    doi = {10.1145/3196709.3196727},
    abstract = {Mobile application designers use onboarding task flows to help first time users learn and engage with key application functionality. Although some guidelines for designing onboarding flows have been offered by practitioners, a systematic, research-informed approach is needed. In this paper, we present the creation of a method for designing mobile application onboarding experiences. We used the minimalist instruction framework to engage twelve university students in an iterative set of design and evaluation activities. Participants interacted with a physical prototype of an educational badging mobile application through a semi-structured exploration and reflection activity, bookended by structured mini-interviews. We found that this method facilitated engagement with participants' meaning-making processes, resulting in useful design insights and the creation of an onboarding task flow. Research opportunities for integrating instructional design and learning approaches in HCI in the context of onboarding are considered.},
    booktitle = {Proceedings of the 2018 Designing Interactive Systems Conference},
    pages = {361–372},
    numpages = {12},
    keywords = {mobile., onboarding, user experience, minimalist instruction, design methods},
    address = {Hong Kong, China},
    series = {DIS '18}
}

@inproceedings{syer2013_empirical_findings_for_mobile_apps,
    author = {Syer, Mark D. and Nagappan, Meiyappan and Hassan, Ahmed E. and Adams, Bram},
    title = {Revisiting Prior Empirical Findings for Mobile Apps: An Empirical Case Study on the 15 Most Popular Open-Source Android Apps},
    year = {2013},
    publisher = {IBM Corp.},
    abstract = {Our increasing reliance on mobile devices has led to the explosive development of millions of mobile apps across multiple platforms that are used by millions of people around the world every day. However, most software engineering research is performed on large desktop or server-side software applications (e.g., Eclipse and Apache). Unlike the software applications that we typically study, mobile apps are 1) designed to run on devices with limited, but diverse, resources (e.g., limited screen space and touch interfaces with diverse gestures) and 2) distributed through centralized "app stores," where there is a low barrier to entry and heavy competition. Hence, mobile apps may differ from traditionally studied desktop or server side applications, the extent that existing software development "best practices" may not apply to mobile apps. Therefore, we perform an exploratory study, comparing mobile apps to commonly studied large applications and smaller applications along two dimensions: the size of the code base and the time to fix defects. Finally, we discuss the impact of our findings by identifying a set of unique software engineering challenges posed by mobile apps.},
    booktitle = {Proceedings of the 2013 Conference of the Center for Advanced Studies on Collaborative Research},
    pages = {283–297},
    numpages = {15},
    address = {Ontario, Canada},
    series = {CASCON '13},
}

@INPROCEEDINGS{tan2018_repairing_crashes_in_android_apps,  
    author={Tan, Shin Hwei and Dong, Zhen and Gao, Xiang and Roychoudhury, Abhik},  
    booktitle={2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE)},   
    title={Repairing Crashes in Android Apps},  
    year={2018}, 
    volume={},
    number={}, 
    pages={187-198},  
    abstract={
      Android apps are omnipresent, and frequently suffer from crashes - leading to poor user experience and economic loss. Past work focused on automated test generation to detect crashes in Android apps. However, automated repair of crashes has not been studied. In this paper, we propose the first approach to automatically repair Android apps, specifically we propose a technique for fixing crashes in Android apps. Unlike most test-based repair approaches, we do not need a test-suite; instead a single failing test is meticulously analyzed for crash locations and reasons behind these crashes. Our approach hinges on a careful empirical study which seeks to establish common root-causes for crashes in Android apps, and then distills the remedy of these root-causes in the form of eight generic transformation operators. These operators are applied using a search-based repair framework embodied in our repair tool Droix. We also prepare a benchmark DroixBench capturing reproducible crashes in Android apps. Our evaluation of Droix on DroixBench reveals that the automatically produced patches are often syntactically identical to the human patch, and on some rare occasion even better than the human patch (in terms of avoiding regressions). These results confirm our intuition that our proposed transformations form a sufficient set of operators to patch crashes in Android.
    },  
    keywords={},  
    doi={10.1145/3180155.3180243},  
    ISSN={1558-1225},  
    month={May},
    publisher = {IEEE},
    address = {Gothenburg, Sweden},
}

@inproceedings{tang2015_software_designers_satisfice,
	location = {Cham},
	title = {Software Designers Satisfice},
	isbn = {978-3-319-23727-5},
	abstract = {The software architecture community has advocated design rationale in the last decade. However, there is little knowledge of how much reasoning is performed when software design judgments are made. In this study, we investigated the amount of design reasoning performed before making a decision. We recruited 32 students and 40 professionals to participate in this software architecture design study. We found that most subjects needed only a few reasons before making their decisions. They considered that giving a few reasons were good enough to judge despite that more reasons could be found. This result shows a satisficing behavior in design decision making. We explore the implications of this common behavior on software architecture design.},
	pages = {105--120},
	booktitle = {Software Architecture},
	publisher = {Springer International Publishing},
	author = {Tang, Antony and van Vliet, Hans},
	editor = {Weyns, Danny and Mirandola, Raffaela and Crnkovic, Ivica},
	date = {2015},
	year = {2015},
	address = {Dubrovnik/Cavtat, Croatia},
}

@inproceedings{upadhyay2017_articulating_the_construction_of_a_web_scraper_for_massive_data_extraction,  
  author={Upadhyay, Shreya and Pant, Vishal and Bhasin, Shivansh and Pattanshetti, Mahantesh K.},  
  booktitle={2017 Second International Conference on Electrical, Computer and Communication Technologies (ICECCT)}, 
  title={Articulating the construction of a web scraper for massive data extraction},  
  year={2017},  
  volume={}, 
  number={}, 
  publisher = {IEEE},
  address = {Coimbatore, India},
  pages={1-4},
  abstract={
    Massive volumes of data are generated by various users, entities, applications and disseminated online. This copious volume of big data is distributed across millions of websites and is available for various applications. Search engines do provide a simple mechanism to access this data. Accessing this data using search engines requires a user to spend time and resources to manually click and download. Clearly, such a manual approach is not scalable for a vast majority of real life applications at the enterprise and organization level. There exist a number of automated approaches to data extraction from the web. Most of these approaches are ad-hoc and domain specific. Therefore, the need for a robust, automated, easy to use framework for extracting content from the web with a minimal human effort across domains appears enticing. The architecture proposed by the authors for a web scraper addresses this gap to harvest data from the web. The proposed web scraping framework offers an easy and feasible approach for parsing and extracting data on a large scale from multiple websites with minimal human intervention. This paper provides an insight into issues relevant to constructing a web scraper and concludes by describing the implementation of a web scraper for harvesting learning objects for an eLearning application.},
  keywords={},
  doi={10.1109/ICECCT.2017.8117827}, 
  ISSN={},  
  month={Feb},
}

@inproceedings{ujwal2017_classification_based_adaptive_web_scraper,  
  author={Ujwal, B.V.S. and Gaind, Bharat and Kundu, Abhishek and Holla, Anusha and Rungta, Mukund},
  booktitle={2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA)},
  title={Classification-Based Adaptive Web Scraper},
  year={2017}, 
  volume={},
  number={}, 
  publisher = {IEEE},
  address = {Cancun, Mexico},
  pages={125-132},
  doi={10.1109/ICMLA.2017.0-168},
}

@inproceedings{not_yet_cited_vargas2020_selecting_thirdparty_libraries_the_practitioners_perspective,
    author = {Larios Vargas, Enrique and Aniche, Maur\'{\i}cio and Treude, Christoph and Bruntink, Magiel and Gousios, Georgios},
    title = {Selecting Third-Party Libraries: The Practitioners’ Perspective},
    year = {2020},
    isbn = {9781450370431},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3368089.3409711},
    doi = {10.1145/3368089.3409711},
    abstract = {The selection of third-party libraries is an essential element of virtually any software development project. However, deciding which libraries to choose is a challenging practical problem. Selecting the wrong library can severely impact a software project in terms of cost, time, and development effort, with the severity of the impact depending on the role of the library in the software architecture, among others. Despite the importance of following a careful library selection process, in practice, the selection of third-party libraries is still conducted in an ad-hoc manner, where dozens of factors play an influential role in the decision.  In this paper, we study the factors that influence the selection process of libraries, as perceived by industry developers. To that aim, we perform a cross-sectional interview study with 16 developers from 11 different businesses and survey 115 developers that are involved in the selection of libraries. We systematically devised a comprehensive set of 26 technical, human, and economic factors that developers take into consideration when selecting a software library. Eight of these factors are new to the literature. We explain each of these factors and how they play a role in the decision. Finally, we discuss the implications of our work to library maintainers, potential library users, package manager developers, and empirical software engineering researchers.},
    booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
    pages = {245–256},
    numpages = {12},
    keywords = {library selection, APIs, empirical software engineering, library adoption, software libraries},
    location = {Virtual Event, USA},
    series = {ESEC/FSE 2020}
}

@inproceedings{viennot2014_a_measurement_study_of_google_play,
  author = {Viennot, Nicolas and Garcia, Edward and Nieh, Jason},
  title = {A Measurement Study of Google Play},
  year = {2014},
  isbn = {9781450327893},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/2591971.2592003},
  doi = {10.1145/2591971.2592003},
  abstract = {
    Although millions of users download and use third-party Android applications from the Google Play store, little information is known on an aggregated level about these applications. We have built PlayDrone, the first scalable Google Play store crawler, and used it to index and analyze over 1,100,000 applications in the Google Play store on a daily basis, the largest such index of Android applications. PlayDrone leverages various hacking techniques to circumvent Google's roadblocks for indexing Google Play store content, and makes proprietary application sources available, including source code for over 880,000 free applications. We demonstrate the usefulness of PlayDrone in decompiling and analyzing application content by exploring four previously unaddressed issues: the characterization of Google Play application content at large scale and its evolution over time, library usage in applications and its impact on application portability, duplicative application content in Google Play, and the ineffectiveness of OAuth and related service authentication mechanisms resulting in malicious users being able to easily gain unauthorized access to user data and resources on Amazon Web Services and Facebook.},
  booktitle = {The 2014 ACM International Conference on Measurement and Modeling of Computer Systems},
  pages = {221–233},
  numpages = {13},
  keywords = {google play, authentication, oauth, android, mobile computing, decompilation, security, clone detection},
  address = {Austin, Texas, USA},
  series = {SIGMETRICS '14},
  status = {MUST-DO include in related works.},
}

@inproceedings{wang2017_exploratory_study_of_the_mobile_app_ecosystem,
    author = {Wang, Haoyu and Liu, Zhe and Guo, Yao and Chen, Xiangqun and Zhang, Miao and Xu, Guoai and Hong, Jason},
    title = {An Explorative Study of the Mobile App Ecosystem from App Developers’ Perspective},
    abstract = {With the prevalence of smartphones, app markets such as Apple App Store and Google Play has become the center stage in the mobile app ecosystem, with millions of apps developed by tens of thousands of app developers in each major market. This paper presents a study of the mobile app ecosystem from the perspective of app developers. Based on over one million Android apps and 320,000 developers from Google Play, we analyzed the Android app ecosystem from different aspects. Our analysis shows that while over half of the developers have released only one app in the market, many of them have released hundreds of apps. We classified developers into different groups based on the number of apps they have released, and compared their characteristics. Specially, we have analyzed the group of aggressive developers who have released more than 50 apps, trying to understand how and why they create so many apps. We also investigated the privacy behaviors of app developers, showing that some developers have a habit of producing apps with low privacy ratings. Our study shows that understanding the behavior of mobile developers can be helpful to not only other app developers, but also to app markets and mobile users.},
    year = {2017},
    isbn = {9781450349130},
    publisher = {International World Wide Web Conferences Steering Committee},
    address_of_publisher = {Republic and Canton of Geneva, CHE},
    url = {https://doi.org/10.1145/3038912.3052712},
    doi = {10.1145/3038912.3052712},
    booktitle = {Proceedings of the 26th International Conference on World Wide Web},
    pages = {163–172},
    numpages = {10},
    keywords = {app clone, app developers, app ecosystem, google play, mobile apps, mobile privacy, android},
    address = {Perth, Australia},
    series = {WWW ’17}
}
  
@inproceedings{wang2018_beyond_google_play,
  title={Beyond google play: A large-scale comparative study of Chinese android app markets},
  author={Wang, Haoyu and Liu, Zhe and Liang, Jingyue and Vallina-Rodriguez, Narseo and Guo, Yao and Li, Li and Tapiador, Juan and Cao, Jingcun and Xu, Guoai},
  booktitle={Proceedings of the Internet Measurement Conference 2018},
  pages={293--307},
  year={2018},
  publisher = {ACM},
  address = {Boston MA, USA},
  relevance_to_phd={In China there are many app stores, both vendor-specific and from large tech companies. These are used by 100M's of users. This paper researches these app stores in depth: 6M apps from 16 Chinese app markets + Google Play. Do any of these app stores provide an Android Vitals like service? Two have a "Quality Rating". Will some of those apps' usage be sent to Google Play?
  Their figure 2 helps provide backing evidence that the apps we assessed, particularly with Kiwix, cover the vast majority of the distribution.},
  quote_1 = {"538,283 developers in Google Play in 2018"},
  quote_2 = {Discusses "user download" counts in Google Play and other app stores. Figure 2 provides a distribution of downloads across markets, and correlates to my assessment of where&when Android Vitals provides reports.},
  quote_3 = {We further analyzed the release or update time of these apps across markets. This is also a metric used for estimating whether developers actively maintain their apps, a strong signal for code quality},
}

@INPROCEEDINGS{not_cited_yet_wang2019_understanding_ineffective_events_and_reducing_test_sequences_for_android_applications__with_monkey,  
    author={Wang, Ping and Yan, Jiwei and Deng, Xi and Yan, Jun and Zhang, Jian},  
    booktitle={2019 International Symposium on Theoretical Aspects of Software Engineering (TASE)},   
    title={Understanding Ineffective Events and Reducing Test Sequences for Android Applications},  
    year={2019}, 
    volume={}, 
    number={}, 
    pages={264-272},  
    abstract={
        Monkey, which is integrated with the Android system, becomes the most widely used test input generation tool, owing to the simplicity, effectiveness and good compatibility. However, Monkey is based on coordinates of screen and oblivious to the widgets and the GUI states, which results in a great many ineffective events that have no contribution to the test. To address the major drawbacks, this paper parses the events of 200 test sequences generated by Monkey into human-readable scripts and manually investigate the effects of these events. We find three types of patterns on the ineffective events, including no-ops, single and combination of effect-free ones, and summarize them into ten rules for sequence reduction. Then, we implement a tool CHARD to match these patterns in real-world traces and prune the redundant events. The evaluation on 923 traces from various apps covering 16 categories shows that CHARD can process 1,000 events in a few seconds and identifies 41.3\% events as ineffective ones. Meanwhile, the reduced sequence keeps the same functionality with the original one that can trigger the same behaviors. Our work can be applied to lessen the diagnose effort for record-and-replay, and as a preprocessing step for other works on analyzing sequences. For instance, CHARD can remove 72.6\% ineffective events and saves 67.6\% time of delta debugging in our experiments.
    },  
    keywords={},  
    doi={10.1109/TASE.2019.00012},  
    ISSN={},  
    month={July},
    thoughts = {
      This is interesting work that simplifies bug reproduction steps for bugs found using Android Monkey; so it doesn't directly apply to my research nonetheless if a bug reported using mobile analytics is also amenable to being reproduced using Android Monkey then this approach may help reduce the bug repro steps.
    }
}

@inproceedings{wang2019_understanding_the_evolution_of_mobile_app_ecosystems_a_longitudinal_measurement_of_google_play,
  title={Understanding the evolution of mobile app ecosystems: A longitudinal measurement study of {Google Play}},
  author={Wang, Haoyu and Li, Hao and Guo, Yao},
  booktitle={The World Wide Web Conference},
  pages={1988--1999},
  year={2019},
  publisher = {ACM},
  address = {San Francisco CA, USA},
  doi={https://doi.org/10.1145/3308558.3313611},
  abstract = {The continuing expansion of mobile app ecosystems has attracted lots of efforts from the research community. However, although a large number of research studies have focused on analyzing the corpus of mobile apps and app markets, little is known at a comprehensive level on the evolution of mobile app ecosystems. Because the mobile app ecosystem is continuously evolving over time, understanding the dynamics of app ecosystems could provide unique insights that cannot be achieved through studying a single static snapshot. In this paper, we seek to shed light on the dynamics of mobile app ecosystems. Based on 5.3 million app records (with both app metadata and apks) collected from three snapshots of Google Play over more than three years, we conduct the first study on the evolution of app ecosystems from different aspects. Our results suggest that although the overall ecosystem shows promising progress in regard of app popularity, user ratings, permission usage and privacy policy declaration, there still exists a considerable number of unsolved issues including malicious apps, update issues, third-party tracking threats, improper app promotion behaviors, and spamming/malicious developers. Our study shows that understanding the evolution of mobile app ecosystems can help developers make better decision on developing and releasing apps, provide insights for app markets to identifying misbehaviors, and help mobile users to choose desired apps.},
}

@inproceedings{not_yet_cited_9463119,  
  author={Wendland, Tyler and Sun, Jingyang and Mahmud, Junayed and Mansur, S. M. Hasan and Huang, Steven and Moran, Kevin and Rubin, Julia and Fazzini, Mattia}, 
  booktitle={2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)},   
  title={Andror2: A Dataset of Manually-Reproduced Bug Reports for Android apps},   
  year={2021},  
  volume={},  
  number={},  
  pages={600-604},  
  abstract={
    Software maintenance constitutes a large portion of the software development lifecycle. To carry out maintenance tasks, developers often need to understand and reproduce bug reports. As such, there has been increasing research activity coalescing around the notion of automating various activities related to bug reporting. A sizable portion of this research interest has focused on the domain of mobile apps. However, as research around mobile app bug reporting progresses, there is a clear need for a manually vetted and reproducible set of real-world bug reports that can serve as a benchmark for future work. This paper presents AndroR2: a dataset of 90 manually reproduced bug reports for Android apps listed on Google Play and hosted on GitHub, systematically collected via an in-depth analysis of 459 reports extracted from the GitHub issue tracker. For each reproduced report, AndroR2 includes the original bug report, an apk file for the buggy version of the app, an executable reproduction script, and metadata regarding the quality of the reproduction steps associated with the original report. We believe that the AndroR2 dataset can be used to facilitate research in automatically analyzing, understanding, reproducing, localizing, and fixing bugs for mobile applications as well as other software maintenance activities more broadly.
  },  
  keywords={},  
  doi={10.1109/MSR52588.2021.00082}, 
  ISSN={2574-3864},  
  month={May},
  comments = {
    These bugs would be very interesting if we had access to Google Play Console reports for when these bugs were detected. The bugs in this paper are filtered to be those that include steps to reproduce them in the bug report for apps that are active in Google Play. 
  }
}

@inproceedings{xie2015_appwatcher_unveiling_the_underground_market_of_trading_mobile_app_reviews,
    author = {Xie, Zhen and Zhu, Sencun},
    title = {AppWatcher: Unveiling the Underground Market of Trading Mobile App Reviews},
    year = {2015},
    isbn = {9781450336239},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2766498.2766510},
    doi = {10.1145/2766498.2766510},
    abstract = {Driven by huge monetary reward, some mobile application (app) developers turn to the underground market to buy positive reviews instead of doing legal advertisements. These promotion reviews are either directly posted in app stores like iTunes and Google Play, or published on some popular websites that have many app users. Until now, a clear understanding of this app promotion underground market is still lacking. In this work, we focus on unveiling this underground market and statistically analyzing the promotion incentives, characteristics of promoted apps and suspicious reviewers. To collect promoted apps, we built an automatic data collection system, AppWatcher, which monitored 52 paid review service providers for four months and crawled all the app metadata from their corresponding app stores. Finally, AppWatcher exposes 645 apps promoted in app stores and 29, 680 apps promoted in some popular websites. The current underground market is then reported from various perspectives (e.g., service price, app volume). We identified some interesting features of both promoted apps and suspicious reviewers, which are significantly different from those of randomly chosen apps. Finally, we built a simple tracer to narrow down the suspect list of promoted apps in the underground market.},
    booktitle = {Proceedings of the 8th ACM Conference on Security \& Privacy in Wireless and Mobile Networks},
    articleno = {10},
    numpages = {11},
    keywords = {app stores, fake reviews, mobile app reviews, opinion mining, underground market},
    location = {New York, New York},
    series = {WiSec '15},
}



@inproceedings{Yang_Prasad_Xie_2013_grey_box_automated_gui_model_generation_for_mobile_apps, 
   address={Rome, Italy},
   title={A Grey-Box Approach for Automated GUI-Model Generation of Mobile Applications},
   ISBN={978-3-642-37057-1},
   abstract = {
     As the mobile platform continues to pervade all aspects of human activity, and mobile applications, or mobile apps for short, on this platform tend to be faulty just like other types of software, there is a growing need for automated testing techniques for mobile apps. Model-based testing is a popular and important testing approach that operates on a model of an app’s behavior. However, such a model is often not available or of insufficient quality. To address this issue, we present a novel grey-box approach for automatically extracting a model of a given mobile app. In our approach, static analysis extracts the set of events supported by the Graphical User Interface (GUI) of the app. Then dynamic crawling reverse-engineers a model of the app, by systematically exercising these events on the running app. We also present a tool implementing this approach for the Android platform. Our empirical evaluation of this tool on several Android apps demonstrates that it can efficiently extract compact yet reasonably comprehensive models of high quality for such apps.},
  booktitle={Fundamental Approaches to Software Engineering},
  publisher={Springer Berlin Heidelberg},
  author={Yang, Wei and Prasad, Mukul R. and Xie, Tao},
  editor={Cortellessa, Vittorio and Varró, Dániel},
  year={2013},
  pages={250–265}
}


@inproceedings{yang2013testing,
  title={Testing for poor responsiveness in Android applications},
  author={Yang, Shengqian and Yan, Dacong and Rountev, Atanas},
  abstract={An important category of defects in Android applications are related to poor responsiveness. When the user interface thread performs expensive operations, the application is sluggish and may fail with an “Application Not Responding” error. Poor responsiveness has serious negative consequences for user perception and marketplace success. We propose a systematic technique to uncover and quantify common causes of poor responsiveness in Android software. When test cases are executed against the application GUI, artificial long delays are inserted at typical problematic operations (e.g., at calls that access the network). This test amplification approach may exhibit increased response times for GUI events, which demonstrates the effects of expensive operations on poor responsiveness observed by the user. The proposed approach successfully uncovered 61 responsiveness problems in eight open-source Android applications, due to inappropriate usage of resources such as network, flash storage, on-device database, and bitmaps.},
  booktitle={2013 1st International Workshop on the Engineering of Mobile-Enabled Systems (MOBS)},
  pages={1--6},
  year={2013},
  organization={IEEE}
}

@inproceedings{yasumatsu2019_software_library_update_practices_mobile_app_devs,
    author = {Yasumatsu, Tatsuhiko and Watanabe, Takuya and Kanei, Fumihiro and Shioji, Eitaro and Akiyama, Mitsuaki and Mori, Tatsuya},
    title = {Understanding the Responsiveness of Mobile App Developers to Software Library Updates},
    year = {2019},
    isbn = {9781450360999},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/3292006.3300020},
    doi = {10.1145/3292006.3300020},
    booktitle = {Proceedings of the Ninth ACM Conference on Data and Application Security and Privacy},
    pages = {13–24},
    numpages = {12},
    keywords = {mobile app developers, android security, software library, mobile apps measurement},
    address = {Richardson, Texas, USA},
    series = {CODASPY ’19}
}

@inproceedings{_duplicated_in_logging_paper_zhou2020_mobilogleak,
  author={R. {Zhou} and M. {Hamdaqa} and H. {Cai} and A. {Hamou-Lhadj}},
  booktitle={2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER)},
  title={MobiLogLeak: A Preliminary Study on Data Leakage Caused by Poor Logging Practices},   
  year={2020},
  volume={},
  number={},
  pages={577-581},
  abstract={Logging is an essential software practice that is used by developers to debug, diagnose and audit software systems. Despite the advantages of logging, poor logging practices can potentially leak sensitive data. The problem of data leakage is more severe in applications that run on mobile devices, since these devices carry sensitive identification information ranging from physical device identifiers (e.g., IMEI MAC address) to communications network identifiers (e.g., SIM, IP, Bluetooth ID), and application-specific identifiers related to the location and the users' accounts. This preliminary study explores the impact of logging practices on data leakage of such sensitive information. Particularly, we want to investigate whether log-related statements inserted into an application code could lead to data leakage. While studying logging practices in mobile applications is an active research area, to our knowledge, this is the first study that explores the interplay between logging and security in the context of mobile applications for Android. We propose an approach called MobiLogLeak, an approach that identifies log statements in deployed apps that leak sensitive data. MobiLogLeak relies on taint flow analysis. Among 5,000 Android apps that we studied, we found that 200 apps leak sensitive data through logging.},
  keywords={mobile computing;program diagnostics;security of data;MobiLogLeak;data leakage;software practice;software systems;sensitive data;mobile devices;sensitive identification information;physical device identifiers;communications network identifiers;application-specific identifiers;log-related statements;application code;mobile applications;log statements;logging practices;security;taint flow analysis;Android apps;Taint Flow Analysis;Mobile Applications;Data Leakage;Logging Practices},  
  doi={10.1109/SANER48275.2020.9054831},
  ISSN={1534-5351},
  month={Feb},
  publisher = {IEEE},
  address = {London, ON, Canada},
}

@InProceedings{zhou2017_user_perceived_control_trust_etc_smartphone,
    author="Zhou, Yun
    and Raake, Alexander
    and Xu, Tao
    and Zhang, Xuyun",
    editor="Wen, Sheng
    and Wu, Wei
    and Castiglione, Aniello",
    title="Users' Perceived Control, Trust and Expectation on Privacy Settings of Smartphone",
    booktitle="Cyberspace Safety and Security",
    year="2017",
    publisher="Springer International Publishing",
    address="Cham",
    pages="427--441",
    abstract="A common issue is that a large number of authorized apps use important and sensitive personal information without arousing users' full awareness. Existing schemes for privacy protection on smartphones try to provide users with privacy settings to control privacy leakage. Privacy settings on smartphone are intended to inform users about risks of privacy leakage and let users take over control of smartphone. Therefore, it is essential to understand and measure how much users perceive and trust these settings. To this end, we design and conduct a fine-grained online survey with 222 respondents. We collect the demographics as well as users' smartphone usage, covering not only participants' basic background information like age, gender, job, but also time of smartphone use per day, respective importance and sensitivity level of personal data, and their smartphone OSs. In this paper, we investigate users' current privacy perception and protection on smartphone in different groups, discussing participants' responses to (1) Rating the importance and sensitivity of personal information; (2) Trust on existing privacy protection; (3) Perceived control on smartphone; (4) Frequency of searching privacy knowledge; (5) Concerns about manufacturer and third-party company's behaviors on personal data and decision.",
    isbn="978-3-319-69471-9"
}

@inproceedings{zhu2015_learning_to_log,
	title = {Learning to Log: Helping Developers Make Informed Logging Decisions},
	isbn = {978-1-4799-1934-5},
	url = {http://ieeexplore.ieee.org/document/7194593/},
	doi = {10.1109/ICSE.2015.60},
	shorttitle = {Learning to Log},
	booktitle = {2015 IEEE/ACM 37th IEEE International Conference on Software Engineering},
	address = {Florence, Italy},
	abstract = {Logging is a common programming practice of practical importance to collect system runtime information for postmortem analysis. Strategic logging placement is desired to cover necessary runtime information without incurring unintended consequences (e.g., Performance overhead, trivial logs). However, in current practice, there is a lack of rigorous specifications for developers to govern their logging behaviours. Logging has become an important yet tough decision which mostly depends on the domain knowledge of developers. To reduce the effort on making logging decisions, in this paper, we propose a "learning to log" framework, which aims to provide informative guidance on logging during development. As a proof of concept, we provide the design and implementation of a logging suggestion tool, Log Advisor, which automatically learns the common logging practices on where to log from existing logging instances and further leverages them for actionable suggestions to developers. Specifically, we identify the important factors for determining where to log and extract them as structural features, textual features, and syntactic features. Then, by applying machine learning techniques (e.g., Feature selection and classifier learning) and noise handling techniques, we achieve high accuracy of logging suggestions. We evaluate Log Advisor on two industrial software systems from Microsoft and two open-source software systems from Git Hub (totally 19.1M {LOC} and 100.6K logging statements). The encouraging experimental results, as well as a user study, demonstrate the feasibility and effectiveness of our logging suggestion tool. We believe our work can serve as an important first step towards the goal of "learning to log".},
	pages = {415--425},
	publisher = {{IEEE}},
	author = {Zhu, Jieming and He, Pinjia and Fu, Qiang and Zhang, Hongyu and Lyu, Michael R. and Zhang, Dongmei},
	urldate = {2017-02-07},
	date = {2015-05},
	year = {2015},
	keywords = {{ICST}2018, mobicom2017}
}

@mastersthesis{not_yet_cited_osvald2017_a_crash_reporting_library_for_android,
  title={A Crash Reporting Library for Android},
  author={Osvald, Marek},
  pages = {91},
  year = {2017},
  abstract = {
    The goal of this master's thesis is to develop a universal and adaptable solution for crash reporting of Android applications. The thesis describes the entire development process of such solution, from its architecture concepts to the process of deployment including a test plan and means of its automation.
  },
  institution = {Masaryk University},
  location = {Brno},
  url = {https://is.muni.cz/th/z23ho/diploma-thesis_Archive.pdf},
  summary = {
    This thesis provides a detailed overview of how crash reporting was designed and implemented as part of a commercial product at AVG. Sadly the source code doesn't seem to be available. Of course some of the findings are dated as the other commercial offerings the author investigated have changed significantly in the intervening years, nonetheless these changes don't obviate the value of the thesis.
  }
}

@mastersthesis{not_yet_cited_chren2014_methods_for_software_failure_data_collection_and_prediction,
  title = {Methods For Software Failure-Data Collection And Prediction},
  url = {https://is.muni.cz/th/j5pe4/CHREN-diploma-thesis-final_Archive.pdf},
  author = {Stanislav Chren},
  year = {2014},
  institution = {Masaryk University},
  location = {Brno},
  pages = {79},
  abstract = {
    This diploma thesis addresses one of the important challenges in software reliability analysis. In theoretical part, the thesis surveys methods for collection of failure data and estimation of failure parameters that are utilised by architecture-based reliability models. It investigates possible data sources and analyses the advantages and limitations of the individual methods. In practical part, the thesis evaluates the applicability of the discovered methods on modern open source software.
  },
  thoughts = {
    His thesis appeared when I searched for PFOD. Amongst other methods, it discusses the study of application logs to measure failure. It might be a useful reference for my 'related works' chapter.
    
    I believe this was his masters thesis based on his LinkedIn profile https://www.linkedin.com/in/stanislav-chren-190092112/ 
  }
  
}

@comment{Info on how to cite LinkedIn from https://apastyle.apa.org/style-grammar-guidelines/references/examples/linkedin-references}
@misc{alexander2021_linkedin_profile,
  url = {https://www.linkedin.com/in/ian-alexander-01353340/},
  title = {Ian Alexander [LinkedIn Profile]},
  author = {Ian Alexander},
  year = {2021},
  note = {Last visited: \nth{21} Dec 2021},
  extract = {
    Co-founder
    Company Name Boundless Labs UK
    Dates Employed Jan 2014 – Present
    Employment Duration 8 yrs
    Location London, United Kingdom
    Built MoodSpace, a digital platform empowering everyone to take control of their mental health. MoodSpace began as a side project which later took on a round of funding and with a team of 6 supported a user base of 300k. Although no longer pursued as a business we continue to maintain the project and release regular updates to 10s of thousands of active users.
    The most recent iteration of the app was built with Kotlin, clean architecture, MVVM, Data binding, Gitlab CI, Coroutines/Flow, and ObjectBox and was architected to enable the use of Kotlin Multiplatform to share ~60\% of the codebase between platforms.
  }
}

@misc{amland2002_slides,
  title = {ET Workshop v. 1.20 - Test Management},
  year = {2002},
  author = {Ståle Amland},
  organization = {Amland Consulting},
  url = {http://www.testingeducation.org/course_notes/amland_stale/cm_200212_exploratorytesting/exploratorytesting_5_management.pdf},
  part_of = {http://www.testingeducation.org/course_notes/amland_stale/cm_200212_exploratorytesting/},
}

@misc{android_android_vitals_guide,
  title = {Use Android vitals to improve your app's performance, stability, and size},
  url = {https://developer.android.com/distribute/best-practices/develop/android-vitals},
  organization = {{Android Developers}},
  author = {{Android Developers}},
  year = {2020},
  note = {Last retrieved 13 July 2021},  
}

@misc{android_app_bundle,
  title = {Android App Bundle - Android Developers},
  url = {https://developer.android.com/platform/technology/app-bundle},
  organization = {{Android Developers}},
  author = {{Android Developers}},
  year = {2021},
  note = {Last retrieved 13 July 2021},  
}

@misc{android_guidelines_core_app_quality,
  url = {https://developer.android.com/docs/quality-guidelines/core-app-quality},
  title = {Core app quality - Android Developers},
    organization = {{Android Developers}},
  author = {{Android Developers}},
  year = {2021},
  note = {Last retrieved 14 July 2021}, 
  abstract = {
    Core app quality - Last updated: May 17, 2021
    This checklist defines a set of core quality criteria and associated tests to help you assess the quality of your app. Some of these criteria might be easy to miss, and the tests help you remember to include them in your test plans.

    The checklist highlights the minimum quality that all apps should meet. Your testing will likely go well beyond what's described here.

    Each item in the quality checklist has a unique ID which you might find helpful to use when you communicate with your team. You can also view the previous version of these guidelines [https://developer.android.com/docs/quality-guidelines/2021/02].
  },
  extract = {
    Stability	PS-S1	CR-all SD-1	The app does not crash or block the UI thread causing ANR (Android Not Responding”) errors. Utilize Google Play’s pre-launch report to identify potential stability issues. After deployment, pay attention to the Android Vitals page in the Google Play developer console.},
}

@misc{android_dropboxmanager,
  title = {DropBoxManager - Android Developers},
  url = {https://developer.android.com/reference/android/os/DropBoxManager},
  year = {2021},
  organization = {{Android Developers}},
  author = {{Android Developers}},
  abstract = {
    Enqueues chunks of data (from various sources -- application crashes, kernel log records, etc.). The queue is size bounded and will drop old data if the enqueued data exceeds the maximum size. You can think of this as a persistent, system-wide, blob-oriented "logcat".

    DropBoxManager entries are not sent anywhere directly, but other system services and debugging tools may scan and upload entries for processing.
  },
  note = {Last retrieved 31 July 2020},  
}

@misc{android_in_app_updates,
  title = {In-app updates | Android Developers},
  url = {https://developer.android.com/guide/playcore/in-app-updates},
  year = {2021},
  organization = {{Android Developers}},
  author = {{Android Developers}},  
}

@misc{android_processes_and_application_lifecycle,
  url = {https://developer.android.com/guide/components/activities/process-lifecycle},
  title = {Processes and Application Lifecycle},
  organization = {{Android Developers}},
  author = {{Android Developers}},
  year = {2020},
  note = {Last retrieved 31 July 2020},
}

@misc{android_store_listing_guide,
  title = {Improve your app’s quality and discoverability},
  url = {https://developer.android.com/distribute/best-practices/launch/store-listing},
  year = {2020},
  organization = {{Android Developers}},
  author = {{Android Developers}},
  note = {Last retrieved 13 July 2021},   
  extracts = {
    The Google Play Store is committed to connecting users with a diverse catalog of high quality apps. Our recommendations are composed of a mix of human curation and algorithmic calculations, of which, the two largest components considered are relevance and quality. The best practices below explain how we evaluate quality for your app, independent of which user might download it.
    
    Review the Android vitals dashboard to see how your app is performing on core vitals metrics including crash rate, ANR rate, excessive wakeups, and stuck partial wake locks in the background. Look at peer benchmarks to see how you measure up to others in your category.
  }
}

@misc{android_vitals_best_practices_key_metrics,
  url = {https://developer.android.com/distribute/best-practices/develop/android-vitals#key-metrics},
  title = {Google Play | Android Developers},
  organization = {{Android Developers}},
  author = {{Android Developers}},
  year = {2020},
  extracts = {
    Key metrics
      Stability | ANR rate: The percentage of users who experienced at least one application not responding (ANR) event during a daily session. ANRs are typically caused by deadlocks or slowness in UI thread and background processes (broadcast receivers).
      Stability | Crash rate: The percentage of users who experienced at least one crash event during a daily session. Crashes are often caused by unhandled exceptions, resource exhaustion, failed assertions, or other unexpected states.
  }
}

@online{appbrain,
  title = {AppBrain - Everything you need for a successful Android app},
  url = {https://www.appbrain.com/},
  year = {2021},
  author = {{AppBrain}},
  organization = {{AppBrain}},
}

@misc{appbrain_appsee,
  title = {{Appsee - Android SDK Statistics | AppBrain}},
  url = {https://www.appbrain.com/stats/libraries/details/appsee/appsee},
  organization = {AppBrain},
  author = {Appbrain},
  year = {2020},
  note = {Last retrieved 31 July 2020},
  abstract = {
    Appsee enables mobile app publishers and developers to track, understand and improve the user experience in their apps. It provides features such as User Recordings, Touch Heatmaps, Realtime App Analytics, and Conversion Funnels.

    Number of apps	Over 790
    Total number of downloads	Over 375 Million
    Tags	Analytics
    Website	https://www.appsee.com/
  }
}

@misc{appbrain_moonpig,
  title = {Moonpig.com - Android developer info on AppBrain},
  url = {https://www.appbrain.com/dev/Moonpig.com/},
  organization = {AppBrain},
  author = {Appbrain},
  year = {2021},
  note = {Last retrieved 25 Sep 2020},  
  abstract = {
    Moonpig.com is an Android developer that has been active since 2012 and has one app (Moonpig: Birthday Card Maker & Gift Shopping App) in Google Play. Moonpig: Birthday Card Maker & Gift Shopping App is listed in the category "Shopping". It is highly ranked in several countries, and is also one of the more popular apps in the Android ecosystem with more than 1 million installs.
  }
}

@misc{appbrain2022_ok_http_stats,
  title = {okHttp - Android SDK statistics | AppBrain },
  url = {https://www.appbrain.com/stats/libraries/details/okhttp/okhttp},
  year = {2022},
  organization = {AppBrain},
  author = {Appbrain},
  note = {Last retrieved 02 Feb 2022},
  abstract = {
  Number of apps Over 36 Thousand
  Total number of downloads	Over 48 Billion
  Statistics
  Market share overall
    5.07\% of apps	
    4.84\% of installs	
  Market share in top apps 
    6.06\% of apps	
    2.84\% of installs	
  Market share in new apps 
    4.00\% of apps	
    5.75\% of installs	
  }
}

@misc{appbrain2021_react_native_stats,
  title = {React Native - Android SDK statistics | AppBrain },
  url = {https://www.appbrain.com/stats/libraries/details/react_native/react-native},
  year = {2021},
  note = {last retrieved 28 Dec 2021},
  organization = {AppBrain},
  author = {Appbrain},
  abstract = {
    React Native lets you build mobile apps using only JavaScript, letting you compose a mobile UI from declarative components.

    Number of apps	Over 32 Thousand
    Total number of downloads	Over 37 Billion
    License	MIT License
    Tags	#3 in App Frameworks, Open Source
    Website	https://facebook.github.io/react-native/
    GitHub	Watchers: 3.7k, Stars: 0.1m, Forks: 21.6k
    Statistics
    Market share overall
    4.44\% of apps	
    3.80\% of installs	
    Market share in top apps 
    8.16\% of apps	
    7.58\% of installs	
    Market share in new apps 
    3.82\% of apps	
    0.03\% of installs
  },
}

@misc{androidauthority2021_huawei_app_gallery,
  title = {Huawei’s Play Store alternative has gotten better, but it’s the apps that count},
  url = {https://www.androidauthority.com/huawei-app-gallery-review-1101306/},
  author = {Robert Triggs},
  year = {2021},
  organization = {{Android Authority}},
}

@misc{androidauthority2021_the_huawei_ban,
  title = {The Huawei ban explained: A complete timeline and everything you need to know},
  url = {https://www.androidauthority.com/huawei-google-android-ban-988382/},
  author = {C. Scott Brown},
  note = {Most recent update at the time of writing was the \nth{12} May 2021.},
  year = {2021},
  organization = {{Android Authority}},
}

@misc{apkcombo_website_about_us,
  title = {Aboust us - APKCombo.com},
  url = {https://apkcombo.com/about},
  year = {2021},
  organization = {APKCombo},
  author = {APKCombo},
  abstract = {
    APKCombo is the largest APK store with 8 million Android games and apps. APKCombo was founded by Harry Phi and Johnny Nguyen in May 2018. We love Android and Technology!

    Every month, over 3 million people trust us to help them find and download file from our client. You can trust our technology to help you download any APK / OBB files for your needs (without country/regional restrictions). APKCombo pulls APK and OBB file directly from Google Play Store. There are no modded APKs on APKCombo. You can check APKs are safe and virus-free before installation with VirusTotal Analyzer (MD5/SHA1/SHA256, Developer Certificate, Android Permissions, Android Activities, Android Services).

    We believe in simplicity. We're excited to simplify idea for everyone through our technology solutions and community. Our slogan is "Simplicity is the key to brilliance".
  },
  note = {Last visited: \nth{01} July 2021},
}

@misc{appsee2015_youtube_visual_analytics_budapest_mobile_meetup,
  title = {Appsee - Visual Analytics presented by Zahi Boussiba at Budapest.Mobile Meetup},
  url = {https://youtu.be/aRN_XrxNCNE},
  year = {2015},
  author = {AppCraft Community},
  abstract = {
    The CEO of Appsee, Zahi Boussiba spoke about Visual Analytics at Budapest.Mobile meetup. 
  }
}

@misc{apteligent2016_data_report_network_crash_edition,
  url = {https://web.archive.org/web/20171119170931/https://data.apteligent.com/research/network-crashes},
  title = {Data Report: Network Crash Edition},
  year = {2016},
  author = {{Apteligent}},
  organization = {{Apteligent}},
  publisher = {Internet Archive: Wayback Machine},
  abstract = {
  In this report we analyze mobile app crashes during interactions with cloud services. We compare the failure rates on iOS and Android, dive into which App Store categories are the most affected by networking issues, and most importantly analyze why these issues occur at such an alarming rate in the first place!

  Network Crash /ˈnɛtwərk kræʃ /
  A crash in a mobile app caused by a network call. For example, an app communicating with a cloud service may return bad data, result in an error, take too long for the request to complete, or simply fail to respond at all.
  }
}

@misc{bcs_code_of_conduct_2021,
  title = {BCS Code of Conduct | BCS},
  url = {https://www.bcs.org/membership-and-registrations/become-a-member/bcs-code-of-conduct/},
  year = {2021},
  organization = {{BCS}},
  author = {{BCS}},
  abstract = {
    The BCS Code of Conduct serves as a unique and powerful endorsement of your integrity and as a code of ethics for IT professionals.
  }
}

@misc{bitbar2019_smartbear_acquired_bitbar,
  title = {SmartBear Acquires Bitbar to Enhance Mobile Testing Automation Offering},
  url = {https://bitbar.com/blog/smartbear-acquires-bitbar/},
  author = {Delfin Vassallo},
  year = {2019},
  organization = {{Bitbar Technologies}},
  publisher = {{SmartBear Inc.}},
  note = {Accessed \nth{16} July 2021},
}

@misc{bolt2019_how_to_programmatically_capture_screen_on_android,
  title = {How to programmatically capture screen on Android: a comprehensive guide},
  url = {https://medium.com/bolt-labs/how-to-programmatically-capture-screen-on-android-a-comprehensive-guide-f500c95e455a},
  author = {Yaroslav Shevchuk},
  year = {2019},
  publisher = {{Medium Inc.}},
  abstract = {
    At Bolt, we encourage employees to use the products we’re building, to provide feedback that helps us continuously improve. To make this process as simple and efficient as possible, we’ve added a special button to the version of the Android app used by employees internally. Upon tap, it gathers device logs, captures screen image, prompts for a description and sends everything directly to special channel with support representatives. Building a robust screen capturing mechanisms has its pitfalls, and we want to share results and the knowledge we gained with the community.
  }
}

@misc{bolton2009_testing_and_noticing,
  title={Testing and Noticing},
  author={Bolton, Michael and Bach, James},
  year={2009},
  url = {https://www.developsense.com/presentations/2009-10-Noticing.pdf},
  publisher = {developsense.com},
}

@misc{books_in_print_donald_knuth,
  title = {Books in Print by Donald E. Knuth},
  url = {https://cs.stanford.edu/~knuth/books.html},
  year = {1999},
  author = {Donald E. Knuth},
  quote = {Click web links for current news about each book of interest. Lists of errors and amendments can be downloaded as plain TeX files or read from DVI files or PostScript files cited on the relevant web pages. You are entitled to a reward of at least 0x$1.00 ($2.56) if you are the first person to report a bona-fide error not on those lists. Each page tells you how to report an error for the book in question.},
}

@misc{catroid_426_soft_crashes_should_not_be_reported_to_the_play_console,
  label = {CATROID-426},
  title = {Soft crashes should not be reported to the play console},
  url = {https://jira.catrob.at/browse/CATROID-426},
  author = {Thomas Schwengler,Thomas Schranz and Wolfgang Slany},
  year = {2019},
  organization = {{Catrobat Project}},
  abstract = {
    Set number of background processes to 0, tap into an activity and switch between Pocket Code and another application. This leads to crashes that we recover from, but are still reported to the play console. We should try to catch those for now and fix them properly later.
  }
}

@misc{not_yet_cited_bumble_chameyev2020_how_we_achieved_a_6x_reduction_of_anrs_part1_collecting_data,
    title = {How we achieved a 6x reduction of ANRs - Part 1: Collecting Data},
    author = {Nickolay Chameyev},
    publisher = {{Medium Inc.}},
    url = {https://medium.com/bumble-tech/how-we-achieved-a-6x-reduction-of-anrs-part-1-collecting-data-7c473ceb1c83},
    year = {2020},
    abstract = {
        One of the worst things that can happen to your app’s responsiveness is an Application Not Responding (ANR) dialogue. A high ANR rate may affect user experience and, potentially, Google Play search positions and featuring.
        At the beginning of the year, we had ANR rate above the Google Play threshold for the Badoo application — operated by its parent company Bumble. So, we set up an ad-hoc team to work on this problem and spent a few months trying different approaches to fix it. By the end of the period, we managed to reduce the number of ANRs by more than six times.
    },
}

@misc{not_yet_cited_bumble_chameyev2020_how_we_achieved_a_6x_reduction_of_anrs_part2_fixing_anrs,
    title = {How we achieved a 6x reduction of ANRs - Part 2: Fixing ANRs},
    author = {Nickolay Chameyev},
    publisher = {{Medium Inc.}},
    url = {https://medium.com/bumble-tech/how-we-achieved-a-6x-reduction-of-anrs-part-2-fixing-anrs-24fedf9a973f},
    year = {2020},
    abstract = {
        In the first part, we discussed what ANR is and what are the ways of tracking it. In this article, you will find information on what problems we found in our application, how we fixed them, and the results we achieved.
    }    
}

@misc{not_yet_cited_bumble_tupikov2020_catching_bugs_on_the_client_side_how_we_developed_our_error_tracking_system,
    title = {Catching bugs on the client-side: how we developed our error tracking system},
    url = {https://medium.com/bumble-tech/catching-bugs-on-a-client-48c793720b9b},
    author = {Eugene Tupikov},
    year = {2020},
    publisher = {{Medium Inc.}},
    abstract = {
        Our team develops several products, Badoo and Bumble, two of the world’s largest dating and connection applications. For both, we have a web version (desktop and mobile) and mobile applications (Android and iOS). With more than millions of users, it’s important for us to gather client-side errors, and for this we use a system of our own code-named Gelato. For the last two years, I have been involved in developing its server-side and throughout this time I have discovered a lot of new things about the world of error tracking systems development that I would like to share with you in this article.
        What we will cover:
        - how we use error information
        - The tools we used previously, and why we developed our own system over a ready-made solution
a brief overview of our system, its architecture, and technology stack.
    },
}


@misc{countly_which_operating_systems_are_supported,
  title = {Countly Help Center - Which operating systems are supported?},
  url = {https://support.count.ly/hc/en-us/articles/360037754031-Android\#which-operating-systems-are-supported},
  year = {2021},
  organization = {Countly},
  author = {Countly},
  note = {Their Android SDK help page was last modified at: June 02, 2021 13:14}
}

@misc{crashscope_project_homepage,
  title = {CrashScope: A Practical Automated Android Testing Tool},
  url = {https://www.android-dev-tools.com/crashscope-home},
  author = {{SEMERU Research Group}},
  organization = {College of William \& Mary --- SEMERU},
  year = {2018},
  note = {Last visited: \nth{01} July 2021},
}

@misc{fdroidwebsite,
  title = {F-Droid - Free and Open Source Android App Repository},
  url = {https://www.f-droid.org/},
  year = {2021},
  organization = {{F-Droid Limited and Contributors}},
  author = {{F-Droid Limited and Contributors}},
  abstract = {
    F-Droid is an installable catalogue of FOSS (Free and Open Source Software) applications for the Android platform. The client makes it easy to browse, install, and keep track of updates on your device.
  },
}

@misc{fileinfo_obb_format,
  title = {.OBB File Extension},
  url = {https://fileinfo.com/extension/obb},
  year = {2021},
  organization = {Sharpened Productions},
  author = {{The FileInfo.com team}},
  note = {Last visited: \nth{01} July 2021},
}

@misc{firebaseblog2016_how_does_firebase_initialize_on_android,
  title = {The Firebase Blog: How does Firebase initialize on Android?},
  url = {https://firebase.googleblog.com/2016/12/how-does-firebase-initialize-on-android.html},
  author = {Doug Stevenson},
  year = {2016},
  organization = {Google},
}

@misc{firebaseblog2017_take_control_of_your_firebase_init_on_android,
  title = {The Firebase Blog: Take Control of Your Firebase Init on Android},
  url = {https://firebase.googleblog.com/2017/03/take-control-of-your-firebase-init-on.html},
  author = {Doug Stevenson},
  year = {2017},
  organization = {Google},
}

@misc{firebase_help_GA4_2021_predefined_user_dimensions,
  title = {[GA4] Predefined user dimensions},
  url = {https://support.google.com/firebase/answer/9268042},
  author = {{Google Firebase}},
  organization = {{Google Inc.}},
  year = {2021},
  extracts = {
    As long as you use the SDK or gtag.js, you don't need to write additional code to collect the following user dimensions from your mobile app and/or website. All of these user dimensions are available for use in Audience conditions, and some of them are also available as general report filters.

    For iOS apps, you must collect IDFA to automatically derive the Age, Gender, and Interests dimensions.
    
    Google signals: We recommend that you activate Google signals https://support-content-draft.corp.google.com/analytics/answer/9445345 to get more holistic demographic and interest data. Google signals is data from users who sign in to Google. When Google signals data is available, Analytics associates event data it collects from users with the Google accounts of users who are signed in and consented (by enabling Ads Personalization) to share this information.
    
    Data thresholds: You must meet the Analytics data thresholds https://support-content-draft.corp.google.com/analytics/answer/9383630 to see demographic and interest data. Google may withhold data to prevent anyone from inferring the identity of individuals based on their demographics and interests. For example, if there are fewer than N instances of Gender=male in a report, then data for the male value may be withheld.
  }
  
}

@misc{firebase_homepage_2021,
  title = {Firebase},
  url = {https://firebase.google.com/},
  author = {{Google Firebase}},
  organization = {{Google Inc.}},
  year = {2021},
  extracts = {
    From the HTML source: 
      <meta property="og:description" content="Firebase is Google's mobile platform that helps you quickly develop high-quality apps and grow your business." />
  }
}

@misc{firebase_help_set_user_properties,
  title = {Set User Properties | Firebase},
  url = {https://firebase.google.com/docs/analytics/user-properties?platform=android},
  year = {2021},
  author = {{Google Firebase}},
  organization = {{Google Inc.}},
  extracts = {
    User properties are attributes you define to describe segments of your user base, such as language preference or geographic location. These can be used to define audiences for your app. This guide shows you how to set user properties in your app. 
    
    Analytics automatically logs some user properties; you don't need to add any code to enable them. If you need to collect additional data, you can set up to 25 different Analytics User Properties per project. Note that user property names are case-sensitive and that setting two user properties whose names differ only in case results in two distinct user properties being logged.
    
    You can't use a small set of user property names reserved by Google: Age, Gender, Interest
    Before you begin: Make sure that you've set up your project and can access Analytics as described in Get Started with Analytics.
  }
}

@misc{gartner_what_is_mobile_app_analytics_software,
  title = {What is Mobile App Analytics software?},
  url = {https://www.gartner.com/reviews/market/mobile-app-analytics/vendors},
  organization = {{Gartner}},
  author = {{Gartner}},
  year = {[2021?]},
  abstract = {
    Mobile app analytics tools collect and report on in-app data pertaining to the operation of the mobile app and the behavior of users within the app. These areas of app analytics are defined as follows: Operational analytics: Provides visibility into the availability and performance of mobile apps in relation to device, network, server and other technology factors. Operational analytics are essential to capture and fix unexpected app behavior (such as crashes, bugs, errors and latency) that can lead to user frustration and abandonment of the app. Such analytics should be applied at both the app testing phase and after release of the app into production. Behavioral analytics: Shows how app users interact with the app to gain actionable insights, drive app improvements and improve business outcomes. Behavioral data can be analyzed based on correlating clicks, swipes, views and other usage stats based on user profiles, segmentation/cohorts, retention, funnel/event tracking and A/B testing.},
}

@misc{googblogs_I_O_2017_everything_new_in_the_google_play_console,
  title = {{I/O 2017: Everything new in the Google Play Console}},
  url = {https://www.googblogs.com/io-2017-everything-new-in-the-google-play-console/},
  author = {Vineet Buch},
  organization = {Google Inc.},
  year = {2017},
}

@misc{googleblogs2020_google_home_reduces_crashes_by_a_third,
  title = {Google Home reduces \#1 cause of crashes by 33\%},
  url = {https://developer.android.com/stories/apps/google-home},
  author = {Google Android Developers},
  year = {2020},
  organization = {Google Inc.},
  extracts = {
    What they did
    The Google Home team decided to incorporate Kotlin into their codebase to make programming more productive and to enable the usage of modern language features like var/val, smart casts, coroutines, and more. As of June 2020, about 30\% of the codebase is written in Kotlin, and Kotlin development is encouraged for all new features.

    The team also adopted Jetpack libraries to improve developer velocity, decrease the need for boilerplate code maintenance, and reduce the necessary amount of code. Jetpack libraries also helped make their code more testable, since there are clearer functional boundaries and APIs.

    Results
    "Efficacy and writing less code that does more is the ‘speed’ increase you can achieve with Kotlin.” - Jared Burrows, Software Engineer on Google Home

    Switching to Kotlin resulted in a reduction in the amount of required code, compared to the equivalent of existing  code. One example is the use of data classes and the Parcelize plugin: a class which was 126 hand-written lines in Java can now be represented in just 23 lines in Kotlin—an 80\% reduction. Additionally, equality and parcelizing methods can be automatically generated and kept up to date. Many nested loops and filtering checks were also simplified using the functional methods available in Kotlin.

    Because Kotlin can make nullability a part of the language, tricky situations can be avoided, like when inconsistent usage of nullability annotations in Java might lead to a missed bug. Since the team started migrating to developing new features with Kotlin, they saw a 33\% decrease in NullPointerExceptions. Since this is the most common crash type on Google Play Console, reducing them led to a dramatically improved user experience.

    With a large, mature app like Google Home—which has over a million lines of code—it's helpful to be able to gradually add Jetpack libraries. Incorporating them allowed the team to consolidate and replace custom tailored solutions, sometimes even with a single library. Since Jetpack libraries can help engineers follow best practices and be less verbose (for example, using Room or ConstraintLayout), readability was increased as well. The team considers many of the newer Jetpack libraries ‘must-haves,’ including ViewModel and LiveData, both of which are used extensively in the Google Home codebase.

    The Google Home app team found the Jetpack KTX integrations with Kotlin coroutines to be especially helpful. The team is now able to avoid tricky asynchronous programming bugs by associating coroutines with lifecycle-aware components like ViewModel.
  }
}

@misc{googleblogs2021_androids_kotlin_first_approach,
  title = {Android’s Kotlin-first approach},
  url = {https://developer.android.com/kotlin/first},
  organization = {Google Inc.},
  author = {Google Inc.},
  year = {2021},  
}

@misc{gtafblog2021_gtaf_accomplishment_2020,
  title = {What we have achieved in 2020 | Greentech Apps Foundation},
  url = {https://gtaf.org/blog/gtaf-accomplishment-2020},
  year = {2021},
  author = {{GTAF}},
  organization = {{GTAF}},
  quotes = {
    Improving user experience: We focused on lowering the crash rates, solving the UX issues to improve the user experience further and keep up with the modern UI trends. Our average daily crashes were reduced from around 2800 to 1060.
    
    Growing more and more: We released a whole new app for Quran Memorization with loads of features and neat user interface. At the year-end, our apps were installed on more than 1 million devices.

    User Profile, analytics and Progress Tracking: We will integrate profile features in many of our applications. So that users can sign in, save their bookmarks and notes and sync with their other devices. Also, this will enable us to implement community-based features in the future. The analytics dashboard is a very useful tool for our daily used applications. How much time we spend on each part, and how we can build a habit out of it can be improved a lot by viewing statistics and progress. That is why we are planning to integrate analytics features in our application.
  }
}

@misc{harty_stareast2005_keynote,
  title = {The Imperative of Non-Functional System Testing},
  maintitle = {StarEast 2005 Conference},
  organization = {{Commercetest Ltd.}},
  publisher = {{SQE}},
  year = {2005},
  author = {Julian Harty},
  address = {Orlando, Florida, USA.},
}

@misc{harty_wama_dataset_examples,
  title = {Dataset: Examples of JSON data generated by Vitals-Scraper script},
  url = {https://dl.acm.org/do/10.1145/3345843/full/},
  year = {2019},
  author = {Julian Harty},
}

@misc{harty2021_logging_practices_arxiv,
      title={Logging Practices with Mobile Analytics: An Empirical Study on Firebase}, 
      author={Julian Harty and Haonan Zhang and Lili Wei and Luca Pascarella and Mauricio Aniche and Weiyi Shang},
      year={2021},
      eprint={2104.02513},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      abstract = {
        Software logs are of great value in both industrial and open-source projects. Mobile analytics logging enables developers to collect logs remotely from their apps running on end user devices at the cost of recording and transmitting logs across the Internet to a centralised infrastructure.

        This paper makes a first step in characterising logging practices with a widely adopted mobile analytics logging library, namely Firebase Analytics. We provide an empirical evaluation of the use of Firebase Analytics in 57 open-source Android applications by studying the evolution of code-bases to understand: a) the needs-in-common that push practitioners to adopt logging practices on mobile devices, and b) the differences in the ways developers use local and remote logging.

        Our results indicate mobile analytics logs are less pervasive and less maintained than traditional logging code. Based on our analysis, we believe logging using mobile analytics is more user centered compared to traditional logging, where the latter is mainly used to record information for debugging purposes.
    },
}

@misc{hbr_what_ai_app_stores_mean_for_radiology,
  title = {{What AI ``App Stores" Will Mean for Radiology}},
  author = {Woojin Kim, Karen Holzberger},
  url = {https://hbr.org/2019/06/what-ai-app-stores-will-mean-for-radiology},
  year = {2019},
  publisher = {HBR},
}

@misc{hp2015_email_Are_your_mobile_apps_summer_ready,
  title = {Are your mobile apps summer-ready?},
  year = {2015},
  author = {{The HP AppPulse Mobile Team}},
  organization = {{ Hewlett-Packard Development Company}},
  contents = {
  Summer's here and there's more to it this year than long days and warm nights. Mobile app users have little tolerance for dealing with slow performing, crashing, and battery hogging apps. The new Summer 2015 Release of HP AppPulse Mobile is now available and it’s packed full of exciting new features to help you address these issues and more, including:
  
  - Crash Analytics with User Crash Trail – Get full visibility into user experience prior to crashes and isolate the root cause of issues
  - UI Performance with HTTP Timeline – Drill down and investigate the HTTP traffic on a timeline to know which service is impacting the user experience
  - UI Performance with Threads Timeline – Conduct a deep dive analysis into those slow user actions with a complete breakdown into threads and methods
  - Automatic Error Correlation – Group errors by reported, failed HTTP requests, and error messages
  - HP Mobile Center Sharing – Apps in HP AppPulse Mobile can now be used in HP Mobile Center
  For an overview of all the new capabilities HP AppPulse Mobile has to offer, check out this short video webinar which demos of all the key features
  },
}

@misc{huawei_accessing_analytics_kit,
  title = {Analytics Kit - Accessing Analytics Kit},
  url = {https://developer.huawei.com/consumer/en/doc/development/HMSCore-Guides-V5/android-accessing-0000001050161888-V5},
  author = {HUAWEI Developers},
  year = {2021},
  organization = {{Huawei}},
  set_event_reporting_policies = {
    Four policies can be combined to determine when the Analytics SDK will attempt to send events. Screenshot of details saved in my dropbox.
  }  
}

@misc{huaweidevelopers_appgallery_review_guidelines,
  title = {AppGallery Review Guidelines},
  url = {https://developer.huawei.com/consumer/en/doc/30202},
  author = {{HUAWEI Developers}},
  year = {2021},
  organization = {{Huawei}},
  extract = {
    Please be aware of the following issues that may lead to a prolonged app review process or even rejection. Before submitting your app for review, please confirm that:

    1. Your app information and metadata are complete and accurate.
    2. Your app does not crash or experience bugs while running.
    3. Your contact information is authentic and valid, so that you can be reached when necessary.
    4. You can provide a valid test account and sign-in information, as well as any hardware or resources that may be required for app review. For example, the resources or information required for special configurations or special test environments need to be clarified.
    5. Your app is available for use during the review period.
    6. Your app complies with the relevant rules detailed in the following documents:
  }
}

@misc{huawei_ag_connect_crash,
  title = {AppGallery Connect - AGConnectCrash},
  url = {https://developer.huawei.com/consumer/en/doc/development/AppGallery-connect-References/agconnectcrash-android-0000001055420438},
author = {HUAWEI Developers},
  year = {2021},
  organization = {{Huawei}},
  abstract = {
    public class AGConnectCrash
    Creates an AGConnectCrash instance and a crash to facilitate debugging.}
}

@misc{huawei_analyticskit,
  title = {Analytics Kit - APP Intelligent Analysis Service - HUAWEI Developer},
  url = {https://developer.huawei.com/consumer/en/hms/huawei-analyticskit},
  author = {HUAWEI Developers},
  year = {2021},
  organization = {{Huawei}},
  abstract = {Provides free data analysis for a wide range of devices and platforms, so you can make informed decisions on product optimization and marketing based on your users' behavior.},
}

@misc{huawei_analyticskit_dataexport_codelab,
  title = {Analytics Kit (Data Export)},
  url = {https://developer.huawei.com/consumer/en/codelabsPortal/carddetails/HMSAnalyticsKit-Data-Export},
  author = {HUAWEI Developers},
  year = {2021},
  organization = {{Huawei}},  
}

@misc{huawei_analyticskit_pre_release_check,
  title = {Analytics Kit - Pre-release Check},
  url = {https://developer.huawei.com/consumer/en/doc/development/HMSCore-Guides-V5/android-pre-release-check-0000001050420841-V5#EN-US_TOPIC_0000001055224472__sd34f1bf539604a8bbea6cbfc6f7ba4b4},
  author = {HUAWEI Developers},
  year = {2021},
  organization = {{Huawei}}, 
  notice = {
    Since HUAWEI Analytics Kit 4.0.3.300, the HMS Core Analytics SDK for Android has been significantly improved in terms of stability, security, and reliability. If the SDK you integrated is earlier than 4.0.3.300, please upgrade it to 4.0.3.300 or later before April 30, 2021. From May 1, 2021, Analytics Kit will not receive data reported by the Analytics SDK earlier than 4.0.3.300 for Android. If you have integrated Remote Configuration, App Linking, or Crash, you also need to upgrade their SDKs for Android before April 30, 2021. Otherwise, functions that depend on Analytics Kit will become unavailable.
  }
}

@misc{huawei_appconnect_terms_of_service,
  title = {Statement About AppGallery and Privacy},
  url = {https://consumer.huawei.com/minisite/cloudservice/hiapp/privacy-statement.htm?code=SG&branchid=2&language=en_US#},
  year = {2021},
  data_collection_includes = {
  The following categories of information will be collected and used in order for you to use the service:
    •Account information, such as name, email address, phone number, date of birth, and third-party account login data.
    •Network information, such as IP address.
    •Service usage information, such as operation logs.
    •Browser information, such as cookies.
  The data controller is Aspiegel SE, a subsidiary of Huawei in Ireland. Your data will also be used for analytical and service improvement purposes. For more information about how we process your data and about your rights, including the right to object, please click here.
  }
}

@misc{huawei_appgallery_cloud_testing,
  title = {AppGallery Connect Help Center - Cloud Testing},
  url = {https://developer.huawei.com/consumer/en/doc/distribution/app/agc-help-cloud-test-0000001156844797},
  author = {HUAWEI Developers},
  year = {2021},
  organization = {{Huawei}},   
}


@misc{huawei_appgallery_connect_service_whitepaper,
  title = {AppGallery Connect Service White Paper},
  url = {https://developer.huawei.com/consumer/en/doc/distribution/app/agc-help-service-white-paper-0000001156658451},
  year = {2021},
  organization = {{Huawei}},
  author = {{HUAWEI Developers}},
}

@misc{huawei_appgallery_integration_check,
  title = {AppGallery Connect Help Center - Integration Check},
  url = {https://developer.huawei.com/consumer/en/doc/distribution/app/agc-help-self-check-0000001100158786},
  author = {HUAWEI Developers},
  year = {2021},
  organization = {{Huawei}},   
}

@misc{huawei_android_crashservice_sdk_version_change_history,
  title = {Android Crash SDK Version Change History},
  url = {https://developer.huawei.com/consumer/en/doc/development/AppGallery-connect-Guides/agc-crash-sdkchangenotes-0000001054941952},
  author = {HUAWEI Developers},
  year = {2021},
  organization = {{Huawei}}, 
}

@misc{huawei_crashservice_codelab,
  title = {AppGallery Connect Crash Service Development},
  url = {https://developer.huawei.com/consumer/en/codelabsPortal/carddetails/CrashService},
  alternate_url = {https://developer.huawei.com/consumer/en/codelab/CrashService},
  author = {Huawei Developers},
  year = {2021},
  organization = {{Huawei}},
}

@misc{huawei_agc_success_stories,
  title = {Crash - Success Stories},
  url = {https://developer.huawei.com/consumer/en/doc/development/AppGallery-connect-Guides/agc-crash-stories-0000001058075936},
  author = {Huawei Developers},
  year = {2021},
  organization = {{Huawei}},
}

@online{huawei_introduction_to_appgallery_connect_crash_service,
  title = {Introduction to AppGallery Connect Crash Service},
  url = {https://forums.developer.huawei.com/forumPortal/en/topic/0204405648104650146},
  author = {Ritesh Chanchal},
  organization = {HUAWEI},
  year = {2020},
  extracts = {
  Crash Service notification
  Crash service monitors app in real time and notifies developer if any crash occurs. Developers will be reminded if a crash meets the following condition in last one hour:
    1. The crash rate of a problem is greater than the threshold 1\%
    2. Number of app launches is greater than 500.
    3. No crash notification has been triggered for this problem before. That is, the system does not repeatedly send notifications for the same problem.
    
  Tips and Tricks
    1.   Huawei Crash services work on non-Huawei device.
    2.   AGConnectCrash.getInstance().testIt(mContext) triggers app crash. Make sure to comment or remove it before releasing your app.
    3.   Crash Service takes around 1 to 3 minutes to post the crash logs on App Gallery connect dashboard/console.
    4.   Crash SDK collects App and system data.
      System data :
      AAID, Android ID (obtained when AAID is empty), system type, system version, ROM version, device brand, system language, device model, whether the device is rooted, screen orientation, screen height, screen width, available memory space, available disk space, and network connection status.
      App data: 
      APK name, app version, crashed stack, and thread stack.
    5.   The Crash SDK collects data locally and reports data to the collection server through HTTPS after encrypting the data.
    
    The Crash SDK collects data locally and reports data to the collection server through HTTPS after encrypting the data.
  }
}

@misc{yet_to_cite_hughes2012_how_to_read_dalvik_SIGQUIT_output,
  title = {How to read Dalvik SIGQUIT output},
  url = {http://elliotth.blogspot.com/2012/08/how-to-read-dalvik-sigquit-output.html},
  author = {Elliott N. Hughes},
  year = {2012},
  abstract = {
    If you're a long-time Java developer you're probably used to sending SIGQUIT to a Java process (either via kill -3 or hitting ctrl-\) to see what all the threads are doing. You can do the same with Dalvik (via adb shell kill -3), and if you're ANRing the system server will be sending you SIGQUIT too, in which case the output will end up in /data/anr/traces.txt (see the logcat output for details).

    Anyway, I've found that very few people actually know what all the output means. I only knew a few of the more important bits until I became a Dalvik maintainer. This post should hopefully clear things up a little.
  }
}

@misc{yet_to_cite_anchor_android_crash_dataset_on_github,
  title = {GitHub - Anchor-Locator/anchor: Locator for Android app Crashes},
  url = {https://github.com/Anchor-Locator/anchor},
  year = {2020},
  author = {Various developers},
  contact = {anchor-locator@gmail.com},
}

@misc{expo2019_issue5839,
  title = {Apps sometimes crash (call stack at BaseExperienceActivity\$2.run)},
  url = {https://github.com/expo/expo/issues/5839},
  year = {2019},
  author = {Wiebe Nieuwenhuis},
}

@misc{huawei_crashservice_github_examples,
  title = {huaweicodelabs/CrashService},
  url = {https://github.com/huaweicodelabs/CrashService},
  author = {Various developers},
  year = {2021},
  organization = {{Huawei}},
}

@misc{huawei2020_appgallery_connect_crash_service_article_on_medium,
  title = {Huawei AppGallery Connect Crash Service},
  url = {https://medium.com/huawei-developers/huawei-appgallery-crash-service-1861bd920143},
  author = {Serkan Mutlu},
  year = {2020},
  organization  = {{Huawei}},
  publisher = {{Medium Inc.}},  
}

@misc{huawei2020_press_release_on_hms_ecosystem,
  title = {Huawei Accelerates the Development of the HMS Ecosystem in Anticipation of the 5G Era},
  url = {https://www.prnewswire.com/news-releases/huawei-accelerates-the-development-of-the-hms-ecosystem-in-anticipation-of-the-5g-era-301010305.html},
  author = {Huawei Consumer Business Group},
  organization = {Cision US Inc.},
  year = {2020},
}
@misc{ibm_mobile_foundation_7_1_app_crash_analytics,
  title = {Foundation 7.1 Accessibility and Application Crash Analytics Support is now available},
  author = {Chevy Hungerford},
  year = {2016},
  url = {https://mobilefirstplatform.ibmcloud.com/blog/2015/12/31/foundation-7.1-accessibility-and-application-crash-analytics-support-is-now-available/},
  abstract = {
    This month IBM MobileFirst™ Platform Foundation released two new features in an iFix for Operational Analytics, accessibility, and application crash analytics.

    Crash Analytics: Previously you have had the ability to capture crashes, now with Operational Analytics you can analyze your captured application crashes, thus allowing you to better monitor and troubleshoot your applications. We now offer new charts for monitoring application crashes and charts for application troubleshooting on the dashboard of the Operational Analytics Console.

    Crash Analytics has an easy client-side implementation which makes capturing crashes a much simpler task. Rogue crashes are common on apps released to the public. Crash Analytics captures and analyzes rogue crashes and informs the Operational Analytics users. Knowing what causes these rogue crashes gives the developer the opportunity to debug and fix without having to spend the time reproducing the problem. With that being said, Crash Analytics helps your developers save time and betters your app user experience.

    Accessibility: Accessibility features enable people with disabilities, such as restricted mobility and limited vision, to work successfully with IBM MobileFirst™ Platform Foundation.

    The IBM MobileFirst Operational Analytics Console now provides accessibility features.
  },
}

@misc{ieee_and_acm_code_1999on,
  title = {Code of Ethics: IEEE-CS/ACM Joint Task Force on Software Engineering Ethics and Professional Practices},
  url = {https://www.computer.org/education/code-of-ethics},
  year = {1999},
  publisher = {IEEE},
  author = {{IEEE-CS/ACM joint task force on Software Engineering Ethics and Professional Practices (SEEPP)}},
}

@misc{inapptics2017_mobile_heatmap_visualise_user_behaviour,
  title = {Mobile Heatmap: One of the Best Tools to Visualize User Behavior},
  url = {https://uxplanet.org/mobile-heatmap-one-of-the-best-tools-to-visualize-user-behavior-f0c056d4c196},
  year = {2017},
  author = {{inapptics}},
  publisher = {{Medium Inc.}},
  abstract = {While traditional mobile app analytics provides key metrics and information on demographics, a mobile heatmap is an essential tool that visually displays user behavior in an app. Thus the value of heatmaps in tracking users’ app usage patterns is invaluable.}
}

@misc{karpenko2019_localhalo_a_social_network_for_neighbors,
  title = {A startup with a Ukrainian co-founder raises \$500k. They are making a social network for neighbors},
  url = {https://ain.ua/en/2019/10/18/localhalo-raises-500k/},
  author = {Olha Karpenko},
  publisher = {{aim}},
  year = {2019},
}

@misc{khan2019_medium_filtering_adb_logcat_efficiently,
  title = {Filtering Android adb logcat efficiently in bash command line},
  url = {https://medium.com/@hissain.khan/filtering-android-adb-logcat-efficiently-in-bash-command-line-4992fb1acd61},
  author = {Sazzad Hissain Khan},
  year = {2019},
  publisher = {{Medium Inc.}},
  organization = {Samsung},
}

@misc{kiwixandroid_issue_1223_bitbar_should_run_tests_on_multiple_devices,
  title = {Bitbar should run tests on multiple devices},
  url = {https://github.com/kiwix/kiwix-android/issues/1223},
  author = {Seán Mac Gillicuddy},
  year = {2019},
  organization = {{Kiwix-Android Project}},  
  abstract = {
  Is your feature request related to a problem? Please describe. Continuation of #110. After adding the devices listed in this ticket the build became nigh unpassable
  Describe the solution you'd like: Get rid of bitbar, use Firebase Test Lab.
  Describe alternatives you've considered: Find through trial and error find a combination on bitbar that works}
}

@misc{kiwixandroid_issue_1228_bibbar_not_working_with_split_apks,
  title = {Bitbar not working with split APKs},
  url = {https://github.com/kiwix/kiwix-android/issues/1228},
  author = {Seán Mac Gillicuddy},
  year = {2019},
  organization = {{Kiwix-Android Project}},
  abstract = {Uploads to bitbar on travis are not functional on develop or any derived branches. There is a possibility that with the advent of #1219 and the generation of a universal apk that bitbar will select it from the group of generated apks.},
}

@misc{kiwixandroid_issue_2907_anrs_exceed_bad_bhaviour_threshold_for_android_10_and_11,
    title = {Kiwix Android: ANRs exceed bad behavior threshold for Android 10 and 11},
    url = {https://github.com/kiwix/kiwix-android/issues/2907},
    author = {Julian Harty},
    year = {2022},
    organization = {{Kiwix-Android Project}},
    abstract = {
        Android Vitals shows that the Kiwix Android app exceeds the bad behavior threshold of 0.47\%. Currently, the ANR rate is 1.06\% which is an increase of 0.12\% over the previous 3 months, and significantly higher than the peer group of apps which has a median of 0.05\%.
    },
}

@misc{koutun2021_how_to_deal_with_tech_debt_at_the_scale_of_super_app,
  title = {How to deal with tech debt at the scale of super app},
  url = {https://medium.com/flo-health/how-to-deal-with-tech-debt-at-the-scale-of-super-app-90da136d576d},
  author = {Maksim Koutun},
  year = {2021},
  publisher = {Medium Inc},
  extract = {
    SLOs (Service Level Objectives) are objectives that the team must achieve to meet agreements, depending on the team’s maturity and product needs. You don’t need many of them — only system-wide and clear for everyone. In our case, they’re the crash rate, size of the app, app start time, and battery usage.
    
    For example, in the case of crash rate, we believe that we are mature enough to achieve a 99.9\% crash-free sessions rate, and we have a direct correlation between the number of such errors and the rating in app stores.
  }
}

@misc{knuth_the_bank_of_san_serriffe,
  title = {Knuth: The Bank of San Serriffe},
  url = {https://cs.stanford.edu/~knuth/boss.html},
  year = {2020},
  author = {Donald E. Knuth},
  publisher = {by author},
  organization = {Stanford University},
}

@misc{levy2016_crash_and_churn_report,
  url = {https://web.archive.org/web/20170317232255/https://data.apteligent.com/research/crash-and-churn},
  title = {Data Report: Crash \& Churn Edition},
  year = {2016},
  author = {{Apteligent}},
  organization = {{Apteligent}},
  publisher = {Internet Archive Wayback Machine},
  abstract = {In an industry first, Apteligent has quantified the correlation between mobile app crashes and increased churn rates. In this report, we not only deduce that a correlation exists — we leverage our data to show exactly how crashes drive an increase in churn. For those new to the space, churn can be thought of as the inverse of retention.},
}

@misc{levy2017_the_crash_and_burn_report_findings,
  title = {The "Crash and Burn" Report Findings},
  url = {https://www.apmdigest.com/the-crash-and-burn-report-findings},
  author = {Andrew Levy},
  year = {2017},
  organization = {Apteligent},
  publisher = {APM digest},
}

@misc{li2017_mining_device_Specific_app_usage_patterns,
      title={Mining Device-Specific Apps Usage Patterns from Large-Scale Android Users}, 
      author={Huoran Li and Xuan Lu},
      year={2017},
      eprint={1707.09252},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@misc{lightstephq2021_observability_will_never_replace_monitoring,
  title = {Observability will never replace Monitoring (because it shouldn’t)},
  url = {https://medium.com/lightstephq/observability-will-never-replace-monitoring-because-it-shouldnt-eeea92c4c5c9},
  author = {Ben Sigelman},
  year = {2021},
  publisher = {{Medium Inc.}},
  extracts = {
    ...the anatomy of observability. There are three layers:
      I. (Open)Telemetry: acquire high-quality data with minimal effort
      II. Storage: “Stats over time” and “Transactions over time”
      III. Benefits: *solve actual problems*
  }
}

@misc{liu2022_a_devs_thoughts_onDeveloper_productivity,
  title = {A dev's thoughts on developer productivity},
  url = {https://about.sourcegraph.com/blog/developer-productivity-thoughts},
  year = {2022},
  author = {Beyang Liu},
  publisher = {{Sourcegraph}},
  extracts = {
    tow nested [developer] loops
    
    developer hertz
    
    Great direction-setting means making the choices that get an end-to-end system up and running as quickly as possible. Getting an end-to-end system up quickly de-risks the overall project. Reaching a shippable state well before the appointed deadline means you can make further improvements in the time remaining. It's helpful to view the destination not as a single point, but as a zone of acceptable outcomes. Get into the acceptable zone first and then improve your position.
  }
}

@misc{lotan2015_apple_apps_and_algorithmic_glitches,
  title = {Apple, Apps and Algorithmic Glitches - A data analysis of iTunes’ top chart algorithm},
  url = {https://medium.com/message/apple-apps-and-algorithmic-glitches-f7bc8dd2cda6},
  author = {Gilad Lotan},
  year = {2015},
  publisher = {{Medium Inc.}},
  extract = {
    On October 29th and December 18th, 2014, something very strange happened to the iTunes top apps chart. Like an earthquake shaking up the region, all app positions in the chart were massively rearranged, some booted off completely. These two extremely volatile days displayed rank changes that are orders of magnitude higher than the norm — lots of apps moving around, lots of uncertainly.
    
    If you build apps for iOS devices, you know that the success of your app is contingent on chart placement. If you use apps on iPhones and iPads, you should realize just how difficult it is for app developers to get you to download their app. Apple deploys an algorithm that identifies the Top Apps across various categories within its iTunes app store. This is effectively a black box. We don’t know exactly how it works, yet many have come to the conclusion that the dominant factor affecting chart placement is the number of downloads within a short period of time.
  }
}

@misc{lotan2015_apples_app_charts,
  title = {Apple’s App Charts: 2015 Data and Trends …or how much harder it is to get into the top charts},
  url = {https://medium.com/i-data/apple-s-app-charts-2015-data-and-trends-abb95300df57},
  author = {Gilad Lotan},
  year = {2015},
  publisher = {{Medium Inc.}},
  extract = {
    I did not observe glitches in this year’s data. The algorithmic system governing the charts appears to have reached some level of stability. On the one hand, a stable system is great — it is predictable, there’s less uncertainty, which makes planning easier. But on the other had, there’s a stronger “rich get richer effect” — once an app makes it to an advantageous position, it’s heightened visibility reinforces it’s continued ranking.
  }
}

@misc{man2021_moonpig_atdd_part1,
  title = {The Android Testing Approach Part 1: ATDD},
  url = {https://medium.com/moonpigtech/the-android-testing-approach-part-1-atdd-6e26e3851c08},
  author = {Justin Man},
  year = {2021},
  publisher = {{Medium Inc.}},
}

@misc{mangold2020_google_analytics_data_sampling_what_you_need_to_know,
  title = {Google Analytics Data Sampling – What You Need To Know},
  author = {Benjamin Mangold},
  url = {https://www.lovesdata.com/blog/google-analytics-data-sampling},
  year = {2020},
  publisher = {lovesdata.com},
}

@misc{rauhut2021_how_accurate_is_sampled_google_analytics_data_a_simple_experiment_to_find_out,
  title = {How accurate is sampled Google Analytics data? A simple experiment to find out},
  author = {Marcus Rauhut},
  year = {2021},
  url = {https://marcusrauhut.com/how-accurate-is-sampled-google-analytics-data-a-simple-experiment-to-find-out/},
}

@misc{mcclintok_mixpanel_update_on_autotrack_data_collection,
  title = {Update on Autotrack data collection},
  url = {https://mixpanel.com/blog/update-autotrack-data-collection/},
  author = {Jon McClintok},
  year = {2018},
  organization = {Mixpanel},
  topics = {
    They screwed up and collected lots of sensitive data using their autotrack data collection.
    They decided to add the following changes to their practices:
      Incorporating additional privacy reviews as part of our design and development processes,
      In-depth security/privacy audits of key existing product areas,
      Operationalizing our response tooling,
      Data filtering and detection,
  }
}

@misc{moonpig_privacy_policy_2019_feb_21,
  title = {Privacy Policy | Moonpig},
  url = {https://web.archive.org/web/20190630124228/https://www.moonpig.com/uk/privacy-notice/},
  year = {2019},
  organization = {{Moonpig.com}},
  author = {Moonpig},
}

@misc{moonpig_privacy_policy_2021_aug_12,
  title = {Privacy Policy | Moonpig},
  url = {https://www.moonpig.com/uk/privacy-notice/},
  year = {2021},
  organization = {{Moonpig.com Limited}},
  author = {Moonpig},
}

@misc{moonpig2021_ipo_prospectus,
  title = {Prospectus},
  url = {https://www.moonpig.group/media/skfhxz3e/moonpig-group-plc-prospectus.pdf},
  year = {2021},
  organization = {{Moonpig Group plc.}},
  author = {{Moonpig Group plc.}},
}

@misc{moodspace2021_privacy_policy,
  title = {Privacy Policy | Moodspace},
  url = {https://moodspace.org/privacy-policy},
  year = {2021},
  author = {{Moodspace}},
}

@misc{mukherjee_implicit_versus_explicit_event_tracking_hits_and_misses,
  title = {Implicit Versus Explicit Event Tracking: Hits and Misses},
  url = {https://iterative.ly/blog/implicit-vs-explicit-event-tracking-hits-and-misses/},
  author = {Debdut Mukherjee},
  organization = {{Iteratively}},
  year = {2020},
  topics = {
  Hits & misses of implicit or codeless event tracking
  Hits & misses of explicit or code-based event tracking
  So what should you choose?
  },
}
@misc{meyer2018_towards_empirical_answers_to_important_engineering_questions,
  title = {Towards empirical answers to important software engineering questions},
  url = {https://bertrandmeyer.com/2018/01/26/towards-empirical-answers-important-software-engineering-questions/},
  year = {2018},
  author = {Bertrand Meyer},
  publisher = {Bertrand Meyer},
  source = {(Adapted from a two-part article on the Communications of the ACM blog.)},
  extracts = {
    The first has to do with the distinction introduced above between the two kinds of possible targets for empirical assessment: products (artifacts) versus processes.
    Both aspects are important, but one is much easier to investigate than the other. For software products, the material of study is available in the form of repositories mentioned above, with their wealth of information about lines of code, control and data structures, commits, editing changes, bug reports and bug fixes. Processes are harder to grasp. You gain some information on processes from the repositories (for example, patterns and delays of bug fixing), but processes deserve studies of their own. For example, agile teams practice iterations (sprints) of widely different durations, from a few days to a few weeks; what is the ideal length? A good empirical answer would help many practitioners. But this example illustrates how difficult empirical studies of processes can be: you would need to try many variations with teams of professional programmers (not students) in different projects, different application areas, different companies; for the results to be believable the projects should be real ones with business results at stake, there should be enough samples in each category to ensure statistical significance, and the companies should agree to publication of some form, possibly anonymized, of the outcomes. The difficulties are formidable.
    
    Indeed, this is what we are entitled to expect from empirical studies: guidance. The slogan of empirical software engineering is that software is worthy of study just like geological strata, photons, and lilies-of-the-valley; OK, sure, but we are talking about human artifacts rather than wonders of the natural world, and the idea should be to help us produce better software and produce software better.
  }
}

@misc{nikgapps,
  title = {NikGApps - Custom Google Apps Package!},
  url = {https://nikgapps.com/},
  author = {{NikGApps team}},
  year = {2021},
  notes = {Last visited~\nth{9} June 2021},
}

@misc{nist_pii,
  title = {PII Glossary | CSRC},
  url = {https://csrc.nist.gov/glossary/term/PII},
  organization = {{National Institute of Standards and Technology}},
  author = {{NIST}},
  year = {2021},
  notes = {Last visited~\nth{31} May 2021},
}

@misc{norwied2012_download_android_install_files,
  title = {Download Android install files *.apk from play.google.com},
  url = {https://norwied.wordpress.com/2012/08/10/download-android-install-files-apk-from-play-google-com/},
  author = {Norbert Wiedermann},
  year = {2012},
  notes = {Last visited~\nth{27} Jul 2021},
}

@misc{nytimes20191221_total_surveillance_is_not_what_america_signed_up_for,
  title = {Total Surveillance Is Not What America Signed Up For},
  url = {https://www.nytimes.com/interactive/2019/12/21/opinion/location-data-privacy-rights.html},
  author = {{The Editorial Board}},
  year = {2019},
  month = {21 Dec},
  publisher = {{NY Times}},
}

@misc{nytimes20210111_who_should_make_the_online_rules,
  title = {Who Should Make the Online Rules? - The New York Times},
  url = {https://www.nytimes.com/2021/01/11/technology/twitter-facebook-parler-rules.html},
  abstract = {A handful of unelected tech executives have tremendous influence on public discourse. Is that right?},
  year = {2021},
  month = {11 Jan},
  author = {Shira Ovide},
  publisher = {{NY Times}},
}

@misc{nytimes20210721_the_nightmare_of_our_snooping_phones,
  title = {The Nightmare of Our Snooping Phones - The New York Times},
  url = {https://www.nytimes.com/2021/07/21/technology/phones-location-data.html},
  unlocked_url = {https://www.nytimes.com/2021/07/21/technology/phones-location-data.html?unlocked_article_code=AAAAAAAAAAAAAAAACEIPuonUktbfqohkT1UZAibJUNMnqBqCgvfeh6Q8gXnzN22RTj1L0-USBc2M8lvEI6p_Yt95lxKqeOh8Cp59Dvpj0r0YeEV3VwijppbDlJpffCht99n2Djho0teQBetntDa3MTX8JbtyyefhtBnaYDG9S7WfhSN6XHttoZZhc16p2XMN1_2FRrYzgo8iqK9nUpNqRj4AZD2Iue3oC3h9PtaBaBLa6momSr0TGGGTzZPHteV2IEgFAknGTXh8_W829NpaXdsSN6r6JBMgE9HshbL7qcq8MesyI6IGg2pHKg&smid=url-share},
  year = {2021},
  month = {21 July},
  author = {Shira Ovide},
  publisher = {{NY Times}},
  abstract = {A Catholic official’s resignation shows the real-world consequences of practices by America’s data-harvesting industries.},
}

@misc{nytimes202206,
  title = {Our data is a curse, with or without Roe},
  abstract = {There is so much digital information about us out there that we can’t possibly control it all.},
  year = {2022},
  month = {29 June},
  author = {Shira Ovide},
  publisher = {{NY Times}},
  url = {https://www.nytimes.com/2022/06/29/technology/abortion-data-privacy.html},
}

@misc{objectbox2020_moodspace_interview,
  title = {MoodSpace Mobile App Use Case},
  year = {2020},
  author = {Alyssa Coke and Ian Alexander},
  url = {https://objectbox.io/moodspace-mobile-app-use-case/},
  abstract = {
    We speak with Ian Alexander, founder and lead developer at MoodSpace, a beautiful app making mental health exercises accessible to everyone. MoodSpace was released in 2019, and has over 150k+ downloads. The COVID-crises highlights the importance of digital support for wellbeing and saw MoodSpace surge. After trying several databases, Ian settled on ObjectBox because of its high performance and ease of use.
  }
}

@misc{opengapps,
  title = {The Open GApps Project},
  url = {https://opengapps.org/},
  organization = {The Open GApps Team},
  author = {{The Open GApps Team}},
  year = {2021},
  label = {Open GApps},
  notes = {Last visited~\nth{9} June 2021},  
}

@misc{openstf_website,
  title = {STF | Smartphone Test Farm},
  url = {https://openstf.io/},
  year = {2018},
  organization = {{CyberAgent, Inc.}},
  author = {{CyberAgent, Inc.}},
}

@misc{overops2021_what_causes_97pct_of_1billion_java_logged_errors,
  title = {We Crunched 1 Billion Java Logged Errors – Here’s What Causes 97\% of Them},
  url = {https://www.overops.com/blog/we-crunched-1-billion-java-logged-errors-heres-what-causes-97-of-them-2/},
  author = {Nick Andrews},
  year = {2021},
  abstract = {97\% of Logged Errors are Caused by 10 Unique Errors},
  organization = {{OverOps}},
}

@misc{read2018_digital_takeover_avionics,
  title = {Digital Takeover},
  author = {Bill Read},
  url = {https://www.aerosociety.com/news/digital-takeover/},
  year = {2018},
  publisher = {{Royal Aeronautical Society}},
}

@misc{coillet2016-wikimedia-kiwix-ten-years,
  title = {No internet? No problem! Kiwix celebrates ten years of offline Wikipedia reading},
  url = {https://diff.wikimedia.org/2016/10/11/kiwix-ten-years/},
  author = {Stéphane Coillet-Matillon},
  year = {2016},
  organization = {{Wikimedia Foundation}},
}

@misc{gaudin2017_wikimedia_kiwix_android,
  title = {Carry the entirety of Wikipedia in your pocket with Kiwix for Android},
  url = {https://diff.wikimedia.org/2013/04/17/carry-the-entirety-of-wikipedia-in-your-pocket-with-kiwix-for-android/},
  author = {Renaud Gaudin},
  year = {2017},
  organization = {{Wikimedia Foundation}},  
}

@misc{gomez2017_wikimedia_kiwix_article,
  title = {The future of offline access to Wikipedia: The Kiwix example},
  url = {https://diff.wikimedia.org/2017/10/02/offline-access-wikipedia-kiwix/},
  author = {Anne Gomez},
  year = {2017},
  organization = {{Wikimedia Foundation}},
}

@misc{yet_to_cite_alibabcloud2020_what_to_do_if_your_app_crashes_during_coronavirus_outbreak,
  title = {What Would You Do If Your App Crashes during Coronavirus Outbreak?},
  url = {https://www.alibabacloud.com/blog/what-would-you-do-if-your-app-crashes-during-coronavirus-outbreak_596036},
  author = {Ding Jie},
  key = {Alibaba},
  organization = {{Alibaba Cloud}},
  year = {2020},
  comments = {A server-/cloud-centric perspective. They encourage chaos engineering and offer fault injection services. From a production stability perspective they offer sentinel - a lightweight throttling framework and other practices to help the service to degrade gracefully.}
}

@misc{muntenescu2020_fewer_crashes_and_more_stability_with_kotlin,
  title = {Fewer crashes and more stability with Kotlin},
  url = {https://medium.com/androiddevelopers/fewer-crashes-and-more-stability-with-kotlin-b606c6a6ac04},
  author = {Florina Muntenescu},
  publisher = {{Medium Inc.}},
  year = {2020},
  abstract = {
    Users expect to have a seamless experience with your app. Crashes can lead to an increase in poor reviews, uninstalls and even damaging your brand perception. From talking to the community we know that one of the main reasons to adopt Kotlin is safer code. In this post I’ll share a couple of the ways Kotlin improved the stability of a few of our partners’ code but we’ll also look at the results of some Google Play store stats and see if there’s a correlation between using Kotlin and the number of crashes (spoilers: there is!).
  }
}

@misc{not_yet_cited_samsung2022_issues_and_bugs_channel,
  title = {Samsung Developers - Issues and Bugs Channel},
  url = {https://developer.samsung.com/partner/IBChannel/form?formName=MDc2MzY1},
  year = {2022},
  abstract = {
    Issues and Bugs Channel
    The issues and bugs channel is a communication channel specifically for app developers, for the purpose of enhancing quality monitoring and quality control of both your apps and Samsung devices.
    Apply for Partnership and Build new connection with us.
  }
}

@misc{scmp2021_chinas_covid_19_tracking_app_crashes,
  title = {China’s Covid-19 tracking app crashes as traffic surges amid fresh coronavirus outbreak},
  url = {https://www.scmp.com/tech/policy/article/3143487/chinas-covid-19-tracking-app-crashes-traffic-surges-amid-fresh},
  year = {2021},
  author = {Xinmei Shen},
  organization = {{South China Morning Post}},
  abstract = {
    The widely used app’s crash on Monday morning caused chaos for commuters in many places across the country
    The China Academy of Information and Communications Technology said the service resumed nationwide on Monday afternoon
    A widely used travel history tracking app jointly developed by the Chinese government and the country’s three mobile network operators crashed on Monday morning, as the country grapples with its most widespread Covid-19 outbreak in months.
    Users of the “telecommunications big data travel history” app could not load the program on Monday morning, resulting in chaos for commuters in many places across the country.
  }
}

@misc{scrumdictionary_chore,
  title = {"Chore."},
  author = {{ScrumDictionary.com}}, 
  note = {Accessed, \nth{08} July 2021},
  year = {2021},
  url = {https://scrumdictionary.com/term/chore/},
}

@misc{not_yet_cited_segmentio_analytics_android_issue_808_how_to_send_application_crashed_event,
    title = {How to send the Application Crashed event},
    url = {https://github.com/segmentio/analytics-android/issues/808},
    note = {Accessed \nth{13} July 2022},
    author = {Pablo Baldez},
    abstract = {
        I'm checking the documentation regarding to the lifecycle events and it's not clear to me how can I send the "Application Crashed" event to segment.
        It seems that it is not an event triggered by default, so I would like to know if there is an easy way to identify when the app is crashed and how to send it to my segment dashboard.
        
        you should be able to do it by implementing UncaughtExceptionHandler (see here) and do a track call in uncaughtException
    },
}

@misc{segmentio_analytics_android_issue_770_fake_implementation_for_testing,
  title = {Fake Implementation for Segment Analytics \#770},
  url = {https://github.com/segmentio/analytics-android/issues/770},
  year = {2021},
  month = {Jul},
  author = {Saad Farooq},
  organization = {Segment},
}

@misc{segment2018_we_test_in_production_you_should_too,
  title = {We test in production. You should too.},
  url = {https://segment.com/blog/we-test-in-production-you-should-too/},
  year = {2018},
  author = {Alan Braithwaite},
  organization = {Segment},  
}

@misc{sentry_customers,
  title = {Customers | Sentry},
  url = {https://sentry.io/customers/},
  quote = {1M developers at over 80K companies already use Sentry. Why don’t you?},
  organization = {{Sentry}},
  year = {2022},
  month = {Jan},
}

@misc{sentry_features_breadcrumbs,
  title = {Reproduce errors without user feedback},
  url = {https://sentry.io/features/breadcrumbs/},
  author = {{Sentry}},
  year = {2021},
  note = {Accessed, \nth{15} Sep 2021},
  quote = {Breadcrumbs show you events that lead to errors.},
}

@misc{sentry2021_mobile_vitals_four_metrics_every_mobile_developer_should_care_about,
  title = {Mobile Vitals - Four Metrics Every Mobile Developer Should Care About},
  url = {https://blog.sentry.io/2021/08/23/mobile-vitals-four-metrics-every-mobile-developer-should-care-about},
  author = {Philipp Hofmann},
  year = {2021},
  extracts = {
    To catch the most frustrating performance issues, you need to explore what’s happening on your users’ devices. That means visibility into how fast your app starts, duration of HTTP requests, number of slow and frozen frames, how fast your views are loading, and more. With Sentry for Mobile, you can now easily monitor your React Native, iOS, and Android app’s performance in real-time without additional setup (or accumulating a pile of testing devices).

    Mobile Vitals
    We believe there are four metrics every mobile team should track to better understand how their app is performing: Cold starts, warm starts, slow frames, and frozen frames. These four metrics, as a core part of Sentry’s performance monitoring, gives you the details you need to not only prioritize critical performance issues but trace the issue down to the root cause to solve them faster.
  }
}

@misc{sigg2016_sovereignty_of_apps_there_s_more_to_relevance_than_downloads,
      title={Sovereignty of the Apps: There's more to Relevance than Downloads}, 
      author={Stephan Sigg and Eemil Lagerspetz and Ella Peltonen and Petteri Nurmi and Sasu Tarkoma},
      year={2016},
      eprint={1611.10161},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
}

@misc{stackoverflow2013_getting_statistics_from_google_play_developer_console_with_an_api,
  title = {Getting statistics from Google Play Developers with an API},
  url = {https://stackoverflow.com/questions/14140728/getting-statistics-from-google-play-developers-with-an-api},
  year = {2013},
  author = {Cécile Fecherolle},
  abstract = {
         I am in charge of developing a website which should be able to show statistics from both Apple's app store and Google Play Store to clients, so they can easily see what's going on. I have figured out some ways to get App Store's data, but the Google Play Developers statistics seem way harder to get. I've heard of scraping, but this wouldn't be a great solution, as it would probably get broken whenever the developers console gets a major update. I'm looking for something which would work like Andlytics or App Annie do, as an example, so I could get data with AJAX or something else (JSON format maybe?) and put it into a database. For now, I haven't found any reliable solution (besides scraping, which seems like a unstable way to go), and this question has been asked a while ago, so I allow myself to ask it again, because maybe now there are some solutions to get around this. All I could find was Google Play APIs, which allow me to fetch data from the public page of the app, but not from developers console, with authentication. Any hints or help will be greatly appreciated :)
  },
  note = {Accessed, \nth{28} Feb 2022},
}

@misc{stackoverflow2018_how_can_i_get_app_crash_log_from_google_play_console,
  title = {How can I get app crash log from Google play console},
  url = {https://stackoverflow.com/questions/53612283/how-can-i-get-app-crash-log-from-google-play-console},
  year = {2018},
  author = {R15},
  note = {Accessed, \nth{28} Feb 2022},
  abstract = {
    I have android app, released to Play Store. Few users are getting crashes & crash asking them to submit report.
    I believe these reports are getting stored in google play store console. I am able to get crash count & related device except crash report. Where can I get all issues log report from console? Thank you
  },
  key_answer = {
    In the new Developer Console experience, starting to be active on November 2nd 2020, follow the steps below to see crashes and Android not responding (ANR)s reports:

    - Login to the developer console: https://play.google.com/apps/publish/
    - In the overview select an app
    - Look for the Quality header on the left menu
    - Select Android vitals
    - Select Crashes and ANRs
    - Change the filter to your needs
    https://stackoverflow.com/a/63612270/340175
  }
}

@misc{stackoverflow2022_minimal_reproducible_example,
  title = {How to create a Minimal, Reproducible Example},
  url = {https://stackoverflow.com/help/minimal-reproducible-example},
  author = {{Stack Overflow online help}},
  year = {2022},
  note ={Accessed, \nth{18} Feb 2022},  
  abstract = {
    When asking a question, people will be better able to provide help if you provide code that they can easily understand and use to reproduce the problem. This is referred to by community members as creating a minimal, reproducible example (reprex), a minimal, complete and verifiable example (mcve), or a minimal, workable example (mwe). Regardless of how it's communicated to you, it boils down to ensuring your code that reproduces the problem follows the following guidelines:
    
    Your code examples should be…
      …Minimal – Use as little code as possible that still produces the same problem
      …Complete – Provide all parts someone else needs to reproduce your problem in the question itself
      …Reproducible – Test the code you're about to provide to make sure it reproduces the problem
    The rest of this help article provides guidance on these aspects of writing a minimal, reproducible example.
  }
}

@misc{stackoverflow2021_search_help,
  url = {https://stackoverflow.com/help/searching},
  title = {How do I search? Help Center, Stack Overflow},
  author = {{Stack Overflow online help}},
  year = {2021},
  note ={Accessed, \nth{17} Nov 2021},
}

@misc{stevenson2016_how_does_firebase_initialize_on_android,
  title = {How does Firebase initialize on Android?},
  url = {https://firebase.blog/posts/2016/12/how-does-firebase-initialize-on-android},
  publisher = {The Firebase Blog},
  year = {2016},
  author = {Doug Stevenson},
  organization = {{Google Inc.}},
}

@misc{firebaseblog2022_whats_new_at_google_io,
  title = {What's new from Firebase at Google I/O 2022},
  url = {https://firebase.blog/posts/2022/05/whats-new-at-google-io},
  author = {Tyler Crowe},
  organization = {{Google Inc.}},
  publisher = {The Firebase Blog},
  year = {2022},
  note = {Accessed, \nth{22} Jun 2022},
}

@misc{sutherland2014_wikimedia_on_kelson,
  title = {Emmanuel Engelhart, Inventor of Kiwix: the Offline Wikipedia Browser},
  url = {https://diff.wikimedia.org/2014/09/12/emmanuel-engelhart-inventor-of-kiwix/},
  author = {Joe Sutherland},
  year = {2014},
  organization = {{Wikimedia Foundation}},
}

@misc{sunderland2019_the_one_star_android_review,
  title = {The 1-Star Android App Review},
  url = {https://medium.com/swlh/the-1-star-android-app-review-b2892756925f},
  author = {Thomas Sunderland},
  year = {2019},
  publisher = {{Medium Inc.}},
  abstract = {
    Deconstructing, Responding, and Avoiding: 3 Real-World Examples
  },
  relevance = {
    A really useful, practical explanation of how this developer addressed three distinct types of 1 star review. The example where they used the meta-data of the end user's device is very practical and a similar technique applies for the failure analysis when using mobile analytics.
  }
}

@misc{ukdataservice2021_research_data_management,
  title = {The importance of managing and sharing data},
  url = {https://ukdataservice.ac.uk/learning-hub/research-data-management/},
  author = {{UKDataService}},
  publisher = {{Economic and Social Research Council}},
  year = {2021},
  note = {Last accessed \nth{27} Nov 2021},
}

@misc{uxcam_hackermoon_2020_heatmapping,
  title = {Mobile App Heatmaps: A Powerful Weapon (And How to Use Them)},
  url = {https://hackernoon.com/mobile-heatmaps-a-powerful-secret-weapon-for-app-companies-311p36v9},
  author = {{UXCam}},
  year = {2020},
  publisher = {HackerMoon},
}

@misc{vodafone2021_huawei_appgallery,
  title = {Vodafone | What is the Huawei AppGallery?},
  url = {https://www.vodafone.co.uk/mobile/brands/huawei/appgallery},
  organization = {{Vodafone UK}},
  author = {Vodafone},
  year = {2021},
  abstract = {
    AppGallery is the official Huawei app platform, like the Google Play Store or the App Store. It’s safe to use, and has a four-layer detection mechanism to ensure app security. You can search for, download, manage, and share apps in the AppGallery. 

    Simply browse and download the apps you want and discover more exclusive offers, innovative experiences and fun, in-app activities.

    It has over one million applications worldwide, and has had over 180 billion cumulative application downloads in the past year. Currently, there are more than 45,000 apps available.
  }
}

@misc{explainxkcd_1098_star_ratings,
    author = {Explain xkcd contributors},
    title = {1098: Star Ratings - Explain xkcd},
    howpublished = {\url{https://www.explainxkcd.com/wiki/index.php?title=1098:_Star_Ratings&oldid=248516}},
    year = {2022},
    note = "[Online; accessed 09-July-2022]",
}

@misc{explainxkcd_937_tornadoguard,
    author = {Explain xkcd contributors},
    title = {937: TornadoGuard - Explain xkcd},
    howpublished = {\url{https://www.explainxkcd.com/wiki/index.php?title=937:_TornadoGuard&oldid=285031}},
    year = {2022},
    note = "[Online; accessed 09-July-2022]",
}

@misc{novoda_akan2016_asking_for_app_feedback_the_effective_way,
    title = {Asking for app feedback — the effective way},
    url = {https://blog.novoda.com/asking-for-app-feedback-the-effective-way/},
    author = {Denis Akan},
    year = {2016},
    publisher = {{Novoda}},
}

@misc{wikipedia_ecological_validity,
  author = "{Wikipedia contributors}",
  title = "Ecological validity --- {Wikipedia}{,} The Free Encyclopedia",
  year = "2020",
  howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Ecological_validity&oldid=975507963}",
  note = "[Online; accessed 29-August-2020]"
  }

@misc{wikipedia__knuth_reward_checks_2020, 
  title={Knuth reward check}, 
  url={https://en.wikipedia.org/wiki/Knuth_reward_check}, 
  journal={Wikipedia}, 
  publisher={{Wikimedia Foundation}},
  author = "{Wikipedia contributors}",
  year={2020}, 
  month={Feb}
}

@misc{wikipedia__digital_twin,
  title = {Digital twin},
  url = {https://en.wikipedia.org/wiki/Digital_twin},
  journal={Wikipedia}, 
  publisher={{Wikimedia Foundation}},
  author = "{Wikipedia contributors}",
  year={2020},
  note={Retrieved 16 July 2020},
}

@misc{wikipedia_firebase,
    author = "{Wikipedia contributors}",
    title = "Firebase --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2021",
    url = "https://en.wikipedia.org/w/index.php?title=Firebase&oldid=1037817193",
    note = "[Online; accessed 27-September-2021]"
}

@misc{wikipedia_moonpig,
    author = "{Wikipedia contributors}",
    title = "Moonpig --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2021",
    url = "https://en.wikipedia.org/w/index.php?title=Moonpig&oldid=1039484682",
    note = "[Online; accessed 24-September-2021]"
}

@misc{wikipedia_post_hoc_fallacy,
    author = "{Wikipedia contributors}",
    title = "Post hoc ergo propter hoc --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2021",
    howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Post_hoc_ergo_propter_hoc&oldid=1020538525}",
    note = "[Online; accessed 2-July-2021]"
}

  @misc{wikipedia_red_pill_or_blue_pill,
    author = "{Wikipedia contributors}",
    title = "Red pill and blue pill --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2022",
    howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Red_pill_and_blue_pill&oldid=1095853140}",
    note = "[Online; accessed 2-July-2022]"
  }

@misc{wikipedia_streetlight_effect,
  author = "{Wikipedia contributors}",
  title = "Streetlight effect --- {Wikipedia}{,} The Free Encyclopedia",
  year = "2020",
  url = "https://en.wikipedia.org/w/index.php?title=Streetlight_effect&oldid=994601415",
  note = "[Online; accessed 17-May-2021]"
  }

@misc{wikipedia_survivorship_bias,
  author = "{Wikipedia contributors}",
  title = "Survivorship bias --- {Wikipedia}{,} The Free Encyclopedia",
  year = "2020",
  howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Survivorship_bias&oldid=977467926}",
  note = "[Online; accessed 22-September-2020]"
}

@misc{wikipedia_rubicon,
    author = "{Wikipedia contributors}",
    title = "Rubicon --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2020",
    howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Rubicon&oldid=989791012}",
    note = "[Online; accessed 30-November-2020]"
  }

@misc{wikipedia_utf16,
    author = "{Wikipedia contributors}",
    title = "UTF-16 --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2022",
    url = "https://en.wikipedia.org/w/index.php?title=UTF-16&oldid=1089416569",
    note = "[Online; accessed 22-June-2022]"
}

@misc{w3techs_utf16,
  title = {Usage statistics of UTF-16 for websites},
  url = {https://w3techs.com/technologies/details/en-utf16},
  author = {{Q-Success trading as W3Techs}},
  year = {2022},
  note = "[Online; accessed 22-June-2022]",
}

@online{7_basic_quality_tools_with_R,
  title = {7 Basic Quality Tools with {R}},
  url = {https://towardsdatascience.com/7-basic-tools-of-quality-using-r-49fef5481e07},
  year = {2019},
  author = {Roberto Salazar},
  organization = {LinkedIn},
  publisher = {{Medium Inc.}},
}

@online{adil2020_sending_logs_from_flutter_apps,
  title = {Sending logs from Flutter apps in real-time using ELK stack \& MQTT},
  url = {https://itnext.io/sending-logs-from-flutter-apps-in-real-time-using-elk-stack-mqtt-c24fa0cb9802},
  author = {Umair Adil},
  organization = {ITNEXT},
  year = {2020},
}

@online{altindag2020_unit_testing_log_messages_made_easy,
  title = {Unit Testing Log Messages Made Easy},
  author = {Hakan Altındağ},
  url = {https://dzone.com/articles/unit-testing-log-messages-made-easy},
  year = {2020},
  abstract = {Unit testing presents specific challenges around logging. A developer and DZone Core members discusses an open source project he created to help.},
  publisher = {{DZone}},
  organization = {{DZone}},
}

@online{amplitude_are_you_data_driven,
  title = {Are You Data-driven, Data-informed or Data-inspired?},
  url = {https://amplitude.com/blog/data-driven-data-informed-data-inspired},
  author = {Shayna Stewart},
  year = {2019},
  organization = {{Y Media Labs}},
}

% Funny how Google included in docs for their Chinese domain https://developer.android.google.cn/topic/performance/vitals
@online{android_vitals_overview_2019,
    title = {Android vitals},
    url = {https://developer.android.com/topic/performance/vitals},
    abstract = {Android vitals is an initiative by Google to improve the stability and performance of Android devices. When an opted-in user runs your app, their Android device logs various metrics, including data about app stability, app startup time, battery usage, render time, and permission denials. The Google Play Console aggregates this data and displays it in the Android vitals dashboard},
    author = {Google Android},
    organization = {{Google Inc.}},
    urldate = {2019-06-17},
    date = {2019},
    year = {2019}
}

@online{android_vitals_performance_anrs,
    title = {ANRs | Android Developers},
    url = {https://developer.android.com/topic/performance/vitals/anr},
    abstract = {
        When the UI thread of an Android app is blocked for too long, an "Application Not Responding" (ANR) error is triggered. If the app is in the foreground, the system displays a dialog to the user, as shown in figure 1. The ANR dialog gives the user the opportunity to force quit the app.
        ANRs are a problem because the app’s main thread, which is responsible for updating the UI, can’t process user input events or draw, causing frustration to the user. For more information on the app’s main thread, see Processes and threads.
    },
    extracts = {
        An ANR will be triggered for your app when one of the following conditions occur:
            - While your activity is in the foreground, your app has not responded to an input event or BroadcastReceiver (such as key press or screen touch events) within 5 seconds.
            - While you do not have an activity in the foreground, your BroadcastReceiver hasn't finished executing within a considerable amount of time.
    },
    year = {2021},
    urldate = {2021-10-27},
    author = {Google Android},
    organization = {{Google Inc.}},
}

@misc{androiddevelopersblog2010_android_error_crash_reports,
  title = {Android Application Error Reports},
  url = {https://android-developers.googleblog.com/2010/05/google-feedback-for-android.html},
  year = {2010},
  author = {Jacek Surazski},
  organization = {{Google Inc.}},
  urldate = {2010-05-21},
  abstract = {
    The upcoming release of Android will include a new bug reporting feature for Market apps. Developers will receive crash and freeze reports from their users. The reports will be available when they log into their Android Market publisher account. No more blind debugging!
  }
}

@online{androiddevelopersblog2012_android_application_error_reports,
  title = {Android Developers Blog: Android Application Error Reports},
  url = {https://android-developers.googleblog.com/2010/05/google-feedback-for-android.html},
  author = {Jacek Surazski},
  year = {2010},
  organization = {{Google Inc.}},
  urldate = {2010-05-21},
  abstract = {
    The upcoming release of Android [Froyo 2.2]will include a new bug reporting feature for Market apps. Developers will receive crash and freeze reports from their users. The reports will be available when they log into their Android Market publisher account. No more blind debugging!
    
    When an app freezes or stops responding, the user can send a bug report to the developer with a click of a button, right from their phone. The new button appears in the application error dialog; if the user chooses to click it, the Google Feedback client running on the device will analyze the offending app and compose a report with information needed to diagnose it. The system is set up with user privacy in mind — the app developer will not receive information which could identify the user in any way. The user can also preview all information that will be sent.
    
    If users choose to do so, they may also send additional system information like device logs. Because there is a chance these may contain private information, they will not be passed on to the developer; they will be used by Google to track down bugs in the Android system itself.
    
    On the receiving end, developers will get tools to diagnose, triage and fix bugs in their apps. A popular app can generate hundreds of thousands of reports. Google Feedback aggregates them into "bugs" - individual programming errors. Bugs are displayed to developers sorted by severity, measured as the rate at which reports for the bug are flowing in.
    
    Clicking on a bug will display information such as stack traces, statistics about which type of hardware the bug occurred on and what versions of the app the user was running. In case of freezes, stack traces for all threads in the app will be displayed. This data should give developers a good idea how well their apps are faring in the wild.
    
    Google is constantly working on improving and extending the feedback feature to provide developers with tools to improve the quality of their apps. The benefits should be felt by both developers and their users.
  }
}

@misc{androiddevelopersblog2017_android_vitals_increase_engagement_etc,
  title = {Android vitals: Increase engagement and installs through improved app performance},
  url = {https://android-developers.googleblog.com/2017/07/android-vitals-increase-engagement-and.html},
  author = {Fergus Hurley},
  year = {2017},
  urldate = {2017-07-10},
  organization = {{Google Inc.}},  
}

@online{androiddevelopersblog2019_io2019_whats_new_in_play,
  title = {Android Developers Blog: I/O 2019: New features to help you develop, release, and grow your business on Google Play},
  url = {https://android-developers.googleblog.com/2019/05/whats-new-in-play.html},
  year = {2019},
  author = {Kobi Glick},
  organization = {{Google Inc.}},
}

@misc{androiddevelopersblog2019_unlock_your_creativity_2_5_billion,
  title = {Unlock your creativity with Google Play Pass},
  url = {https://android-developers.googleblog.com/2019/09/unlock-your-creativity-with-google-play.html},
  year = {2019},
  author = {Shazia Makhdumi},
  extract = {
    With over 2.5 billion active Android devices, Google Play helps your apps and games get discovered by billions of users worldwide. 
  },
  organization = {{Google Inc.}},
}

@misc{androiddevelopersblog2021_gpc_powers_better_strategic_decisions_etc,
  title = {Google Play Console powers better strategic decisions with new engagement metrics and unique benchmarks},
  url = {https://android-developers.googleblog.com/2021/03/google-play-console-powers-better.html},
  author = {Tom Grinsted},
  year = {2021},
  organization = {{Google Inc.}},
}

@online{androiddevelopers2020_permission_denials,
  title = {Permission Denials - Android Developers},
  url = {https://developer.android.com/topic/performance/vitals/permissions},
    author = {{Google Android Documentation}},
    organization = {{Android Developers}},
    urldate = {2020-11-24},
    year = {2020},
}

@misc{androiddevelopers2015_integrate_play_data_into_your_workflow_with_data_exports,
  title = {Integrate Play data into your workflow with data exports},
  url = {https://android-developers.googleblog.com/2015/04/integrate-play-data-into-your-workflow.html},
  year = {2015},
  author = {Frederic Mayot},
  organization = {{Google Inc.}},
  urldate = {2015-04-29},
  abstract = {
    The Google Play Developer Console makes a wealth of data available to you so you have the insight needed to successfully publish, grow, and monetize your apps and games. We appreciate that some developers want to access and analyze their data beyond the visualization offered today in the Developer Console, which is why we’ve made financial information, crash data, and user reviews available for export. We're now also making all the statistics on your apps and games (installs, ratings, GCM usage, etc.) accessible via Google Cloud Storage.
  }
}

@misc{appannie2021,
  title = {App Annie - The App Analytics and App Data Industry Standard},
  url = {https://www.appannie.com/en/},
  organization = {{App Annie}},
  author = {{App Annie}},
  year = {2021},
  products = {
    App Annie Intelligence: With data on over 8 million apps and thousands of websites, get the complete picture of the mobile landscape you need to acquire and retain customers, prioritize your roadmap, enter new markets, and optimize ROI.
    App Annie Ascend: Manage, enrich and identify hidden performance opportunities across your own advertising and monetization data. Access intuitive dashboards, advanced normalization tools, and data from 400+ partner connections — all in one place.
    App Annie Connect: Track your own apps' most critical data, including downloads, revenue, usage, and advertising – all in one place.
  }
}

@online{apple2020_how_to_review_your_apps_crash_logs,
  title = {How to review your app’s crash logs},
  url = {https://developer.apple.com/news/?id=nra79npr},
  year = {2020},
  urldate = {2020-06-09},
  author = {{Apple Inc.}},
  organization = {{Apple}},
}

@online{appleappstore2021_app_completeness,
  title = {App Store Review Guidelines - Apple Store: 2. Performance - App Completeness},
  url = {https://developer.apple.com/app-store/review/guidelines/#app-completeness},
  author = {{Apple Inc.}},
  organization = {Apple Inc.},
  year = {2021},
  note = {Visited \nth{12} July 2021},
  quote = {Please don’t treat App Review as a software testing service. We will reject incomplete app bundles and binaries that crash or exhibit obvious technical problems.},
  see_also = {After You Submit -> Bug Fix Submissions: For apps that are already on the App Store, bug fixes will no longer be delayed over guideline violations except for those related to legal or safety issues. If your app has been rejected, and qualifies for this process, please use the Resolution Center to communicate directly with the App Review team indicating that you would like to take advantage of this process and plan to address the issue in your next submission.}
}

@online{appleappstore2021_review_avoiding_common_app_rejections,
  title = {App Review - App Store - Apple Developer: Avoiding common app rejections},
  url = {https://developer.apple.com/app-store/review/},
  author = {{Apple Inc.}},
  organization = {Apple Inc.},  
  year = {2021},
  quote = {On average, over 40\% of app rejections are for Guideline 2.1 – Performance: App Completeness.},
  extract = {
    Avoiding common app rejections
      We’ve highlighted some of the most common issues that cause apps to get rejected to help you better prepare your apps before submitting them for review.
    Crashes and bugs
      You should submit your app for review only when it is complete and ready to be published. Make sure to thoroughly test your app on devices running the latest software and fix all bugs before submitting. For apps already on the App Store that may have minor guideline issues, bug fixes can be approved as long as there are no legal concerns.
  },
}

@online{appledeveloper2020_bug_reporting_feedback_assistant_for_developers,
  url = {https://developer.apple.com/bug-reporting/},
  title = {Bug Reporting - Feedback Assistant for Developers},
  year = {2020},
  urldate = {2020-08-28},
  author = {{Apple Developers}},
  organization = {{Apple}},
}

@online{calleosoftware_AppPulseMobile,
  title = {AppPulse Mobile},
  url = {https://www.calleosoftware.co.uk/products/application-monitoring/apppulse-mobile},
  year = {[2015?]},
  author = {{Calleo Software}},
  organization = {{Calleo Consultants Ltd.}},
}

@online{ft2020_apple_risks_losing_an_epic_challenge,
  title = {Apple risks losing an epic challenge},
  url = {https://www.ft.com/content/a01807f8-606c-4444-8a27-398984e3bf3d},
  year = {2020},
  author = {{The Editorial Board}},
  organization = {{The Financial Times Limited}},
  quotes = {
	This week, Yvonne Gonzalez Rogers, a US district judge, ruled that while the two companies were locked in litigation Apple could continue to ban Fortnite from its App Store for violating its guidelines. But she also ruled that Apple could not revoke Epic’s right to access its developer ecosystem, harming innocent bystanders. 

	Existing laws do not adequately cover all the complex dynamics and inherent conflicts of interest in corporate-run digital markets, such as Apple’s App Store, Google’s Play Store or Amazon’s Marketplace. As it is, Apple itself plays in its own market as an app developer while operating as judge, jury, executioner and court of last appeal for all others. If Apple does not itself update its App Store to distinguish between those roles and become more flexible and transparent, then it can hardly complain if legislators eventually deploy far more blunt instruments to enforce those changes.
  },
}

@online{ft2020_apple_tracks_iphone_users_without_consent,
  title = {Apple tracks iPhone users without consent, claims activist Max Schrems},
  url = {https://www.ft.com/content/aa43188a-0624-48b2-bc18-96b1e78df836},
  author = {Javier Espinoza and Siddharth Venkataramakrishnan},
  year = {2020},
  publisher = {Financial Times},
  organization = {{The Financial Times Limited}},
  quote_1 = {According to noyb, the unique tracking code generated by each iPhone lets Apple and all iPhone app developers see how users behave without their knowledge or agreement},
}

@online{ft2021_building_trust_in_ai_systems_is_essential,
  title = {Building trust in AI systems is essential},
  url = {https://www.ft.com/content/85b0882e-3e93-42e7-8411-54f4e24c7f87},
  year = {2021},
  author = {{The Editorial Board}},
  organization = {{The Financial Times Limited}},
  quotes = {
	"Translating such high principles into everyday practice is hard, especially when so much money is at stake. But three rules should always apply. First, teams that develop AI systems must be as diverse as possible to reduce the risk of bias. Second, complex AI systems should never be deployed in any field unless they offer a demonstrable improvement on what already exists. Third, algorithms that companies and governments deploy in sensitive areas such as healthcare, education, policing, justice and workplace monitoring should be subject to audit and comprehension by outside experts."

  }
}

@online{flutter_dev_site,
  title = {Flutter - Beautiful native apps in record time},
  url = {https://flutter.dev/},
  organization = {{Google LLC}},
  author = {{Google}},
  year = {2020},
  urldate = {2020-09-09},
  abstract = {Flutter is Google’s UI toolkit for building beautiful, natively compiled applications for mobile, web, and desktop from a single codebase.},
}

@online{gdpr_article_17_right_to_erasure,
  title = {Art. 17 GDPRRight to erasure (`right to be forgotten’)},
  url = {https://gdpr-info.eu/art-17-gdpr/},
  author = {{intersoft consulting}},
  organization = {{intersoft consulting services AG}},
  year = {2020},
  notes = {Retrieved 08 October 2020},
  privacy = {https://gdpr-info.eu/imprint-privacy-policy/},
}

@online{google_code_in_archive,
  title = {Google Code-in Archive},
  url = {https://codein.withgoogle.com/archive/},
  year = {2020},
  organization = {{Google Inc.}},
  author = {{Google}},
  abstract = {Google Code-in was a contest that introduced pre-university students (ages 13-17) to open source software development. The contest was held for 10 years starting in November 2010 and wrapping up the final contest in January 2020.},
}

@online{google_play_how_to_use_the_play_console,
  title = {How to use the Play Console},
  url = {https://support.google.com/googleplay/android-developer/answer/6112435?hl=en-GB},
  organization = {{Google}},
  author = {{Google}},
  year = {2020},
  summary = {Describes the 4 steps to register for a Google Play Developer Account: 1. Sign up for a Google Play Developer account, 2. Accept the Developer Distribution Agreement, 3. Pay the registration fee, of \$25 USD, 4 Complete your account details.}
}

@misc{google_play_launch_checklist,
  title = {Launch checklist},
  url = {https://developer.android.com/distribute/best-practices/launch/launch-checklist},
  year = {2020},
  last_updated = {Last updated 2020-04-16.},
  author = {{Google Developers}},
  organization = {{Google}},
  quote = {[Checklist step 2] `Prepare your developer account. Sign up for a developer account and check your developer account details are accurate. If you're going to sell products, set up your merchant account.'}
}

@online{google_play_policy_center_broken_functionality,
  author = {{Google Inc.}},
  organization = {{Google Inc.}},
  title = {[Broken Functionality] - Spam and Minimum Functionality - Developer Policy Center},
  url = {https://play.google.com/about/spam-min-functionality/min-functionality/},
  year = {2019}
}

@misc{google_play_troubleshoot_app_statistics_problems,
  author = {{Google Inc.}},
  organization = {{Google Inc.}},
  title = {Troubleshoot app statistics problems},
  url = {https://support.google.com/googleplay/android-developer/answer/9859366?hl=en-GB},
  year = {2022},
  abstract = {If you're having trouble with your app statistics data, the information below may help.},
  extracts = {
    View updates to Google Play statistics
    Note: The updates below are entered and ordered chronologically. Older updates may no longer be relevant.
    
    - 26 July and 26 August 2019: Missing user acquisitions data. On two separate occasions during the dates above, a temporary error caused the loss of some user acquisitions data from Play Console. We are unable to fix the data for this time period.
    - August 2019: Subscription analytics bug fix. On 21 August, we fixed a bug in our subscriptions data. For many apps, this may have caused a change in active subscriptions reported in Play Console and subscription analytics CSV exports for the past year and more. Revenue reporting in both the Play Console and CSV exports is not affected by this change. Data prior to 25 May 2018 could not be corrected and has been removed from Play Console.
    - July–August 2019: Incorrect attribution of deep link traffic. Between 25 July and 8 August 2019, the majority of traffic from other acquisition channels were incorrectly classified as 'Play Store (organic)'. You may see an increase in 'Play Store (organic)' visits during this time period. We are unable to fix the data for this time period.
    - May–July 2019: Under-reporting Play Store search acquisition performance. From 28 May to 31 July 2019, visits and installs from Store Listing previews (which allow users to view screenshots and install directly from search results) were not included in the Play Store search acquisition channel. These installs were instead reported as 'Installs without Store Listing visit'. You may see a drop in Play Store Search visits and installs and an increase in 'Installs without store listing visit' during this time period. We are unable to fix the data for this time period.
    - 4 March 2019: Install data discrepancy. Between 4 March 2019 and 6 March 2019, some updates were mistakenly reported as uninstalls followed by installs. Your install data for this period may be inflated.
    - 15 October 2018: Data source update. The data source for some metrics within Play Console has been updated. The updated data source measures some metrics, like 'Installs on active devices', more accurately. You  may also notice a change to other metrics and most dimensions (such as operator or language).
    - 17 July 2018: Pre-installs included in installation metrics. As of 17 July 2018, more comprehensive pre-install data is included in installation metrics. If your app comes pre-installed on any devices, you may notice an increase in 'Installs by user', 'Installs by device' and 'Installs on active devices'.
    - 16 July 2018: Deprecation of 'Cumulative installs by user'. As of 16 July 2018, your app's 'Cumulative installs by user' data will no longer be available on the Statistics page. If you need to calculate this data point, you can take your app's final 'Cumulative installs by user' entry and add the sum of 'Installs by user' since that date.
    - May 2018: Changes to your Play Console data. Over the next several weeks, you may notice some changes to reports, app deletion and account data within Play Console.
    - 25 October 2017: Corrected data for new, active and cancelled subscriptions. Between 9 May 2017 and 17 July 2017, some subscriptions that failed at purchase were miscategorised as started subscriptions. Sales and payout reports weren't impacted, but data on new, active and cancelled subscriptions may have been inflated in other Google Play Console reports. The data from this time period has now been corrected.
    - 5 December 2016: Increased refresh rate for 'daily installs and uninstalls by user' data. Starting on 5 December 2016, we'll reduce the latency associated with updating your app's 'daily installs by user' and 'daily uninstalls by user' data on the Statistics page. This means that you'll be able to access this data sooner than you previously could. For some apps, you may notice a slight shift in 'daily installs by user' and 'daily uninstalls by user' data on this date.
    - 30 November 2016: Change to Store Listing experiment results and your app's current installs data. Store Listing experiment results: Starting from 30 November 2016, results for any new or pending store listing experiments will be based on the metric: 'Installs on active devices'. If you started an experiment before 1 September that's scheduled to end after 1 December, we'll provide you with data going back to 1 September. For the most accurate results, we recommend ending any experiments that began before 1 September and creating a new experiment. Current installs data: Your 'Current installs by device' and 'Current installs by user' data will be available until 30 November 2016 on the Statistics page and in exports. For data after 30 November, use 'Installs on active devices' instead. For more information, see the 'Introduction of installs on active devices' section below.
    - 29 September 2016: Introduction of 'Installs on active devices': We're introducing a new metric called 'Installations on active devices'. Initially, you'll see this metric on your app's Statistics page, with data going back to 1 September. Later this year, the new metric will replace 'Current installs by device' and 'Current installs by user' in all current reports. You'll still be able to access historical data for the replaced metrics in charts and CSV downloads. Why are install metrics changing? 'Installations on active devices' shows how many devices that have been online at least once in the past 30 days have your app installed. Your app's 'Current installs by device' and 'Current installs by user' data has included devices that were last active up until several months ago. By switching to 'Installs on active devices', we're able to focus on devices used within the last 30 days to show you recent user engagement with your app. Impact on your metrics: Since 'Current installs by device' and 'Current installs by user' included a longer activity window, it's expected that you'll see lower numbers for your app's 'Installs on active devices' data. (They also published a timeline that was relevant during the changeover period.)
    - 29 November 2015: Change in 'Current installs by user', 'Current installs by device' and 'Total installs by user'. Starting from 29 November 2015, we will introduce new criteria for counting user and install metrics to account for users and devices going inactive. This change will affect the following metrics: 'Current installs by user', 'Current installs by device' and 'Total installs by user'. As we make these changes, you may notice that these metrics decrease over time for some apps.
    - 27 June 2015: Update to Google Play installation statistics. On 27 June 2015, we updated the install statistics on Play Console to use more accurate and reliable data. As a result, some developers may see small changes in their apps’ install and uninstall data on their apps’ Statistics pages.
    - 28 May 2014: Counting of pre-installed apps and multiple users on the same device. On 28 May 2014, we updated our statistics to make corrections to how we counted factory resets for pre-installed apps and apps that are used by multiple users on the same device. Most developers will see minimal impact to metrics, though certain developers will see significant impact to Daily Device Installs, Daily Device Uninstalls and Current Device Installs.
    - 5 August 2013: Update to better identify active devices. As of 5 August 2013, we are updating our stats algorithm to better identify active devices. This update will improve the accuracy of the installation metrics for active device installs and active user installs. When the new algorithm is rolled out, you may notice changes in some of the metrics associated with your app. Many apps will be affected by this change; however, we expect a difference of less than 2\% for most apps.
    - December 2012 – January 2013: Possible decline in total user installs and total device installs for paid apps due to data migration. From December 2012 to January 2013, developers may notice a decrease in the total user install and total device install counts of the paid apps displayed in Play Console. This is the result of an ongoing data migration, and is the true count for your app. The migration will be done in stages and will continue until January 2013. This migration will not affect previous developer payouts.
    - 4 April 2012: Better identification of Android platform version. As of 4 April 2012, we have improved the accuracy of statistics regarding the Android platform version number. As a result, you will see fewer devices classified as 'Other' or '0' and more devices classified accurately
    - 13 February 2012: Inactive Android devices. As of 13 February 2012, we have removed data about Android devices that are no longer active, which corrected active users and devices for many apps.
    - 28 January 2012: Total user installs undercounting. As of 28 January 2012, we have resolved an issue that caused undercounting of total user installs.
    - 2 May 2011: Change in the number of total installs. As of 2 May 2011, we've changed the basis for calculation of install numbers provided in Play Console. Previously, app installs were calculated per user and did not account for certain uninstall activities (for example, when a device was factory reset). Installs are now counted per device, with duplicates more effectively filtered out. This offers you a more accurate measure of your app distribution in the Android ecosystem. You may notice that this change has resulted in a one-off decrease in the number of total installs for your apps. We apologise for any confusion that this may have caused.
    - 18 October 2011: Decrease in the number of active installs. As of 18 October 2011, developers may notice a decrease in the active install counts displayed in Play Console. Some app updates were being counted as device installs. We are now using a different calculation methodology which does not incorporate installing an app update. Going forward, the active installs metric will correctly reflect the device installations. Please note that total install counts were not affected.
  }
}

@misc{google_play_view_crashes_and_anr_errors,
  title = {View crashes and application not responding (ANR) errors},
  url = {https://support.google.com/googleplay/android-developer/answer/9859174},
  author = {{Google Inc.}},
  organization = {{Google Inc.}},
  year = {2019},
}

@online{google_summer_of_code,
  title = {Google Summer of Code},
  url = {https://summerofcode.withgoogle.com/archive/},
  year = {2020},
  organization = {{Google Inc.}},
  author = {{Google}},
  abstract = {Google Summer of Code is a global program focused on bringing more student developers into open source software development. Students work on a three month programming project with an open source organization during their break from university.

  Since its inception in 2005, the program has brought together over 17,000 student participants and over 35,000 mentors from 124 countries worldwide. Google Summer of Code has produced over 38 million lines of code for 715 open source organizations.},
}

@online{hall2015_HP_courts_developers_with_tools_for_monitoring_mobile_apps,
  title = {HP Courts Developers with Tools for Monitoring Mobile Apps},
  url = {https://thenewstack.io/hp-courts-developers-with-tools-for-monitoring-mobile-apps/},
  author = {Susan Hall},
  year = {2015},
  publisher = {{The New Stack}},
  organization = {{The New Stack}},
}

@online{helloworld2017,
  title = {Say `Hello World' in 28 Different Programming Languages},
  url = {https://excelwithbusiness.com/blog/say-hello-world-in-28-different-programming-languages/},
  year = {2017},
  author = {Amanda Fielding},
  organization = {Excel with Business},
  _details_obtained_from = {https://excelwithbusiness.com/blog/tag/languages/ the first result},
  abstract = {Computers are dumb. They only do what they’re told. How do you tell a computer what to do? You use a programming language. The very first thing you’ll do when learning a new programming language is how to make the computer display “Hello, World”.}
}

@online{dodson2019_google_completely_terminated_our_new_business_etc,
  title = {Google completely terminated our new business via our Google Play Developer Account},
  url = {https://blog.usejournal.com/google-wrongly-terminated-our-new-business-via-our-google-play-developer-account-5f5b7b742542},
  author = {Mark Dodson},
  organization = {HoopApp},
  publisher = {{Medium Inc.}},
  abstract = {A plea to all android app developers and small start-up tech business owners to come together and force Google to change their automatic termination policies. And see https://www.hoopapp.co.uk/},
  year = {2019},
  month = {February},
  day = {7},
  note = {Last checked on 2020-01-26}
}

@misc{googlesupport_reviews_analysis,
  title = {View and analyse your app's ratings and reviews},
  url = {https://support.google.com/googleplay/android-developer/answer/138230},
  year = {2022},
  author = {{Google}},
  extract = {
  Analyse your reviews:
  To help you target the most impactful improvements to your app or game, you can view top trends and issues that users mention in your app's reviews. For tips on analysing your reviews, visit the Android Developers site.
  To see top trends and issues for your app, open Play Console and go to the Reviews analysis page.
  The following features are available on the web version of Play Console.
    - Review highlights: see popular themes in your app's reviews
    - Benchmarks and topics: see how different topics impact your app rating
    - Updated ratings: see how users update ratings and reviews over time
  }
  
}

@misc{google_use_pre_launch_reports,
  title = {Use pre-launch reports to identify issues},
  url = {https://support.google.com/googleplay/android-developer/answer/7002270?hl=en},
  year = {2020},
  author = {{Google}},
}

@misc{parker2012_infinite_scrolling,
  title={Infinite Scrolling},
  author={Parker, Stefan and Odio, Sam and Mosseri, Adam},
  year={2012},
  month=jan # "~12",
  publisher={Google Patents},
  note={US Patent App. 12/833,901}
}

@misc{pecorini_blue_pill_or_red_pill,
  title = {Blue Pill Or Red Pill ?},
  url = {https://www.qcc.cuny.edu/socialSciences/ppecorino/INTRO_TEXT/Chapter\%201\%20Introduction/Blue-Pill-orRed-Pill.htm},
  author = {Philip A. Pecorino},
  year = {2000},
  comment = {The date is estimated based on the HTML source and the bibliography.},
  extracts = {
    Surveys indicate many people prefer the comfort of the world of the BLUE Pill  See one such survey.
    The Blue Pill path is attractive to those with concern for self over others and for the present over the long term but it too often proves to be quite the opposite of what was hoped for when choosing the Blue Pill.
  }
}

@misc{peters2021_google_fixes_issue_causing_android_apps_to_crash_etc_webview,
  title = {Google fixes issue causing Android apps to crash with updates to Chrome and WebView},
  year = {2021},
  author = {Jay Peters},
  url = {https://www.theverge.com/2021/3/22/22345696/google-android-apps-crashing-fix-system-webview},
  publisher = {{The Verge}},
}

@misc{play_console_help_android_vitals_2019,
    title = {Monitor your app's technical performance with Android vitals},
    url = {https://support.google.com/googleplay/android-developer/answer/7385505},
    url_in_2022 = {https://support.google.com/googleplay/android-developer/answer/9844486},
    abstract = {
      Using Play Console, you can view data to help you understand and improve your app's battery usage, stability and render time.
      The following data is collected from users who have opted in to automatically share usage and diagnostics data from a subset of Android devices and OS versions. For more information about how Android users opt in to share data, go to the Accounts Help Centre https://support.google.com/accounts/answer/6078260.
    },
    author = {{Google}},
    urldate = {2019-07-23},
    year = {2019}
}

@misc{play_console_help_compare_your_apps_android_vitals_and_ratings_with_peer_groups,
  title = {Compare your app’s Android vitals and ratings with custom peer groups},
  url = {https://support.google.com/googleplay/android-developer/answer/9842755},
  abstract = {
    With custom peer groups, you can compare your app’s Android vitals and ratings data with a group of apps that you select.
    Here are a few important things to know about custom peer groups:
    - You can create two custom peer groups: one for Android vitals and another for ratings.
    - You can edit a custom peer group for Android vitals up to three times per month.
    - Once you create a custom peer group for Android vitals or ratings, you can select it on other pages within that section.
    - If an app in a custom peer group becomes unavailable (e.g. unpublished apps or apps that opt out from peer benchmarking), it’s replaced with a recommended app.
  },
  year = {2022},
  author = {{Google Inc.}},
  note = {Retrieved \nth{1} March 2022},
}

@misc{googlepatent_hyman2016_collecting_application_usage_analytics,
  title={Method and system for collecting and providing application usage analytics},
  author={Hyman, Jonathan and Magnuson, William},
  year={2016},
  month=jan # "~19",
  publisher={Google Patents},
  note={US Patent 9,239,771}
}

@online{appbrain_download_statistics_june_2019,
    title = {Android app download statistics on Google Play},
    url = {https://www.appbrain.com/stats/android-app-downloads},
    author = {AppBrain},
    organization = {{AppBrain}},
    urldate = {2019-06-20},
    date = {2019-06-19},
    year = {2019}
}


@online{appbrain_android_analytics_libraries_23-oct_2019,
    title = {Android analytics libraries},
    url = {https://www.appbrain.com/stats/libraries/tag/analytics/android-analytics-libraries},
    author = {{AppBrain}},
    urldate = {2019-10-22},
    date = {2019-10-23},
    year = {2019},
    note = {Retrieved 2019-Oct-23}
}

@online{appbrain_android_crash_reporting_libraries_18_oct_2019,
    title = {Android crash reporting libraries},
    url = {https://www.appbrain.com/stats/libraries/tag/crash-reporting/android-crash-reporting-libraries},
    author = {{AppBrain}},
    urldate = {2019-08-09},
    date = {2019-08-08},
    year = {2019},
    month = {Oct},
    day = {18},
    note = {Retrieved 2018-Oct-18}
}


@online{appbrain2021_firebase,
  url = {https://www.appbrain.com/stats/libraries/details/firebase/firebase},
  title = {Firebase},
  year = {2021},
  author = {{AppBrain}},
  note = {Statistics obtained on 2021-Jan-05},
  extracts = {
      Statistics
    Market share overall
    68.30\% of apps	
    82.88\% of installs	
    Market share in top apps 
    94.73\% of apps	
    76.63\% of installs	
    Market share in new apps 
    79.25\% of apps	
    99.58\% of installs	
  }
}

@online{___answersblog_2015_may_crashlytics-no1-in-performance,
  url = {https://web.archive.org/web/20160304014842/http://answers.io/blog/answers-named-2-in-mobile-analytics-crashlytics-1-in-performance},
  title = {Answers Named \#2 in Mobile Analytics, Crashlytics \#1 in Performance},
  author = {Wayne Chang},
  year = {2015},
  organization = {{Twitter Inc.}},
  abstract = {
    Before Answers, developers had to wade through mountains of data about their apps to find what they were looking for. We wanted to fix this, so we went to the drawing board and set out to build a mobile analytics solution you didn’t need to analyze.

    Today, just ten months after our launch, we’re proud to announce that SourceDNA, the world's largest database of mobile app intelligence, reported Answers is ranked as #2 on iOS and #3 on Android in the mobile analytics space.
    
    We’re also excited to see that SourceDNA named Crashlytics as the number one most implemented performance SDK -- with a higher score than #2-6 combined.
  }
}

@online{___answersblog_2015_june_update,
  title = {Answers June update: behind the curtain},
  url = {https://web.archive.org/web/20160414070440/https://answers.io/blog/answers-june-update-behind-the-curtain},
  author = {Brian Swift},
  year = {2015},
  organization = {{Twitter Inc.}},
  abstract = {
    While we’ve been heads down working on some major upgrades for Answers, we thought it would be the perfect time to look back on how far our team has come. Answers is about to celebrate its first birthday, and in the past year, we’ve reached some incredible milestones and learned a lot about what you need in your analytics solution.

    In early 2014, the same team that built Crashlytics worked on a “Hackweek” project focused on what an analytics product would look like if we decided to build it. With the support of Twitter’s executive team, we formed a small team of six and built what was then called “Insights by Crashlytics.” But 48 hours before “Insights” was slated to launch, we decided to change the name to what you know today as Answers.

    The rest of the journey has been incredible.

    We went from processing 50 billion sessions a month to over five billion sessions every day. In May, Answers was ranked the #2 mobile analytics solution on iOS, and #3 on Android. But that just scratches the surface of where we’ve come from, and where we’re heading. We’re humbled by the support and community that has been built around Answers, and we’re thrilled for the future.
  },
}

@online{burke2014_wayne_chang_interview,
  title = {Wayne Chang, Crashlytics co-founder and Twitter developer lead},
  url = {https://www.siliconrepublic.com/play/the-interview-wayne-chang-crashlytics-co-founder-and-now-twitter-developer-lead},
  author = {Elaine Burke},
  year = {2014},
  publisher = {{Silicon Republic}},
  organization = {{Silicon Republic}},
  quotes = {
    The product was built for developers to let them know what caused their apps to crash, down to the line of code.
    Chang was heavily involved in the creation of Twitter Fabric and, for him, developers will always be at the heart of his concerns. One feature he highlighted was the way in which Twitter Fabric uses Crashlytics data in a way that saves developers from information overload or “analysis paralysis”.
  }
}

@online{catrobat_first_steps_into,
  title = {First steps into Catrobat},
  url = {https://developer.catrobat.org/first_steps},
  year = {2015},
  organization = {{Catrobat}},
  author = {{The Catrobat Project}},
  urldate = {2021-04-28},
}

@online{catrobat_project,
  title = {Home - Catrobat},
  url = {https://catrobat.org/},
  year = {2021},
  urldate = {2021-04-28},
  organization = {Catrobat},
  author = {Catrobat},
}

@online{cfdr_usenix,
  title = {THE COMPUTER FAILURE DATA REPOSITORY (CFDR) - USENIX},
  url = {https://www.usenix.org/cfdr},
  organization = {{USENIX}},
  author = {{USENIX}},
  year = {2021},
  details = {Public datasets from 1996 to 2009, seemingly active from 2006 to 2009},
}

@online{chang2015_how_six_people_built_crashlytics,
  title = {The inside story of Answers: How six people built the number one most popular mobile analytics tool in just a few months},
  url = {https://chang.com/how-six-people-built-the-2-mobile-analytics-tool-in-just-a-few-months-full-article/index.html},
  author = {Wayne Chang},
  organization = {{Crashlytics}},
  year = {2015},
  note = {Retrieved 2020-Dec-23},
}

@online{ebling2018_so_s9_specific_webview_device_crash_report,
  title = {S9/S9+ specific WebView device crash report},
  url = {https://stackoverflow.com/questions/49645746/s9-s9-specific-webview-device-crash-report},
  year = {2018},
  author = {Andrew Ebling},
  organization = {Stack Exchange},
  publisher = {Stack Overflow},  
}

@online{exodus_privacy_project,
   title = {exodus},
   key = {exodus privacy project},
   url = {https://reports.exodus-privacy.eu.org/en/},
   organization = {{Exodus Privacy}},
   urldate = {2020-09-09},
   year = {2020},
   abstract = {Exodus Privacy is a non-profit organization led by hacktivists. Its purpose is to help people get a better understanding of the Android applications tracking issues.},
   note = {Retrieved 2020-Sep-09}
}

@online{github_catroid,
  title = {Catrobat/Catroid: Writing programs on an Android device without prior knowledge.},
  url = {https://github.com/Catrobat/Catroid},
  year = {2021},
  author = {{Catrobat Team}},
  organization = {{Catrobat}},
  journal = {GitHub repository},
  urldate = {2021-04-28},
}

@online{iteratively_homepage,
  title = {Iteratively - Capture customer data you trust},
  url = {https://iterative.ly/},
  key = {Iteratively},
  year = {2020},
  urldate = {2020-09-11},
  organization = {{Iteratively, Inc}},
}

@online{kersten2018_the_age_of_software_needs_value_stream_architects,
  title = {The Age of Software Needs Value Stream Architects},
  author = {Mik Kersten},
  year= {2018},
  urldate = {2022-05-16},
  publisher = {TheNewStack},
  url = {https://thenewstack.io/the-age-of-software-needs-value-stream-architects/},
  thoughts = {
    There are germs of various key ideas in his work on this topic such as the concept of engineering and optimising for delivering value [streams] through architecting systems that do so where those systems include fast, useful, clear feedback. This article probably wasn't peer-reviewed and probably isn't aimed at a research audience.
    
    "...own and architect the software value streams which accelerate the flow of business value to customers?"
    
    "...a holistic view of the value stream of a product from portfolio/feature inception to delivery to customer feedback."
    
    "...continually streamline and increase the velocity of deliverables across the organization is crucial to enterprise adaptability and success."
    
    The article doesn't define the role of a value stream architect (VSA) adequately (it does describe the role of product owner in the context of a VSA).
    
    "Through the collection and analysis of feedback data, the mechanism for driving change should take the form of the Architect adding work items to each of the product backlogs. These items are continuous improvement ideas that the Architect has identified through working with the team, based on the current backlog and the expected business result metrics, and utilizing value stream metrics and feedback in retrospectives." There seems to be the potential for a mutually beneficial symbiotic relationship between using usage-derived analytics (of various forms including crash reporting) to feed the VSA and for the VSA to use the information derived from the usage-derived analytics to help the developers improve their practices and their products.
    
    The flow framework, illustrated in 
    https://thenewstack.io/3-trends-in-tracking-software-delivery/ , includes 4 value stream metrics: features, defects, risks, and debts under the topic of flow distribution. PS: That article might help support my PhD claims on the merits of using usage-derived mobile analytics. 
  }
}

@online{kim2017_what_information_does_crashlytics_collect_from_end_users,
  title = {What information does Crashlytics collect from End Users?},
  url = {https://stackoverflow.com/questions/43610494/what-information-does-crashlytics-collect-from-end-users},
  author = {Michael Kim and Mike Bonnell and Jaber Shabeek},
  year = {2017},
  organization = {Stack Exchange},
  publisher = {Stack Overflow},
  abstract = {
    I'm using Crashlytics in fabric. I read privacy-policy.pdf to check privacy issue. I found stored information includes device state information, unique device identifiers and so on. I'd like to know what exact information is gathered for device state information and unique device identifier.

    Thanks & Regards, Michael Kim.
  }
}

@online{kiwix_about_the_project,
  title = {About Kiwix},
  url = {https://www.kiwix.org/en/about/},
  year = {2021},
  organization = {{Kiwix Association}},
  author = {Kiwix},
  urldate = {2021-05-28},
}

@misc{github_kiwix_android,
  title = {Kiwix for Android},
  url = {https://github.com/kiwix/kiwix-android},
  year = {2021},
  author = {Various contributors},
  journal = {GitHub repository},
  organization = {{GitHub, Inc.}},
}

@online{kiwix_release_2_5_0,
    title = {Release 2.5.0},
    url = {https://github.com/kiwix/kiwix-android/releases/tag/2.5.0},
    abstract = {},
    author = {Mac Gillicuddy, Seán},
    organization = {{The Kiwix Project}},
    journal = {GitHub repository},
    urldate = {2019-06-17},
    year = {2019}
}

@online{kiwix_release_2_5_3,
    title = {Release 2.5.3},
    url = {https://github.com/kiwix/kiwix-android/releases/tag/2.5.3},
    abstract = {},
    author = {Mac Gillicuddy, Seán},
    organization = {{The Kiwix Project}},
    journal = {GitHub repository},
    urldate = {2019-08-20},
    year = {2019}
}

@online{knuth_trutex,
  title = {An Example of Donald Knuth's Reward Check},
  url = {http://www.truetex.com/knuthchk.htm},
  author_with_PhD = {Richard J. Kinch {Phd}},
  author = {Richard J. Kinch},
  year = {1999},
  organization = {{TrueTeX}},
  key = {Kinch},
}

@online{littledata2020_google_analytics_doesnt_match_shopify,
  title = {Why your Google Analytics data doesn't match your Shopify data},
  url = {https://drive.google.com/file/d/1VTxaih8TVZ9V9hLekKVEgmHP55JdeoB4/view},
  year = {2020},
  key = {Littledata},
  organization = {Littledata},
  sections = {
    1. Top 6 reasons for innacuracy[sic],
    2. How a data mismatch damages your bottom line,
    3. Comparing different tracking methods,
  },
  note = {The guide appears to be incomplete.},
  urldate = {2020-12-28},
}

@online{noyb2020_noyb_files_complaint_against_apples_tracking_code_idfa,
  title = {noyb files complaints against Apple's tracking code "IDFA"},
  url = {https://noyb.eu/en/noyb-files-complaints-against-apples-tracking-code-idfa},
  key = {nyob},
  organization = {{NOYB – European Center for Digital Rights}},
  year = {2020},
}

@online{r_date_conversion_article,
  title = {Easily Converting Strings to Times and Dates in R with flipTime},
  url = {https://www.displayr.com/r-date-conversion/},
  author = {Matthew McLean},
  organization = {{Displayr, Inc}},
  year = {2017},
}

https://github.com/Displayr/flipTime/graphs/contributors 
@online{r_date_conversion_github,
  title = {flipTime - Tools for manipulating and presenting time series data},
  url = {https://github.com/Displayr/flipTime},
  author = {Various},
  organization = {{Displayr, Inc}},
  journal = {GitHub repository},
  year = {2017},
}

@online{r_bloggers_date_formats_in_r,
  title = {Date Formats in R},
  url = {https://www.r-bloggers.com/date-formats-in-r/},
  year = {2013},
  author = {Mollie Taylor},
  organization = {R-bloggers},
}

@online{not_cited_yet_segment2019_the_tools_of_today_arent_the_tools_of_tomorrow,
  title = {The tools of today aren’t the tools of tomorrow},
  author = {Calvin French-Owen},
  year = {2019},
  organization = {Twilio Segment},
  url = {https://segment.com/blog/the-tools-of-today-arent-the-tools-of-tomorrow/},
  thoughts = {
    They present lots of interesting examples of how organisations change their use of various analytics tools. Sadly it doesn't focus on mobile apps, or on developer-oriented analytics, nonetheless it provides useful background context for the world of poly-use of analytics offerings.
  }
}

@online{r_bloggers_using_colclasses,
  title = {Using {colClasses} to Load Data More Quickly in {R}},
  url = {https://www.r-bloggers.com/using-colclasses-to-load-data-more-quickly-in-r/},
  year = {2013},
  author = {Mollie Taylor},
  organization = {R-bloggers},
}

@online{firebasesupport2020_dependencies_of_firebase_sdks_on_google_play_services,
  title = {Dependencies of Firebase Android SDKs on Google Play services},
  url = {https://firebase.google.com/docs/android/android-play-services},
  year = {2020},
  author = {{Google Developers}},
  organization = {{Google Inc.}},
  abstract = {Some Firebase Android SDKs depend on Google Play services, which means they will only run on devices and emulators with Google Play services installed. These Firebase SDKs communicate with the Google Play services background service on the device to provide a secure, up-to-date, and lightweight API to your app. Certain Android devices, such as Amazon Kindle Fire devices or those sold in some regions, do not have Google Play services installed.},
  note = {Updated 2020-12-16 UTC},
}

@online{firebasesupport2022_crashlytics_data_retention_policy,
  url = {https://firebase.google.com/support/privacy},
  title = {Privacy and Security in Firebase},
  year = {2022},
  author = {{Google Developers}},
  organization = {{Google Inc.}},
  extracts = {
    Crashlytics Installation UUIDs, Crash traces, Breakpad minidump formatted data (NDK crashes only)
    How it helps: Firebase Crashlytics uses crash stack traces to associate crashes with a project, send email alerts to project members and display them in the Firebase Console, and help Firebase customers debug crashes. It uses Crashlytics Installation UUIDs to measure the number of users impacted by a crash and minidump data to process NDK crashes. The minidump data is stored while the crash session is being processed and then discarded. Refer to Examples of stored device information for more detail on the types of user information gathered.
    Retention: Firebase Crashlytics retains crash stack traces, extracted minidump data, and associated identifiers (including Crashlytics Installation UUIDs) for 90 days.
    Note: Firebase Crashlytics stores minidump data only temporarily in order to process NDK crashes.
  }
}

@online{fowler_datensparsamkeit_2013,
	title = {Datensparsamkeit},
	url = {https://www.martinfowler.com/bliki/Datensparsamkeit.html},
	abstract = {Datensparsamkeit is a German word that's difficult to translate properly into English. It's an attitude to how we capture and store data, saying that we should only handle data that we really need.},
	author = {Fowler, Martin},
	urldate = {2019-05-03},
	date = {2013-12-12},
	year = {2013},
	file = {Datensparsamkeit:/Users/julianharty/Zotero/storage/NZZBCYCZ/Datensparsamkeit.html:text/html}
}

@online{google_account_help_android_share_data_2019,
    title = {Share usage \& diagnostics information with Google},
    url = {https://support.google.com/accounts/answer/6078260},
    abstract = {To help us improve Android, you can let your device send us information about how you use it and how it’s working.},
    author = {Google},
    urldate = {2019-06-14},
    date = {2019},
    year = {2019}
}

@misc{harty_aymer_playbook_website,
	title = {The {Mobile} {Analytics} {Playbook} ({Website})},
	url = {http://www.themobileanalyticsplaybook.com/},
	urldate = {2017-10-03},
	author = {Harty, Julian and Aymer, Antoine},
	file = {The Mobile Analytics Playbook:/Users/julianharty/Library/Application Support/Zotero/Profiles/4slbw694.default/zotero/storage/3T9BHTRQ/www.themobileanalyticsplaybook.com.html:text/html},
}

@online{harty_beaufort_scale_2018,
    title = {Beaufort Scale of Testing Software},
    url = {http://blog.bettersoftwaretesting.com/2018/04/beaufort-scale-of-testing-software/},
    abstract ={},
    author = {Harty, Julian},
    urldate = {2019-06-19},
    date = {2018-04-13},
    year = {2018}
}

@online{hurd2016_answer_to_how_do_i_develop_android_apps_on_kindle_fire,
  title = {How do I develop Android apps on Kindle Fire?},
  author = {Blake Hurd},
  organization = {Amazon Inc.}},
  publisher = {{Qu{ora}},
  year = {2016},
  developing_for_kindle_fire = {
    Amazon’s Fire OS is Amazon’s own version of Android OS. It pretty closely tracks Google’s releases of the OS, so you will find that most Android apps work just fine on it. The main difference is that the Fire OS does not include Google Services app which many of Googles’ service APIs rely on. Instead it includes a variety of Amazon’s libraries and APIs which you get access to. If your app doesn’t depend on Google Service APIs, then you can just submit it to the Amazon App Store; it should work fine on Fire devices. Or you can install it onto your own personal device in the same way you would any Android device.
  },
}

@online{chelo2020_firebase_does_not_collect_age_or_gender_data,
  title = {Firebase does not collect age and gender data},
  url = {https://stackoverflow.com/questions/59969976/firebase-does-not-collect-age-and-gender-data},
  author = {Simone Chelo},
  year = {2020},
  organization = {Stack Exchange},
  publisher = {Stack Overflow},
}

@online{itil_ishikawa_example,
  title = {Annex 6C: Ishikawa Diagrams},
  url = {http://gurri-itil.tripod.com/Service\%20Support/cd/content/ss06_6c.htm},
  organization = {{SAS Institute}},
  author = {Unspecified},
  year = {2000},
  copyright = {Crown Copyright 2000},
}

@online{izzy_android_without_google_microg_2015,
    title = {Android without Google: microG},
    url = {https://android.izzysoft.de/articles/named/android-without-google-5a},
    author = {Rehberg, Andreas Itzchak}, 
    urldate = {2019-06-14},
    date = {2015-10-27},
    year = {2015}
}
% izzy's name found in https://android.izzysoft.de/text/books/images/inoffizielles_android_1.png

@online{joe2016_firebase_analytics_demographics,
  title = {Firebase Analytics demographics},
  url = {https://stackoverflow.com/questions/37401842/firebase-analytics-demographics},
  author = {Joe},
  year = {2012},
  organization = {Stack Exchange},
  publisher = {Stack Overflow},
  abstract = {
    The Firebase Analytics dashboard shows a card for demographics, including age and gender.

    According to https://support.google.com/firebase/answer/6317486?hl=en Firebase should be automatically collecting age and gender. Can anyone explain how it collects that information, and if anything else needs to be done in order to provide it?

    In my dashboard, I'm seeing no age or gender data. I also haven't found any public API in the firebase-analytics SDK that would allow for setting the age or gender of the user.

    If it comes from the advertiser id, does that mean we must also integrate with AdMob in order to get that data -- and if we don't use AdMob, then age and gender are impossible to gather? And does AdMob then also have to be linked with the Firebase project before it can start populating that data?
  }
}

@online{krysmanski2012_so_redirect-stdout-to-logcat-in-android-NDK,
  title = {Redirect stdout to logcat in Android NDK},
  url = {https://stackoverflow.com/questions/10531050/redirect-stdout-to-logcat-in-android-ndk},
  author = {Sebastian Krysmanski},
  year = {2012},
  organization = {Stack Exchange},
  publisher = {Stack Overflow},
}

@online{learner2011_so_how_to_access_anrs_and_tombstones,
  title = {how to access android files /data/anr/traces.txt and /data/tombstones/tombstones},
  url = {https://stackoverflow.com/questions/5467972/how-to-access-android-files-data-anr-traces-txt-and-data-tombstones-tombstones},
  year = {2011},
  author = {learner},
  organization = {Stack Exchange},
  publisher = {Stack Overflow},  
}

@online{mopinion2017_top11_mobile_in_app_feedback_tools,
  title = {Top 11 Best Mobile In-App Feedback Tools: An Overview},
  url = {https://mopinion.com/top-11-best-mobile-in-app-feedback-tools-an-overview/},
  year = {2017},
  author = {Erin Gilliam Haije},
  organization = {mopinion},
  publisher = {mopinion},
}

@online{NHS_organ_donation_in_england,
    title = {Organ donation law in England - NHS Organ Donation},
    url = {https://www.organdonation.nhs.uk/uk-laws/organ-donation-law-in-england/},
    abstract = {From spring 2020, all adults in England will be considered to have agreed to be an organ donor when they die unless they have recorded a decision not to donate or are in one of the excluded groups. This is commonly referred to as an ‘opt out’ system. You may also hear it referred to as 'Max and Keira's Law'.},
    author = {NHS Blood and Transplant},
    organization = {{NHS, UK}},
    urldate = {2019-06-20},
    date = {2019-06-20},
    year = {2019}
}

@online{mcquate_I_saw_you_were_online,
    title={‘I saw you were online’: How online status indicators shape our behavior},
    url={https://www.washington.edu/news/2020/04/13/how-online-status-indicators-shape-our-behavior/},
    author={Sarah McQuate},
    organization = {University of Washington},
    year={2020},
    summary={A summary of User Experiences with Online Status Indicators' written for a general audience.},
    quote_1={Then the researchers asked the participants to time themselves while they located the settings to turn off “appearing online” in each app they used regularly. For the apps that have settings, participants gave up before they found the settings 28\% of the time. For apps that don’t have these settings, such as WhatsApp, participants mistakenly thought they had turned the settings off 23\% of the time.},
    quote_2={“When you put some of these pieces together, you’re seeing that more than a third of the time, people think they’re not broadcasting information that they actually are,” Cobb said. “And then even when they’re told: ‘Please go try and turn this off,’ they’re still not able to find it more than a quarter of the time. Just broadly we’re seeing that people don’t have a lot of control over whether they share this information with their network.”},
    quote_3={},
    quote_4={},
}

@online{play_console_help_view_crashes_2019,
    title = {View crashes \& application not responding (ANR) errors},
    url = {https://support.google.com/googleplay/android-developer/answer/6083203},
    abstract = {Using the Play Console, you can view data for crashes and application not responding (ANR) errors for your apps. Data comes from Android devices whose users have opted in to automatically share their usage and diagnostics data.
    
    Also, if you use pre-launch reports to identify issues with your apps, crashes found during testing are listed with your app’s crashes and ANRs. However, because crashes found while generating a pre-launch report come from test devices, they don’t affect your crash statistics.},
    author = {Google},
    urldate = {2019-06-14},
    date = {2019},
    year = {2019}
}

@online{play_console_help_understanding_pre_launch_reports_2022,
  title = {Understand your pre-launch report},
  url = {https://support.google.com/googleplay/android-developer/answer/9844487},
  year = {2022},
  author = {Google},
  abstract = {
    This article helps you to understand your pre-launch report results by providing an overview of the errors, warnings or issues that your report might uncover. If you want to know how to set up and run a pre-launch report, go to Use a pre-launch report to identify issues.

    When your pre-launch report is available, you can view a test summary that includes the number of errors, warnings and minor issues found during testing, categorised by issue type. You’ll also see a recommendation based on your app’s testing results.

    Note: Though the pre-launch report is a practical and powerful tool that can help you to improve your app, Google can’t guarantee that tests will identify all issues. To make sure that your results are as comprehensive and relevant to you as possible, review and update your pre-launch report settings.
  },
  stability_extract = {
    Each section of the stability tab details issues found during testing, which can include:

    The issue type and icon:
    Red indicates an error
    Yellow indicates a warning
    Green indicates that testing found no issues
    The number of devices the issue was detected on
    The stack trace associated with the issue
    The relevant API (if applicable)
    The number of times the issue was detected during testing (if applicable)
    Next to each issue, you can select Show more for granular details about the issue, such as the device's name, screen size, Android version, RAM, application binary interface (ABI) and locale. You can select each device model to see the device specifications, view a screenshot and video from testing, demo loop output and stack traces (which you can also download). Be advised that the availability of these details may vary.

    Note: Crashes found during testing are also listed on your app’s crashes and ANRs page. Since crashes found while generating a pre-launch report come from test devices, they don’t affect your crash statistics.

    View test devices without issues
    At the bottom of the Stability tab, you can view the Test devices without issues table to see information about tests that did not turn up any issues. 

    On each row, you'll see the name of the testing device, the device's Android version and an icon showing whether your app had any testing issues.
  }
}

@online{play_console_use_a_pre_launch_report_to_identify_issues_2022,
  title = {Use a pre-launch report to identify issues},
  url = {https://support.google.com/googleplay/android-developer/answer/9842757},
  year = {2022},
  author = {Google},
  abstract = {
    This article explains how to set up and run a pre-launch report. If you’ve run a pre-launch report and you want to know how to interpret the results, go to Understand your pre-launch report.

    A pre-launch report is automatically generated when you publish an app to internal, closed or open testing. It helps to identify issues proactively before your app reaches users. It includes tests for: Stability issues, Android compatibility issues, Performance issues, Accessibility issues, Security vulnerabilities, Privacy issues
  },
  five_languages_extract = {
    Step 4: View test reports for specific languages
    If you'd like to view test results for specific languages, you can set up language preferences on the Pre-launch report settings page. You can select up to five languages. 

    Tip: Since the pre-launch report runs automatically when you upload a test app bundle, you can only add language preferences after the initial test completes.  

    Set up language preferences
    Open Play Console.
    Select an app.
    On the left menu, select Testing > Pre-launch report > Settings.
    Under 'Test your app in specific languages', select + Add language.
    Select up to five languages. For future tests, you'll only see test results from these languages.
    Note: If you don’t select any languages, we’ll automatically select languages in which your app has the most installs.
    Click Save
  }
}

@online{google2020_pre-launch-reports-false-positives,
  title = {Could you fix Pre-launch report false-positives triggered by your own code base?},
  url = {https://issuetracker.google.com/issues/160907013},
  authors = {Various contributors},
  authors-x = {{gr...@gmail.com et al}},
  year = {2020},
  publisher = {{Google Android IssueTracker}},
}

@online{popper_crokage_2019,
    title = {CROKAGE: A New Way to Search Stack Overflow},
    url = {https://stackoverflow.blog/2019/08/14/crokage-a-new-way-to-search-stack-overflow/?cb=1},
    abstract = {},
    author = {Popper Ben},
    urldate = {2019-08-29},
    date = {2019-08-14}
}

@online{salomonbrys_github_anr_watchdog,
  title = {SalomonBrys/ANR-WatchDog- A simple watchdog that detects Android ANRs (Application Not Responding).}, 
  url = {https://github.com/SalomonBrys/ANR-WatchDog},
  author = {Salomon Brys},
  year = {2013},
  urldate = {2021-07-21},
  organization = {GitHub},
}

@online{schreckengost2019_extending_elastic_stack_android_adb,
  title = {Extending the Elastic Stack to Fit Our Needs},
  url = {https://crossbrowsertesting.com/blog/development/extending-the-elastic-stack-to-fit-our-needs/},
  author = {Harold Schreckengost},
  urldate = {2020-12-23},
  organization = {{SmartBear CrossBrowserTesting}},
  year = {2019},
  extract = {
    we built at CrossBrowserTesting using libbeat was a custom shipper for Android mobile device logs. We have hundreds of real Android devices and trying to keep track of logging across that many devices poses a unique challenge that the majority of businesses will never have.
    
    With Androidbeat, our in-house data shipper for Android mobile devices, we had several requirements:
    
    It needed to use the standard Android Debug Bridge (adb) functionality built in to Andro
    It needed to handle temporary unavailability of a device gracefully
    It needed to be able to add in metadata about the devices attached, such as our unique identifiers, the severity of the logs, and the time the message was sent.
  }
}

@online{scott2017_android_app_crash_noclassdeffounderror_on_samsung_lollipop_devices,
  title = {Android app crash NoClassDefFoundError on Samsung Lollipop devices},
  url = {https://stackoverflow.com/questions/46814593/android-app-crash-noclassdeffounderror-on-samsung-lollipop-devices},
  author = {Michael Scott},
  year = {2017},
  organization = {Stack Exchange},
  publisher = {Stack Overflow},  
}

@misc{se2012_story_points_for_bug_fixing_tasks_in_scrum,
  title = {Story points for bug fixing tasks: Is it suitable for Scrum?},
  url = {https://softwareengineering.stackexchange.com/questions/162145/story-points-for-bug-fixing-tasks-is-it-suitable-for-scrum},
  author = {palacsint},
  year = {2012},
  organization = {Stack Exchange},
  publisher = {Software Engineering}, 
  question = {
    I'm just wondering if we should assign story points to bug fixing tasks or not. JIRA, our issues-tracking software, does not have story point field for Bug type issues (it's only for Storys and Epics).
    Should we add the Bug issue type to the applicable issue types of the Story Points field? What are the pros and cons? Would it be suitable for Scrum?
  },
}

@misc{so2021_shankar_strange_crash_in_cronetDynamite.apk,
  title = {Strange Crash in CronetDynamite.apk (offset 0x1000) Android},
  author = {Hari Shankar S},
  year = {2021},
  organization = {Stack Exchange},
  publisher = {Stack Overflow},  
}

@online{github2017_k9mail_issue_2705,
  title = {Deleted mail from inbox is doubled in trash directory},
  url = {https://github.com/k9mail/k-9/issues/2705},
  author = {gandogar},
  year = {2017},
  key = {K-9 Mail GitHub},
  organization = {{K-9 Mail}},
  journal = {GitHub repository},
  urldate = {2020-12-23},
  comment = {The issue was opened on 25 Aug 2017 and took until 24 Apr 2019. Multiple people contributed examples.},
}

@online{github2020_k9mail_logging_errors,
  title = {Logging Errors - K-9 Mail},
  url = {https://github.com/k9mail/k-9/wiki/LoggingErrors},
  abstract = {K-9 Mail has controllable debug logging. Users can activate logging to help diagnosing problems and errors.},
  year = {2020},
  authorx = {Various},
  key = {K-9 Mail GitHub},
  organization = {{K-9 Mail}},
  journal = {GitHub repository},
  urldate = {2020-12-23},
}

@online{github2020_sematext_logsene_android,
  title = {Sematext Logs is ELK as a Service.},
  url = {https://github.com/sematext/sematext-logsene-android},
  authorx = {Various},
  key = {Sematext GitHub},
  year = {2020},
  urldate = {2020-12-23},
  organization = {{Sematext}},
  journal = {GitHub repository},
  abstract = {
    Sematext Logs is ELK as a Service. This library lets you collect mobile analytics and log data from your Android applications using Sematext. There is an equivalent library for shipping logs from iOS available... Use the Mobile Application Logs Integration to get out-of-the-box reports with the most important information about your mobile applications.}
}

@online{github2021_searching_code_github_docs,
  title = {Searching code - GitHub Docs},
  abstract = {You can search for code on GitHub and narrow the results using these code search qualifiers in any combination.},
  url = {https://docs.github.com/en/search-github/searching-on-github/searching-code},
  organization = {GitHub Inc.},
  year = {2021},
  author = {{GitHub Inc.}},
}

@misc{rcdailey2018_ndk_redirect_to_logcats,
  title = {How to redirect stdout and stderr to android logcats?},
  url = {https://github.com/android/ndk/issues/671},
  journal = {GitHub repository},
  author = {Robert Dailey},
  year = {2018},
}

@misc{reddy2022_crashlytics_fails_to_track_app_startup_crashes,
  title = {Crashlytics fails to track app startup crashes},
  url = {https://github.com/firebase/firebase-android-sdk/issues/3467},
  year = {2022},
  author = {Vibin reddy and argzdev},
  publisher = {{Github Inc.}},
}

@misc{rugg2015_beyond_the_80_20_principle,
  title = {Beyond the 80:20 Principle},
  url = {https://hydeandrugg.wordpress.com/2015/09/12/beyond-the-8020-principle/},
  author = {Gordon Rugg and Jennifer Skillen and Colin Rigby},
  year = {2015},
}

@online{safedk2015_in_app_protection_on_youtube,
  title = {SafeDK - In App Protection},
  url = {https://youtu.be/tGwAXEouy80},
  organization = {{SafeDK}},
  year = {2015},
  urldate = {2020-05-26},
  author = {{SafeDK}},
}

@online{techcrunch2015_quettra_mobile_analytics_acquired,
  title = {SimilarWeb Buys Quettra To Move Deeper Into Mobile Analytics And Big Data},
  url = {https://techcrunch.com/2015/12/10/similarweb-buys-quettra-to-move-deeper-into-mobile-analytics-and-big-data/},
  year = {2015},
  author = {Ingrid Lunden},
  organization = {TechCrunch},
  quotes = {
    "Yet more consolidation is happening in the world of analytics. Today Israeli web analytics and traffic measurement startup SimilarWeb announced that it has acquired Quettra, a provider of mobile analytics and measurement tools founded by Ankit Jain, the former head of search and discovery in the Google Play store.",
    
  },
}

@online{timber_io_homepage_2020,
  title = {Log Better. Solve Problems Faster.},
  url = {https://timber.io/},
  urldate = {2020-08-18},
  author = {{Timber Technologies, Inc.}},
  abstract = {Timber is a new kind of cloud-based logging system designed for applications and developers. Spend less time debugging and more time shipping.},
  quote_0 = {Structured logging meets readability. Developers read their logs. Timber's thoughtful presentation gives you the power of structured logs without sacrificing readability.},
  quote_1 = {Do more with your logs. Structured data, events, context, timings, metrics? We've got you covered.},
  quote_2 = {User Context. Always know which user generated which log with automatic user context.},
  quote_3 = {Tail a user},
}

@misc{techyourchance2021_contentprovider_in_android_libraries_considered_harmful,
  title = {ContentProvider in Android Libraries Considered Harmful},
  url = {https://www.techyourchance.com/contentprovider-in-android-libraries-considered-harmful/},
  year = {2021},
  author = {Vasiliy Zukanov},
  organization = {{TechYourChance}},
}

@misc{tsiombikas2014_native_NDK_stdio_to_android_log,
  title = {How to use standard output streams for logging in android apps},
  url = {https://codelab.wordpress.com/2014/11/03/how-to-use-standard-output-streams-for-logging-in-android-apps/},
  author = {John Tsiombikas},
  year = {2014},
}

@online{twitter2015_edsolovey_handling_5B_sessions_a_day_in_real_time,
	title = {Handling five billion sessions a day – in real time},
	url = {https://blog.twitter.com/engineering/en_us/a/2015/handling-five-billion-sessions-a-day-in-real-time.html},
	urldate = {2017-10-16},
	author = {@edsolovey},
	year = {2015},
	organization = {{Twitter}},
	file = {Handling five billion sessions a day – in real time:/Users/julianharty/Library/Application Support/Zotero/Profiles/4slbw694.default/zotero/storage/DXUH8T5P/handling-five-billion-sessions-a-day-in-real-time.html:text/html}
}

@online{twitterdev2015_a_deep_dive_into_the_answers_backend,
  title = {Twitter Flight 2015 - A Deep Dive into the Answers Backend by Ed Solovey},
  url = {https://youtu.be/G1LrMXR3Zko},
  organization = {{Twitter}},
  year = {2015},
  urldate = {2020-12-27},
  author = {{Twitter Developers}},
  abstract = {Answers is a real-time, opinionated mobile analytics product. In the 15 months since its public launch, Answers has grown to be one of the most widely adopted mobile analytics SDK's. Today it links over a million events per second, and every day processes more than 6 billion mobile app sessions across tens of thousands of mobile apps. This talk will briefly introduce the product and go over some of the back-end architecture design that helps us scale it to these volumes: stream/batch processing, probabilistic data structures, intelligent event grouping during stream processing, tailored key-value schemas, and sampling.},
}

@online{zipternet_github,
  title = {Zipternet GitHub project},
  url = {https://github.com/ISNIT0/AndroidCrashDummy/blob/9c6d85b633568fcf64a78fc55e810ae9f5e864b4/app/src/main/java/com/example/user/androidtestapp/MainActivity.java},
  journal = {GitHub repository},
  year = {2019},
  note = {Last checked on 2019-10-12}
}

@misc{android_webview_app_2022,
  title = {Android System WebView},
  url = {https://play.google.com/store/apps/details?id=com.google.android.webview},
  author = {{Google Inc.}},
  year = {2022},
  note = {Last visited: \nth{2} Feb 2022},
  extract = {
    Android WebView is a system component powered by Chrome that allows Android apps to display web content. This component is pre-installed on your device and should be kept up to date to ensure you have the latest security updates and other bug fixes.
  }
}

@misc{bbc_iplayer_app_april_2021_webview_information,
  title = {BBC iPlayer - Apps on Google Play [cached 05 Apr 2021]},
  url = {https://play.google.com/store/apps/details?id=bbc.iplayer.android&start=100&hl=en_IE&gl=US},
  year = {2021},
  author = {{BBC}},
  extract = {BBC iPlayer
    Media Applications Technologies for the BBC Entertainment
    TeenTeen
    126,994
    Add to wishlist
    Screenshot Image Screenshot Image Screenshot Image Screenshot Image Screenshot Image Screenshot Image Screenshot Image Screenshot Image Screenshot Image Screenshot Image Screenshot Image Screenshot Image Screenshot Image Screenshot Image Screenshot Image Screenshot Image Screenshot Image Screenshot Image
    If you are experiencing a problem when opening or using the BBC iPlayer app, updating the Android System WebView via Google Play should now resolve the issue.
    1. Navigate to Play Store app
    2. Search for Android System WebView
    (https://play.google.com/store/apps/details?id=com.google.android.webview)
    3. Select the "Update" option

    BBC iPlayer brings you the latest and greatest TV series and box sets from the BBC. Watch live, on-demand or download to take away with you - all in one app!
  },
}

@misc{bbcnews2021_google_fixes_crashing_android_app_issues,
  title = {Google fixes crashing Android app issues},
  url = {https://www.bbc.co.uk/news/technology-56496783},
  author = {{BBC News}},
  year = {2021},
  article = {
    Google has fixed a problem that meant Android phone apps were crashing for many users.
    The issues began on Monday and affected apps such as Gmail, Facebook and Amazon.
    It appears that an update to the Android System WebView, which allows Android apps to display web content, was to blame.

    Google told users to update their Android System WebView and Google's Chrome browser.

    "We have resolved the issue with WebView that caused some apps on Android to crash for some users. Updating Android System WebView and Google Chrome via Google Play should now resolve the issue," a Google spokesman told the BBC.

    Social media was filled with reports of apps crashing, and DownDetector, a website that measures outages, showed a surge in problems for Gmail and Amazon. Android WebView is a system component that is pre-installed on all Android devices.

    To update it, users need to: navigate to the Play Store app, search for Android System WebView, select the "Update" option, repeat these steps for Google Chrome
  }
}

@misc{bbcnews2020_nhs_covid_19_app_bluescreen_glitch,
  title = {NHS Covid-19 app suffers 'blue screen' glitch},
  url = {https://www.bbc.com/news/technology-54958785},
  author = {{BBC News}},
  publisher = {{BBC News}},
  year = {2020},
  article = {
    The NHS Covid-19 app has stopped working for many iPhone owners, who are unable to get it to launch. Users report being stuck at a blue loading screen with the contact-tracing app's logo - but nothing else happens.
    
    The NHS has published a workaround for the problem in its help files, but has not said what caused the problem or when it will be fixed. Apple does not believe the problem is at its end, since it has not seen the issue arise in other countries' apps.
    
    Many different nations use the same underlying technology, which is designed by Apple and Google, to notify users if they were recently near to someone who subsequently tested positive for the virus.

    Some users have deleted and reinstalled the app to fix the fault, but that deletes useful information - this includes a log of venues the user has checked into via QR barcode scans.

    The NHS's workaround instead asks users to reset their iPhone's location and privacy settings. It also recommends users have the most up-to-date version of Apple's iOS operating system downloaded and installed.

    But carrying out the reset prevents all apps on the handset from using the device's location until they are granted permission again.

    NHS Covid app updated to 'fix' phantom messages
    Contact-tracing app not sharing data with police

    Some users have said they fixed the problem by force-quitting the app - which can be done by flicking the frozen screen up and off the display - and then re-launching it.

    The problem first emerged last week, but complaints became more frequent over the weekend and into Monday. The cause, however, remains unclear. In a statement, the Department of Health and Social Care said it was aware of the issue. "The app is still scanning, even if the screen appears blue," it said. "There are simple steps iPhone users can take to resolve this issue, which are set out on the app's website, and work is underway to identify the cause. "Users experiencing this issue should make sure their Apple iOS is updated to the latest version of the software." 
  }
}

@misc{bbcnews2022_google_moves_to_make_android_apps_more_private,  
    title = {Google moves to make Android apps more private},
    url = {https://www.bbc.co.uk/news/technology-60403963},
    year = {2022},
    author = {Jane Wakefield},
    publisher = {{BBC News}},
    topics = {    
        discusses Google's Privacy Sandbox in Chrome, and their failed Federated Learning of Cohorts (Floc) , Flurry suggesting US consumers opt-out 96\% of the time according to Apple, and Apple's restrictions on app developers who now need permission from users to access the Identifier for Advertisers.
        NB: around June 2022 a story emerged on how hackers are able to bypass 2FA, etc. within the browser to obtain login credentials. 
    }
}

@misc{bbcnews2022_google_signup_fasttrack_to_surveillance,
  title = {Google sign-up a 'fast track to surveillance', consumer groups say},
  url = {https://www.bbc.co.uk/news/technology-61980233},
  year = {2022},
  author = {{BBC News}},
  publisher = {{BBC News}},  
  topics = {
    Privacy by default should be the default, and a one-click turn all settings off. Web and app activity one of the settings enabled by default. 
  }
}

@misc{beuc2022_google_surveillance_action,
  title = {European consumer groups take action against Google for pushing users towards its surveillance system},
  url = {https://www.beuc.eu/publications/european-consumer-groups-take-action-against-google-pushing-users-towards-its/html},
  year = {2022},
  publisher = {{BEUC}},
  author = {Sébastien Pant},
  claims = {
    - Google is using deceptive design, unclear language and misleading choices when consumers sign up to a Google account to encourage more extensive and invasive data processing.
    - Contrary to its claims, the tech giant is thwarting consumers who want to better protect their privacy.
    - Consumer groups are taking action on suspected breaches of the GDPR.
  }
}

@misc{bischoff2020_firebase_missconfiguration,
  title = {Estimated 24,000 Android apps expose user data through Firebase blunders},
  author = {Bischoff, Paul},
  url = {https://www.comparitech.com/blog/information-security/firebase-misconfiguration-report/},
  year = {2020},
  publisher = {{Comparitech}},
}

@misc{crittercism2015_homepage,
  title = {Mobile Application Intelligence - Crittercism},
  url = {https://web.archive.org/web/20150908160428/https://www.crittercism.com/},
  year = {2015},
  author = {{Crittercism, Inc.}},
  claims = {
    Let Crittercism give you insight to success. We monitor billions of digital customer experiences each day. Deliver great functionality so users stay engaged with your app.


    Protect your mobile revenue. Monitor how and when your app stops working so you can resolve problems before they cost you users.
    Keep your developer’s life simple. With just one line of code, your team can organize and prioritize errors for a stronger plan of attack.
    Diagnose critical issues, see which devices and operating systems cause trouble, and prioritize bug fixes for the biggest business impact.
    Crittercism takes less than five minutes to install, runs in the background, and comes with world-class support (just in case).
    
    Don’t let small issues become big problems. Using a complete set of tools, get real-time data that shows where and how errors occur so you can resolve issues quickly and keep your app running seamlessly.

    Transaction Monitoring. Monitor critical mobile workflows such as login, checkout, and barcode scan.Track revenue at risk, success rates, and slow transactions that could lead to abandonment.
    
    Crash & Error Monitoring. See all mobile app crashes (and gracefully handled exceptions) in real time from the end users perspective so you’re in the know, not the last to know.
    
    3rd Party API Monitoring. Poorly performing APIs take a toll on your business revenue. Get latencies, error rates, data bandwidth and the number of API requests so you can troubleshoot.
  }
}

@misc{dave2020_reuters_firebase_squeeze,
  title = {Google critics see its {Firebase} tools as another squeeze play},
  author = {Dave, Pavesh},
  publisher = {Reuters},
  year = {2020},
  url = {https://uk.reuters.com/article/us-google-antitrust-focus/google-critics-see-its-firebase-tools-as-another-squeeze-play-idUKKBN2161GA},
}

@misc{desai2022_amazon_codewhisperer,
  title = {Introducing Amazon CodeWhisperer, the ML-powered coding companion},
  url = {https://aws.amazon.com/blogs/machine-learning/introducing-amazon-codewhisperer-the-ml-powered-coding-companion/},
  author = {Ankur Desai and Atul Deo},
  year = {2022},
  publisher = {{AWS}},
  abstract = {
    We are excited to announce Amazon CodeWhisperer, a machine learning (ML)-powered service that helps improve developer productivity by providing code recommendations based on developers’ natural comments and prior code. With CodeWhisperer, developers can simply write a comment that outlines a specific task in plain English, such as “upload a file to S3.” Based on this, CodeWhisperer automatically determines which cloud services and public libraries are best suited for the specified task, builds the specific code on the fly, and recommends the generated code snippets directly in the IDE.

    Although the cloud has democratized application development by giving on-demand access to compute, storage, database, analytics, and ML, the traditional process of building software applications still requires developers to spend a lot of time writing boilerplate sections of code that aren’t directly related to the core problem that they’re trying to solve. Even the most experienced developers find it difficult to keep up with multiple programming languages, frameworks, and software libraries, while ensuring that they’re following the correct programming syntax and best coding practices. As a result, developers spend a significant amount of time searching and customizing code snippets from the web. With CodeWhisperer, developers can stay focused in the IDE and take advantage of real-time contextual recommendations, which are already customized and ready to use. Fewer distractions away from the IDE and ready-to-use, real-time recommendations help you finish your coding tasks faster and provide a productivity boost.

    In this post, we discuss the benefits of CodeWhisperer and how to get started.
  }
}

@misc{ditlea1999_knuth_rewriting_the_bible_in_0s_and_1s,
  title = {Rewriting the Bible in 0's and 1's},
  url = {https://web.archive.org/web/20181109034030/https://www.technologyreview.com/s/400456/rewriting-the-bible-in-0s-and-1s/},
  year = {1999},
  publisher = {MIT Technology Review},
  author = {Steve Ditlea},
  note = {Internet Archive copy, retrieved \nth{22} Jun 2022},
}


@misc{google_play_developer_policy_center,
  title = {Print view: Google Play Developer Policy Center},
  url = {https://play.google.com/about/developer-content-policy-print/},
  year = {2019},
  author = {{Google}},
}

@misc{graham_measuring_2009,
	title = {Measuring the effectiveness of testing using {DDP}},
	pages = {22},
	journaltitle = {case studies},
	author = {Graham, Dorothy},
	date = {2009},
	year = {2009},
	langid = {english},
	file = {Graham - 2009 - Measuring the effectiveness of testing using DDP.pdf:/Users/julianharty/Zotero/storage/3J7ZYZ9B/Graham - 2009 - Measuring the effectiveness of testing using DDP.pdf:application/pdf}
}

@misc{martinez2019_google_just_terminated_our_startup_google_play_publisher_account_on_xmas_day, 
  title={Google just terminated our start-up Google Play Publisher Account on Christmas day}, url={https://android.jlelse.eu/google-just-terminated-our-start-up-google-play-publisher-account-on-christmas-day-5cb69a454da0}, 
  abstractNote={An open letter from an Android developer to the Android Community and specially to Purnima Kochikar, director of Google Play, Apps & Games…}, 
  publisher = {{Medium Inc.}}, 
  author={Martínez, Pablo A.}, 
  year={2019}, 
  month={Jul}
}

@misc{marcher2021_how_google_terminated-a-developer,
    title = {How Google Play "terminated" a developer},
    url = {https://diegomarcher.medium.com/how-google-play-terminated-a-developer-432c23cbe486},
    author = {Diego Marcher},
    publisher = {{Medium Inc.}},
    year = {2021},
}

@misc{mea2019_google_just_deleted_my_nearly_10_year_old_app_etc,
  title = {Google just deleted my nearly 10-year-old free \& open-source Android app},
  url = {https://medium.com/@mmathieum/google-just-deleted-my-nearly-10-year-old-free-open-source-android-app-7fbc52edc50a},
  author = {Mathieu Méa},
  year = {2019},
  publisher = {{Medium Inc.}},
  
}

@misc{moneycontrolnews2021_cowin_apps_crash,
  title = {CoWIN, Aarogya Setu apps crash as COVID vaccine registration for 18-44 age group opens},
  url = {https://www.moneycontrol.com/news/coronavirus/cowin-aarogya-setu-apps-crash-as-covid-vaccine-registration-for-18-44-age-group-opens-6826441.html},
  author = {{MoneyControl News}},
  year = {2021},
  article = {
    Several users are reporting issues with the app.
    
    The CoWIN and Aarogya Setu apps crashed shortly after the registration for COVID -19 vaccination of the age groups 18 to 44 opened at 4 pm today. Several users reported they were unable to access these apps to register for vaccination.

    The government recently announced that citizens between the ages of 18 and 44 would be able to register for the COVID-19 vaccination from 04:00 pm (IST) on April 28.
    The issues range from server issues, “Cowin Server is facing issues, please try later”, to timeout errors, “504 Gateway Time-out”. We were also able to confirm the error. Several Twitter users have been critical of the government’s lack of preparedness for the influx of traffic on the app.

    Cowin server has been crashed
    Aarogya setu app working like IRCTC app.
    No reservations on time
    Retweet #CowinApp#CovidIndia#CoWin#cowinregistration#Maharashtra#MaharashtraFightsCorona#mumbai#MumbaiFightsCorona#pune#PuneFightsCorona#UddhavThackeray#cowinregistrationpic.twitter.com/Qupq2cAljd

    — Shashank shukla (@ishukla_sk) April 28, 2021


    #CoWin@narendramodi u cannot run one app, how would run the country? just asking? #CowinApp#cowinregistration#MautKaSaudagarpic.twitter.com/ZOvBQXaS4z

    — Logical_Bhartiya (@LogicalBhartiy2) April 28, 2021


    #cowinregistration#CoWin#NarendraModi
    Expected.. pic.twitter.com/Fulb1q393Q

    — Suresh Raju (@IamSureshNV) April 28, 2021''


    Waiting for OTP#VaccineRegistration pic.twitter.com/01p9EHWBrt

    — Rohan (@rohanreplies) April 28, 2021

    It is worth noting that even if you are able to register for the vaccine, it might take a bit of time before you can actually get it as many states are saying that the Serum Institute of India (SII) has promised them supplies only after May 15. 
  }
}

@misc{winder2020_forbes_on_the_class_action_firebase_analytics,
  author = {Winder, Davey},
  title = {Google Still Tracks App Users When They've Opted Out, Privacy Lawsuit Alleges},
  url = {https://www.forbes.com/sites/daveywinder/2020/07/15/google-still-tracks-app-users-when-theyve-opted-out-privacy-lawsuit-alleges-class-action-firebase-data-tracking/#75b7d5c53864},
  year = {2020},
  note = {Retrieved 16 July 2020},
}


@misc{zeller2012_udacity_software_debugging_course,
  title = {Software Debugging - Automating the Boring Tasks},
  url = {https://www.udacity.com/course/software-debugging--cs259},
  author = {Andreas Zeller and Gundega Dekena},
  year = {2012},
  note = {Visited \nth{14} July 2021},
  publisher = {{Udacity Inc.}},
  source_of_year = {https://www.whyprogramsfail.com/},
}

@misc{zeller2021_tweet_the_devils_guide_to_incremental_research_15_18,
  title = {The Devil's Guide to Incremental Research 15/18},
  author = {Andreas Zeller},
  url = {https://twitter.com/AndreasZeller/status/1382702371474653192?s=20},
  year = {2021},
  abstract = {
    Things to avoid: User studies. Real code. Talking to practitioners. All you'll learn is that your assumptions were all wrong, your approach won't work, and the problem is much hairier than expected. Don't let reality come between you and your publication. (15/18)
  }
  
}


@online{18f_dot_voting,
  title = {Dot voting - 18F methods},
  url = {https://methods.18f.gov/discover/dot-voting/},
  organization = {United States government},
  author = {18F},
  year = {2019},
  source = {https://github.com/18F/methods/blob/c83cf54e90db3541ed85d1334eb2b87e9926ce5b/_methods/dot-voting.md},
  note = {Retrieved 11 November 2020},
}


@online{android_crash_dummy,
  title = {AndroidCrashDummy GitHub project},
  url = {https://github.com/ISNIT0/AndroidCrashDummy},
  year = {2017},
  author = {Joe Reeve and Julian Harty},
  organization = {{Commercetest Limited.}},
  journal = {GitHub repository},
  note = {Last checked on 2021-01-03}    
}

@misc{android_behavior_changes_apps_targeting_api_level_28plus,
  title = {Behavior changes: apps targeting API level 28+},
  url = {https://developer.android.com/about/versions/pie/android-9.0-changes-28},
  year = {2018},
  author={{Android Project}},
  organization ={{Android Developers}},
  note = {Last checked on 2021-09-25},
  extract = {
    Android 9 (API level 28) introduces a number of changes to the Android system. The following behavior changes apply exclusively to apps that are targeting API level 28 or higher. Apps that set targetSdkVersion to API level 28 or higher must modify their apps to support these behaviors properly, where applicable to the app.

    For changes that affect all apps running on Android 9, regardless of which API level they target, see Behavior changes: all apps \url{https://developer.android.com/about/versions/pie/android-9.0-changes-all}.
  }
}

@online{android_dashboard,
  title={Distribution dashboard},
  url={https://developer.android.com/about/dashboards/},
  author={{Android Project}},
  organization ={{Android Developers}},
  date={May 7, 2019},
  year={2019},
  month={May},
  day={7},
  urldate={11-Oct-2019},
  note = {Last checked on 2019-10-12}
}

@online{apk_expansion_files,
  title = {APK Expansion Files | Android Developers},
  url = {https://developer.android.com/google/play/expansion-files},
  author = {{Android Project}},
  organization ={{Android Developers}},
  year = {2021},
  urldate = {05-Jun-2021},
  note = {Last checked on 2021-06-05},
}

@online{android2022_firebase_crash_integration_into_android_studio_electric_eel,
  title = {App Quality Insights from Firebase Crashlytics},
  url = {https://developer.android.com/studio/preview/features#aqi},
  year = {2022},
  note = {accessed 09 Jun 2022},
  author = {{Android Project}},
  organization ={{Android Developers}},  
  abstract = {
    App Quality Insights from Firebase Crashlytics

    Starting with Android Studio Electric Eel, you can see and act on app crash data from Firebase Crashlytics directly in the IDE. This integration pulls stack trace data and crash statistics from Crashlytics into the new App Quality Insights tool window in the IDE, so you don't have to jump back and forth between your browser and the IDE. Development teams can benefit from key capabilities including the following:

    See lines in your code highlighted when they have related Crashlytics event data.
    See the stack trace for top crashes and click on the stack trace to jump to the relevant lines in your code.
    See summary statistics about top crash and non-fatal events, for example grouped by device manufacturer and Android version.
    Filter events by severity, time, and app version.
    Get a browser link that opens the Crashlytics dashboard page with more details about the event.
    With the Android Studio and Crashlytics integration, you can write code and address top crash issues all in the same spot. This enriched development experience helps you stay informed about your app's performance and minimize disruptions for your users. If you encounter any issues with this feature, file a bug. %https://issuetracker.google.com/issues/new?title=%5BAQI%5D%20%3CTitle%3E&cc=adarshf@google.com,makuchaku@google.com,%20jbakermalone@google.com&format=PLAIN&component=192708&type=BUG&priority=P1&severity=S2&hotlistIds=4012235&assignee=vkryachko@google.com&template=840533

    If you're not using Crashlytics yet and would like to learn more about its offerings, see Firebase Crashlytics. https://firebase.google.com/products/crashlytics
  },
}

@online{android_log_assert,
  title = {AndroidLogAssert},
  url = {https://github.com/ISNIT0/AndroidLogAssert},
  year = {2017},
  organization = {{Commercetest Limited}},
  author = {Joe Reeve and Julian Harty},
  journal = {GitHub repository},
  note = {Last checked on 2021-01-03},
}

@online{android-stability-analysis,
  title = {Android Vitals Analysis GitHub project},
  url = {https://github.com/commercetest/android-stability-analysis},
  year = {2019},
  author= {{Commercetest Limited}},
  organization = {{Commercetest Limited}},
  journal = {GitHub repository},
  note = {Last checked on 2019-10-14}, 
}

@online{android_vitals_best_practices,
  title = {Use Android vitals to improve your app's performance, stability, and size},
  url = {https://developer.android.com/distribute/best-practices/develop/android-vitals.html},
  author={{Android Project}},
  organization ={{Android Developers}},
  note = {Last checked on 2020-11-02},
  quote1 = {Apps whose metrics are higher have greater promotability, which raises their ranking in Google Play Store searches. They also are more likely to be eligible for the New & Updated and Editor's Choice collections on Google Play, and to be nominated in the Google Play Awards.},
  year = {2019},
}

@misc{android2010_froyo_highlights_new_developer_services,
  title = {Android 2.2 Platform Highlights - New Developer Services},
  url = {https://web.archive.org/web/20100722094037/https://developer.android.com/sdk/android-2.2-highlights.html#DeveloperServices},
  year = {2010},
  author={{Android Project}},
  organization ={{Android Developers}},
  note = {Source: Wayback Machine: Last checked on 2021-06-17},
}

@misc{android_cronet_library,
  title = {Perform network operations using Cronet},
  url = {https://developer.android.com/guide/topics/connectivity/cronet},
  year = {2021},
  organization = {{Google Inc.}},
  author={{Android Project}},
  abstract = {
    Cronet is the Chromium network stack made available to Android apps as a library. Cronet takes advantage of multiple technologies that reduce the latency and increase the throughput of the network requests that your app needs to work.
    
    The Cronet Library handles the requests of apps used by millions of people on a daily basis, such as YouTube, Google App, Google Photos, and Maps - Navigation & Transit.
  }
  
}

@misc{android_platform_system_crash_reporter,
  title = {[Android] platform/system/crash\_reporter - Git at Google},
  url = {https://android.googlesource.com/platform/system/crash_reporter/},
  author={{Android Project}},
  year = {2016},
  organization = {{Google Inc.}},
  abstract = {
    crash_reporter is a deamon running on the device that saves the call stack of crashing programs. It makes use of the Breakpad library.

    During a build, Breakpad symbol files are generated for all binaries. They are packaged into a zip file when running m dist, so that a developer can upload them to the crash server.

    On a device, if the user has opted in to metrics and crash reporting, a Breakpad minidump is generated when an executable crashes, which is then uploaded to the crash server.

    On the crash server, it compares the minidump's signature to the symbol files that the developer has uploaded, and extracts and symbolizes the stack trace from the minidump.
  }
}

@misc{android_webview_privacy,
  title = {User privacy in WebView reporting | Android Developers},
  url = {https://developer.android.com/guide/webapps/webview-privacy},
  year = {2020},
  author={{Android Project}},
  organization ={{Android Developers}},
  note = {Last checked on 2021-06-17},  
}

@online{apple_app_review_overview,
  title = {App Review},
  url = {https://developer.apple.com/app-store/review/},
  author = {Apple},
  organization = {Apple},
  year = {2020},
  note = {Last checked on 2020-11-02},
  abstract = {We review all apps and app updates submitted to the App Store in an effort to determine whether they are reliable, perform as expected, respect user privacy, and are free of objectionable content. As you plan and build your app, use these guidelines and resources to help your app approval go as smoothly as possible.},
  quote_1 = {Review times may vary by app. On average, 50\% of apps are reviewed in 24 hours and over 90\% are reviewed in 48 hours.}
}

@online{apple_ios_share_diagnostics,
  title = {Share analytics, diagnostics, and usage information with Apple},
  url = {https://support.apple.com/en-vn/HT202100},
  author = {Apple},
  organization = {Apple},
  year = {2017},
  note = {Last checked on 2022-06-09},
  abstract = {
    Apple asks customers to help improve the iOS by occasionally providing analytics, diagnostic, and usage information. Apple collects this information anonymously.
  }
}

@online{chooseanopensourcelicense2020,
  title = {Choose an open source license},
  url = {https://choosealicense.com/},
  author = {Various},
  organization = {{GitHub, Inc.}},
  year = {2020},
  note = {Accessed:~\nth{16} November 2020},
}

@online{codehouse2020_cohort_analysis,
  title = {Cohort Analysis and how to use it in Google Analytics},
  url = {https://www.codehousegroup.com/insight-and-inspiration/digital-strategy/cohort-analysis-and-how-to-use-it-in-google-analytics},
  author = {Amith Singh},
  year = {2020},
  organization = {{Codehouse}},
  urldate = {2020-07-31},
}

@online{datethics2020_workshop,
  title = {{DatEthics 2020} workshop},
  url = {https://mobilehci-2020.datacentricdesign.org},
  author = {Jacky Bourgeois, Aaron Ding, Jered Vroon and Ella Peltonen},
  organization = {ACM},
  address = {Virtual Venue},
  year = {2020},
  urldate = {2020-10-05},
  abstract = {The Internet of Things makes human activity data – what people do, how they move, how they socialise – an abundant resource. However, this rich and intimate perspective on people, which uniquely shape and characterise their behaviours, can have tremendous ethical implication if data is handled irresponsibly. Being personal, contextual and accessible, mobile devices are key facilitators of (ir)responsible collection and use of data. In this workshop, we will use the Future Workshop approach to develop a research agenda towards an ethical data-centric design of intelligent behaviours. As part of this approach, we will (1) criticise the current mechanisms and infrastructure to frame ethical challenges, (2) fantasise on futures which support user and designer values, and (3) implement a research agenda for the MobileHCI community to emphasise the barriers to tackle. The outcomes of this workshop will foster ethical research and inspire the MobileHCI community.}
}

@online{dotmocracy,
  title = {How to Use Dot Voting Effectively},
  url = {https://dotmocracy.org/},
  organization = {Feedback Frames},
  author = {Jason Diceman},
  year = {2020},
  note = {Retrieved 11 November 2020},  
}

@online{firebasecrashlytics2020_customize_crash_reports,
  title = {Customize your Firebase Crashlytics crash reports},
  url = {https://firebase.google.com/docs/crashlytics/customize-crash-reports?platform=android},
  author = {{Google Developers}},
  year = {2020},
  organization = {{Google Inc.}},
}

@misc{gergelyorosz2021_twitter_mobile_app_poll,
  title = {Twitter Poll - how many mobile devs?},
  url = {https://twitter.com/GergelyOrosz/status/1345288831029956610},
  author = {Gergely Orosz},
  abstract = {
    (Native) mobile engineers: how many iOS / Android devs work on the same app at your company? Meaning contributing to the same app's codebase/dependencies. If it's more than 20 engineers, would love to hear the company name in a comment.},
    year = {2021},
    note = {Twitter poll with 818 votes, Jan 2, 2021},
}

@online{googleanalytics2021_the_cohort_analysis_report,
  title = {The Cohort Analysis report},
  url = {https://support.google.com/analytics/answer/6074676?hl=en},
  year = {2021},
  organization = {{Google Inc.}},
  author = {{Google Inc.}},
}

@online{log-captor-github-project,
  title = {LogCaptor},
  author = {Hakky54 and fossabot},
  url = {https://github.com/Hakky54/log-captor},
  journal = {GitHub repository},
  organization = {{GitHub, Inc.}},
  note = {Last checked 2021-09-16},
  year = {2021},
}

@online{mitlicense2020_ongithub,
  title = {MIT License},
  url = {https://choosealicense.com/licenses/mit/},
  author = {{Massachusetts Institute of Technology}},
  organization = {{GitHub, Inc.}},
  year = {2020},
  note = {Accessed:~\nth{16} November 2020},
}

@online{nngroup_dot_voting,
  title = {Dot Voting: A Simple Decision-Making and Prioritizing Technique in UX},
  url = {https://www.nngroup.com/articles/dot-voting/},
  year = {2019},
  summary = {By placing colored dots, participants in UX workshops, activities, or collaborative sessions individually vote on the importance of design ideas, features, usability findings, and anything else that requires prioritization.},
  organization = {Nielsen Norman Group},
  author = {Sarah Gibbons},
}

@online{segment_analytics_for_android_docs,
  title = {Analytics for Android},
  url = {https://segment.com/docs/connections/sources/catalog/libraries/mobile/android/#analytics-for-android},
  year = {2020},
  organization = {{Segment.io inc}},
  author = {{Segment.io}},
  quote_0 = {Analytics-Android saves up to 1000 calls on disk, and these never expire.},
}

@online{segmentio_supporting_6_months_offline,
  title = {Supporting 6 months offline \#701},
  url = {https://github.com/segmentio/analytics-android/issues/701},
  author = {John Hatton and Prayansh Srivastava},
  organization = {{SIL International and Segment.io}},
  journal = {GitHub repository},
  year = {2020},
  details = {Our non-profit reading app is used to support literacy education in developing countries, and the analytics are used to see which books are being read, and how well kids are doing on comprehension questions. We use Segments warehouse service to populate our postgresql DB, then offer public dashboards showing how and where books are being read. So we love that this sdk saves 1000 events while waiting to send them when online. However, that is no longer enough.

  In an upcoming aid project, 1400 tablets containing our app will go out to kids in a super rural part of the Pacific. Every six months, the tablets will be brought together for the purpose of getting new books and collecting analytics. The project needs our app to store up to six months of reading activity; at a minimum of 3 events per tiny book, some kids may blow past the 1000 events. Our event payloads are tiny.

  #1 We are thinking of doing a fork that increases this to 10 * 10,000. Do you see any red flags on that? If you'd rather we submit a PR that makes it configurable, we could do that instead.

  #2 Even during the every-six-months gathering of the tablets, the project would still like to not depend on a rural connection to the internet. They would like to automatically pull the QueueFile off of every machine and store it for later processing. Does that sound feasible? In other words, are there any known barriers to us writing some utility that takes 1400 queue files, then sends each even to Segment, one at a time?}
}

@online{vitals_scraper_github_package,
    title = {Vitals Scraper source code on GitHub},
    url = {https://github.com/commercetest/vitals-scraper},
    author = {Julian Harty and Joseph E. Reeve},
    organization = {{Commercetest Limited.}},
    journal = {GitHub repository},
    year = {2019},
    note = {Last checked on 2019-10-12}
}

@comment{Thanks to https://tex.stackexchange.com/questions/500642/urldate-not-being-considered/500665#500665 for the note field}

@online{vitals_scraper_npm_package,
    title = {Vitals Scraper npm package},
    author = {Julian Harty and Joseph E. Reeve},
    organization = {{Commercetest Limited.}},
    year = {2019},
    url = {https://www.npmjs.com/package/vitals-scraper},
    note = {Last checked on 2019-10-12}
}


@online{google_play_developer_distribution_agreement,
    title = {Google Play Developer Distribution Agreement},
    url = {https://play.google.com/intl/ALL_at/about/developer-distribution-agreement.html},
    author = {{Google Inc.}},
    organization ={{Google Inc.}},
    urldate = {11-Oct-2019},
    date = {15-Apr-2019},
    year = {2019},
}

@online{google_play_download_and_export_monthly_reports,
  title = {Download \& export monthly reports},
  url = {https://support.google.com/googleplay/android-developer/answer/6135870},
  author = {{Google Inc.}},
  organization = {{Google}},
  year = {2020},
}

@online{google_play_share_usage_and_diagnostics_info_with_google,
  title = {Share usage \& diagnostics information with Google},
  url = {https://support.google.com/accounts/answer/6078260},
  author = {{Google Inc.}},
  organization = {{Google}},
  year = {2020},
  quote_1 = {Battery level, How often you use your apps, Quality and length of your network connections (like mobile, Wi-Fi, and Bluetooth)},
  quote_2 = {Turn usage & diagnostics on or off. Important: If you turn off usage and diagnostics, your device can still get essential services, like a new version of Android. Turning off usage and diagnostics won’t affect info that apps might collect.
  
  To choose whether to send usage and diagnostics info to Google:
  1. Open your device's Settings app.
  2. Tap Google And then More More And then Usage & diagnostics.
  3. Turn Usage & diagnostics on or off.
  Tip: If you use a shared device, other user profiles may change this setting.},
  quote_3 = {For example, Google can use usage and diagnostics info to improve: 
  - Battery life: Google can use info about what's using the most battery on your device to help make common features use less battery.
  - Crashing or freezing on devices: Google can use info about when apps crash and freeze on your device to help make the Android operating system more reliable.
  Some aggregated info can help partners, like Android developers, make their apps and products better, too.},
}

@online{nii_shonan_workshop_152,
   title = {Release Engineering for Mobile Applications},
   maintitle = {NII Shonan Meeting 152},
   address = {Shonan Village Center, Japan},
   year = {2019},
   month = {December},
   author = {Shane McIntosh and Yasutaka Kamei and Meiyappan Nagappan},
   url = {https://shonan.nii.ac.jp/seminars/152/},
   organization = {{NII Shonan}},
}

@online{nii_shonan_152_workshop_report,
   title = {Release Engineering for Mobile Applications},
   maintitle = {NII Shonan Meeting Report 152},
   address = {Shonan Village Center, Japan},
   year = {2019},
   month = {December},
   author = {Shane McIntosh and Yasutaka Kamei and Meiyappan Nagappan},
   url = {https://shonan.nii.ac.jp/docs/No.152.pdf},
   organization = {{NII Shonan}},
}

@online{guardiannewspaper_right_to_be_forgotten_articles,
  title = {The Guardian newspaper - right to be forgotten articles},
  url = {https://www.theguardian.com/technology/right-to-be-forgotten},
  author = {{Guardian Newspaper: Various authors}},
  year = {2020},
  publisher = {{Guardian News \& Media Limited}},
  organization = {{The Guardian Newspaper}},
  quote = {About 129 results for Right to be forgotten},
}

@online{Robospice01,
  author = "St\'{e}phane Nicolas",
  year = "2018",
  title = "README for RoboSpice",
  url = "https://github.com/stephanenicolas/robospice/blob/release/README.md",
  month = "jan",
  lastaccessed = "June 12, 2019",
}


@online{using_puppeteer_to_automate_your_google_analytics_testing,
  title = {Using Puppeteer to automate your {Google Analytics} testing},
  url = {https://www.dumkydewilde.nl/2020/04/using-puppeteer-to-automate-your-google-analytics-testing/},
  author = {Dumky de Wilde},
  organization = {Beyond measure},
  year = {2020},
  month = {April},
  urldate = {2020-08-18}
}

@online{using_the_itly_cli_verify_the_instrumentation,
  title = {{Using the Itly CLI}},
  organization = {Iteratively},
  key = {iteratively},
  url = {https://iterative.ly/docs/using-the-itly-cli#step-5-verify-the-instrumentation},
  urldate = {2020-08-19},
  year = {2020},
  abstract = {Itly is Iteratively’s command line app. It works hand-in-hand with the Iteratively web app and enables developers to quickly and correctly instrument tracking code in their apps.},
  quote_1 = {Step 5: Verify the instrumentation#
  To make sure you’re tracking all the right events, and that you’re tracking those events correctly, Itly can lint your source code and warn you about any errors. For example, Itly can tell if you’ve forgotten to track any required events, or if you’re not passing along all required properties.
  The verify command will scan your source code for tracking calls and compare the results to what's expected per your team's tracking plan. Include --update to update your company's tracking plan online and share the latest analytics implementation status with your team. If the command reports all green, you're all good!

  You can configure your CI pipeline to automatically run the verify command at check-in so you never miss another analytics bug again.},
}

@online{using_the_itly_cli_itly_verify,
  title = {itly verify | {Using the itly CLI}},
  organization = {Iteratively},
  key = {iteratively},
  url = {https://iterative.ly/docs/using-the-itly-cli#itly-verify},
  urldate = {2020-08-19},
  year = {2020},
  quote_1 = {itly verify
    Verify (lint) your source code for analytics.
    
    Run this command in the root folder of your project. The command will scan your source files, locate all calls to the Itly tracking library, and let you know which events are being tracked, and which have yet to be instrumented.

    Include --update to update your company's tracking plan online and share the latest analytics implementation status with your team. Your teammates will be able to tell when events were first implemented, the last time they've been detected in the source code, and where exactly in the source code they are tracked.
    }
}

@mastersthesis{nilsson2016_a_recipe_for_responsiveness_for_improving_android_apps_spotify_masters,
  title={A Recipe for Responsiveness: Strategies for Improving Performance in Android Applications},
  author={Nilsson, Elin},
  year={2016},
  school = {Umeå University},
  abstract = {Mobile applications are expected to be fast and responsive to user interaction, despite challenges mobile platforms and devices face in terms of limited computational power, battery, memory, etc. Ensuring that applications are performant is however not trivial, as performance bugs are difficult to detect, fix, and verify. In order for mobile applications and devices to appear perfectly responsive to the user, they need to meet a 60 frames per second frame rate, and keep load times preferably between 0-300 ms. Meeting these expectations means that there is no room for performance bugs, so there is a need for improving and developing better testing tools and strategies in order to help mobile developers improve performance in their applications.

  This thesis investigates strategies for testing and improving performance in Android applications by conducting a literary study, and a case study with the Spotify Android application. Some of the key findings of this thesis include promising results from tools that visualise sources of performance bugs in the user interface of applications, as well as proposed strategies and tools aimed to help developers profile and improve performance in their Android applications.},
  url = {http://urn.kb.se/resolve?urn=urn:nbn:se:umu:diva-125772},
}

@phdthesis{adam2009balancing,
  title={Balancing privacy needs with location sharing in mobile computing},
  author={Adam, Karim Anthony},
  year={2009},
  school={The Open University},
  doi = {10.21954/ou.ro.00004b35},
  oro = {http://oro.open.ac.uk/19253/},
}

@phdthesis{awwad2017_automated_bidi_testing,
  title={Automated Bidirectional Languages Localization Testing for Android Apps Development},
  author={Aiman Mamdouh Ahmad Ayyal Awwad},
  school={{TU Graz}},
  year={2017},
}

@phdthesis{al2019software_engineering_in_the_age_of_app_stores,
  title={Software Engineering in the Age of App Stores: Feature-Based Analyses to Guide Mobile Software Engineers},
  author={Al-Subaihin, Afnan A},
  year={2019},
  school={UCL (University College London)}
}

@phdthesis{didar2018data_analytics_phd_thesis,
  title={Data analytics for decision support in software release management},
  author={Didar Al Alam, SM},
  year={2018},
  publisher={Graduate Studies},
  type = {{Ph.D.} dissertation},
  school={University of Calgary},
  doi={10.11575/PRISM/31856},
  relevance_to_phd={Plan Monitor Improve Decision Making Framework. BTW: supervised by Guenther Ruhe.},
  journal = {" "},
}

@phdthesis{kong2021_taming_android_app_crashes,
  title={Taming Android App Crashes},
  author={Kong, Pingfan},
  year={2021},
  school={University of Luxembourg,​ Luxembourg City, Luxembourg},
  type = {{Ph.D.} dissertation},
  url = {http://hdl.handle.net/10993/46741},
  download = {https://orbilu.uni.lu/bitstream/10993/46741/1/Dissertation_Pingfan_KONG_Uni_Luxembourg.pdf},
  abstract = {
  App crashes constitute an important deterrence for app adoption in the android ecosystem. Yet, Android app developers are challenged by the limitation of test automation tools to ensure that released apps are free from crashes. In recent years, researchers have proposed various automation approaches in the literature. Unfortunately, the practical value of these approaches have not yet been confirmed by practitioner adoption. Furthermore, existing approaches target a variety of test needs which are relevant to different sets of problems, without being specific to app crashes.
  
  Resolving app crashes implies a chain of actions starting with their reproduction, followed by the associated fault localization, before any repair can be attempted. Each action however, is challenged by the specificity of Android. In particular, some specific mechanisms (e.g., callback methods, multiple entry points, etc.) of Android apps require Android-tailored crash-inducing bug locators. Therefore, to tame Android app crashes, practitioners are in need of automation tools that are adapted to the challenges that they pose. In this respect, a number of building blocks must be designed to deliver a comprehensive toolbox.
  
  First, the community lacks well-defined, large-scale datasets of real-world app crashes that are reproducible to enable the inference of valuable insights, and facilitate experimental validations of literature approaches. Second, although bug localization from crash information is relatively mature in the realm of Java, state-of-the-art techniques are generally ineffective for Android apps due to the specificity of the Android system. Third, given the recurrence of crashes and the substantial burden that they incur for practitioners to resolve them, there is a need for methods and techniques to accelerate fixing, for example, towards implementing Automated Program Repair (APR).
  
  Finally, the above chain of actions is for curative purposes. Indeed, this "reproduction, localization, and repair" chain aims at correcting bugs in released apps. Preventive approaches, i.e., approaches that help developers to reduce the likelihood of releasing crashing apps, are still absent. In the Android ecosystem, developers are challenged by the lack of detailed documentation about the complex Android framework API they use to develop their apps. For example, developers need support for precisely identifying which exceptions may be triggered by APIs. Such support can further alleviate the challenge related to the fact that the condition under which APIs are triggered are often not documented.
  
  In this context, the present dissertation aims to tame Android crashes by contributing to the following four building blocks:
  
  Systematic Literature Review on automated app testing approaches:
  We aim at providing a clear overview of the state-of-the-art works around the topic of Android app testing, in an attempt to highlight the main trends, pinpoint the main methodologies applied and enumerate the challenges faced by the Android testing approaches as well as the directions where the community effort is still needed. To this end, we conduct a Systematic Literature Review (SLR) during which we eventually identified 103 relevant research papers published in leading conferences and journals until 2016. Our thorough examination of the relevant literature has led to several findings and highlighted the challenges that Android testing researchers should strive to address in the future. After that, we further propose a few concrete research directions where testing approaches are needed to solve recurrent issues in app updates, continuous increases of app sizes, as well as the Android ecosystem fragmentation.
  
  Locating Android app crash-inducing bugs:
  We perform an empirical study on 500 framework-specific crashes from an open benchmark. This study reveals that 37 percent of the crash types are related to bugs that are outside the crash stack traces. Moreover, Android programs are a mixture of code and extra-code artifacts such as the Manifest file. The fact that any artifact can lead to failures in the app execution creates the need to position the localization target beyond the code realm. We propose ANCHOR, a two-phase suspicious bug location suggestion tool. ANCHOR specializes in finding crash-inducing bugs outside the stack trace. ANCHOR is lightweight and source code independent since it only requires the crash message and the apk file to locate the fault. Experimental results, collected via cross-validation and in-the-wild dataset evaluation, show that ANCHOR is effective in locating Android framework-specific crashing faults.
  
  Mining Android app crash fix templates:
  We propose a scalable approach, CraftDroid, to mine crash fixes by leveraging a set of 28 thousand carefully reconstructed app lineages from app markets, without the need for the app source code or issue reports. We develop a replicative testing approach that locates fixes among app versions which output different runtime logs with the exact same test inputs. Overall, we have mined 104 relevant crash fixes, further abstracted 17 fine-grained fix templates that are demonstrated to be effective for patching crashed apks. Finally, we release ReCBench, a benchmark consisting of 200 crashed apks and the crash replication scripts, which the community can explore for evaluating generated crash-inducing bug patches.
  
  Documenting framework APIs' unchecked exceptions:
  We propose Afuera, an automated tool that profiles Android framework APIs and provides information on when they can potentially trigger unchecked exceptions. Afuera relies on a static-analysis approach and a dedicated algorithm to examine the entire Android framework. With Afuera, we confirmed that 26739 unique unchecked exception instances may be triggered by invoking 5467 (24\%) Android framework APIs. Afuera further analyzes the Android framework to inform about which parameter(s) of an API method can potentially be the cause of the triggering of an unchecked exception. To that end, Afuera relies on fully automated instrumentation and taint analysis techniques. Afuera is run to analyze 50 randomly sampled APIs to demonstrate its effectiveness.Evaluation results suggest that Afuera has perfect true positive rate. However, Afuera is affected by false negatives due to the limitation of state-of-the-art taint analysis techniques.
  }
}


@phdthesis{oliver2018_first_steps_in_retrofitting_a_versatile_sw_testing_architecture,
  title={First Steps in Retrofitting a Versatile Software Testing Infrastructure to Android},
  author={Oliver, Carol Anne},
  year={2018},
  school = {Florida Institute of Technology},
  address = {Melbourne, Florida, USA},
}


@phdthesis{shuba2019mobile,
  title={Mobile Data Transparency and Control},
  author={Shuba, Anastasia},
  year={2019},
  school={UC Irvine},
  relevance_to_phd={The research studies information mobile apps collect and send over the network, often for advertising, also for analytics. The author developed various tools (AntMonitor, AntShield, NoMoAds, and AutoLabel) to help users see what was happening and to block unwanted communications. The tools can block analytics data being sent, which would affect our approach by reducing the number of users the data is collected from. They discuss related works that use static analysis to identify third-party data collection libraries.}
}

@phdthesis{stanik2020_requirements_intelligence_on_the_analysis_of_user_feedback,
  title={Requirements Intelligence: On the Analysis of User Feedback},
  author={Stanik, Christoph},
  year={2020},
  school = {Universität Hamburg},
  relevance_to_phd = {continuous sources for requirements-related information; comparison between explicit and implicit user feedback (like app usage data)},
  abstract = {
    Traditionally, software requirements engineering involved users through workshops, interviews, and observations in the early software development phases. Although beneﬁcial to software teams, these approaches are challenging to carry out continuously and can involve only a limited number of users. In recent years, requirements stakeholders started analyzing explicit user feedback, such as written app reviews, and implicit user feedback like app usage data as continuous sources for requirements-related information. Yet, research highlights that stakeholders rarely use explicit and implicit user feedback in their decision-making process because they receive it in large and unﬁltered amounts, making a manual analysis unfeasible. As user satisfaction is crucial for the success of an app, stakeholders need automated approaches for analyzing user feedback to understand user needs and to guide their decision-making. In an interview study, we found that stakeholders need to know how their apps perform, to eﬃciently identify innovative features, and to understand reported issues and bugs.

    This dissertation introduces requirements intelligence, a framework that continuously collects, preprocesses, ﬁlters, as well as transforms and matches explicit and implicit user feedback to requirements. The framework aims to generate insights for stakeholders in an integrated interactive visualization. The core enablers for requirements intelligence include two main analysis activities on explicit and implicit feedback: Feedback ﬁltering and feedback to requirements analysis. Feedback ﬁltering is the activity that identiﬁes requirements-relevant feedback, such as problem reports, inquiries, and feature requests. Feedback to requirements extracts the software features users discuss and matches them with the features as documented on, e.g., app pages. We developed and empirically evaluated supervised machine learning approaches for both feedback types and activities. Our approaches rely on crowdsourcing studies for training machine learning models and on benchmarking experiments for identifying the optimal machine learning models.

    Based on our requirements intelligence framework, we iteratively developed the prototype feed.ai. We evaluated feed.ai with a total of 15 stakeholders from a major telecommunication company for 12 months. We found that the stakeholders agreed with 92\% of the automated ﬁltering results indicating high accuracy. The stakeholders found requirements intelligence beneﬁcial for departments working with user feedback like customer care, marketing, and technology innovation. In a ﬁnal survey, ten stakeholders anonymously rated feed.ai’s functionality, on average, with 4.1/5 and its usability with 4.3/5. They further reported that feed.ai helped them to reduce 70\% of their time spent on analyzing user feedback, indicating a high eﬀectiveness of our approach.},
}

@phdthesis{zieris2020_phd_qualitative_analysis_of_knowledge_transfer_in_pair_programming,
  title={Qualitative Analysis of Knowledge Transfer in Pair Programming},
  author={Zieris, Franz},
  year={2020},
  school = {Freie Universitaet Berlin},
  abstract = {
    Pair programming (PP) is the practice of two developers working closely together on one computer to solve a technical task. It is used by developers in industry in order to tackle difficult problems, to produce code with better design and fewer defects, and to learn together and from another. The transfer and acquisition of knowledge is central to all these expectations, but whether and how they actually come to pass is an open question.

    My goal is to understand the mechanisms of knowledge transfer in PP in order to formulate practically relevant results and advice for software developers. I perform a qualitative analysis based on the Grounded Theory Methodology and the Base Layer for pair programming research. I analyze 27 industrial PP sessions recorded in ten companies: The developers work on their everyday tasks covering different aspects of software development, with whom and for as long they want, totaling 40 hours of material. I performed supporting field observations and ad hoc interviews in two of the companies.

    My results are a detailed bottom-up conceptualization of knowledge transfer processes in pair programming, ranging from individual utterances, over knowledge transfer episodes, to overall session dynamics that are shared across different types of pairings. In particular, I find that:

    1. Knowledge is transferred in basically all PP sessions, not just in supposed “expert/novice” constellations. 2. Knowledge regarding the software system is by far the most commonly transferred type, and most developers appear familiar with transferring it to or from the partner and to acquire it together. 3. General software development knowledge is also transferred between partners, but less than system knowledge and only after the pair dealt with its system knowledge needs. 4. Additional types of knowledge, such as application domain concepts, were not explicit topics but merely showed up as identifiers in the source code. 5. Pairs that maintain a shared understanding of the system and software development in general may have short,but highly productive focus phases; others may suffer from a breakdown of the pair process when such a shared understanding is lacking. A missing shared plan, reduced workspace awareness, or language barriers further reduce their togetherness.

    I validated the high-level concepts with practitioners from four companies—two from the original data collection and two additional—and developed three ideas for how to put my results to use in everyday software development.
  },
  doi = {http://dx.doi.org/10.17169/refubium-28718},
  url = {https://refubium.fu-berlin.de/handle/fub188/28968},
  keywords = {qualitative analysis, empirical software engineering, grounded theory, pair programming},
}

@techreport{not_yet_cited_appdynamics_AppDynamics_App_Attention_Index_2021,
  title = {AppDynamics App Attention Index 2021},
  subtitle = {Who Takes The Rap For The App?},
  url = {https://www.appdynamics.com/c/dam/r/appdynamics/Gated-Assets/analyst-reports/AppDynamics-App-Attention-Index-2021.pdf},
  author = {{AppDynamics LLC}},
  year = {2021},
  publisher = {{AppDynamics LLC}},
  address = {San Francisco, USA},
  extracts = { on page 14
   - So what sorts of issues might cause a consumer to walk away?
   Today, the experience of using an application can be impacted by many variables, from those that sit within the application itself – such as pages loading slowly, poor response times, downtime, or security failures; through to external factors outside the application – such as bad internet connectivity, 4G/5G mobile network issues, slow payment gateways or technical issues with third party plugins. This has caused massive complexity for brands and for the technologists responsible for monitoring performance and delivering seamless digital experiences for end users.
   - Why? Because many of these issues have not traditionally been the responsibility of the application ‘owner’ – the technologists responsible for monitoring the application, and identifying and fixing performance issues before they impact the customer.
   - But in a world where consumer expectations of digital experiences have reached new heights, the question of ‘Who takes the rap for the app? ’ lays responsibility squarely at the door of the failed application – or more importantly, the brand behind the application. No matter the cause.

    Top five problems encountered with digital services over past 12 months 
    1) Slow page / screen loading
    2) Poor connectivity
    3) Crashing / stopped working
    4) Slow streaming of content / media
    5) Slows down device / drains battery
  }
}

% Thanks to https://tex.stackexchange.com/questions/203901/how-to-cite-iso-or-british-standards-in-latex-bibtex
% and to https://tex.stackexchange.com/questions/54441/bib-sorting-without-author
@techreport{BS_7925_1_1998,
  type = {Standard},
  key = {BS 7925-1:1998},
  author = {Various},
  year = {1998},
  title = {Software testing. Vocabulary},
  volume = {1998},
  address = {London, UK},
  institution = {British Standards Institution},
}

@techreport{RFC3164,
  author = {C. Lonvick},
  title = {The BSD Syslog Protocol},
  howpublished = {Internet Requests for Comments},
  type = {RFC},
  number = {3164},
  year = {2001},
  month = {August},
  issn = {2070-1721},
  publisher = {RFC Editor},
  institution = {RFC Editor},
  shorthand = {RFC3164},
}

@techreport{iso25010-2011-en,
  type = {standard},
  title = {ISO/IEC 25010:2011(en)
Systems and software engineering — Systems and software Quality Requirements and Evaluation (SQuaRE) — System and software quality models},
  author = {Technical Committee : ISO/IEC JTC 1/SC 7 Software and systems engineering},
  pages = {34},
  year = {2011},
  institution = {ISO},
}

@techreport{iso29119-1-2013,
  type = {standard},
  title={ISO/IEC/IEEE 29119-1:2013(E): Software and Systems Engineering Software Testing Part 1:Concepts and Definitions},
  author={ISO/IEC/IEEE 29119-1:2013(E)},
  institution={International Organization for Standardization and International Electrotechnical Commission and Institute of Electrical and Electronics Engineers and IEEE-SA Standards Board},
  isbn={9780738185972},
  series={IEEE Std},
  year={2013},
  address={New York, USA},
  publisher={IEEE}
}

@techreport{dimensionalresearch2015_mobile_app_use_and_abandonment,
  title = {Mobile app Use and Abandonment Global Survey of Mobile app Users},
  year = {2015},
  publisher = {{HP}},
  author = {{Dimensional Research}},
  institution = {{Dimensional Research}},
  executive_summary = {
    Users are reaching for mobile devices numerous times everyday to use mobile apps. These users shared that peer reviews and ratings significantly affect their app choices. App stability and performance heavily define the user experience and overall satisfaction. Yet this study finds that users are experiencing severe app performance issues regularly. Importantly, apps that exhibit these issues are quickly abandoned after just a few occurrences.
    For companies who create mobile apps, while good performance can lead to app downloads and satisfied users, poor performance will result in quick app abandonment and brand tarnishment.
  },
  participants = {A total of 3011 participants that use mobile apps completed the global survey including those in Belgium, Canada, France, Germany, Netherlands, United Kingdom, and United States.},
  methodology = {Mobile devices users in North America and Europe were invited to participate in a survey on the topic of mobile apps. The survey was administered electronically and participants were offered a token compensation for their participation.},
  research_goal = {The primary research goal was to capture hard data on the key factors that determine end user satisfaction with mobile apps. In addition, research sought to determine what users did when they were unsatisfied with a mobile app.},
}

@techreport{gartner2015_market_guide_for_mobile_app_analytics,
  title = {Market Guide for Mobile App Analytics},
  author = {Wong, J.},
  institution = {{Gartner Inc.}},
  year = {2015},
  url = {https://www.gartner.com/doc/3130520?ref=SiteSearch&sthkw=mobile\%20app\%20analytics&fnl=search&srcId=1-3478922254},
}