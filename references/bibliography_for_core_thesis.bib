@comment{https://tex.stackexchange.com/questions/159806/add-text-comment-into-bibliography}
@comment{https://tex.stackexchange.com/questions/439328/adding-comments-in-bib-file-that-survive-bibdesks-save}
@comment{MUST_DO review the content of citations, https://tex.stackexchange.com/questions/400593/warning-empty-institution-and-warning-empty-journal}
@comment{also useful https://tex.stackexchange.com/questions/175106/make-phd-citations-say-dissertation-rather-than-thesis}
@comment{https://www.openoffice.org/bibliographic/bibtex-defs.html provides a succinct guide of the main definition types and their fields.}

@comment{Here's a blank template useful to copy, paste and then populate with the details of the reference.}
\iffalse
@online{_blank_2019,
    title = {},
    url = {},
    abstract = {},
    author = {},
    urldate = {},
    date = {}
}
/fi

@article{acm_shonan_workshops_2020,
    author = {Kawarabayashi, Ken-Ichi},
    title = {The NII Shonan Meeting in Japan},
    year = {2020},
    issue_date = {April 2020},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {63},
    number = {4},
    issn = {0001-0782},
    url = {https://doi.org/10.1145/3378546},
    doi = {10.1145/3378546},
    journal = {Commun. ACM},
    month = mar,
    pages = {48–49},
    numpages = {2}
}

@article{agrawal2020_better_software_analytics_via_duo,
    author={Agrawal, Amritanshu
    and Menzies, Tim
    and Minku, Leandro L.
    and Wagner, Markus
    and Yu, Zhe},
    title={Better software analytics via ``DUO'': Data mining algorithms using/used-by optimizers},
    journal={Empirical Software Engineering},
    year={2020},
    month={May},
    day={01},
    volume={25},
    number={3},
    pages={2099-2136},
    abstract={This paper claims that a new field of empirical software engineering research and practice is emerging: data mining using/used-by optimizers for empirical studies, or DUO. For example, data miners can generate models that are explored by optimizers. Also, optimizers can advise how to best adjust the control parameters of a data miner. This combined approach acts like an agent leaning over the shoulder of an analyst that advises ``ask this question next'' or ``ignore that problem, it is not relevant to your goals''. Further, those agents can help us build ``better'' predictive models, where ``better'' can be either greater predictive accuracy or faster modeling time (which, in turn, enables the exploration of a wider range of options). We also caution that the era of papers that just use data miners is coming to an end. Results obtained from an unoptimized data miner can be quickly refuted, just by applying an optimizer to produce a different (and better performing) model. Our conclusion, hence, is that for software analytics it is possible, useful and necessary to combine data mining and optimization using DUO.},
    issn={1573-7616},
    doi={10.1007/s10664-020-09808-9},
    url={https://doi.org/10.1007/s10664-020-09808-9}
}



@article{ali2019behavior_catrobat,
  title={Behavior-Driven Development as an Error-Reduction Practice for Mobile Application Testing},
  author={Ali, Zulfiqar},
  journal={International Journal of Computer Science Issues (IJCSI)},
  volume={16},
  number={2},
  pages={1--10},
  year={2019},
  publisher={International Journal of Computer Science Issues (IJCSI)},
  doi = {10.5281/zenodo.3234110},
  url = {https://www.ijcsi.org/papers/IJCSI-16-2-1-10.pdf},
}

@article{ali2019using_catrobat,
	author = {Zulfiqar Ali and Aiman M Awwad and Wolfgang Slany},
	title = {Using Executable Specification and Regression Testing for Broadcast Mechanism of Visual Programming Language on Smartphones},
	journal = {International Journal of Interactive Mobile Technologies (iJIM)},
	volume = {13},
	number = {02},
	year = {2019},
	keywords = {Mobile Application, Regression Testing, Behavior Driven Development, Visual Programming Environment, Catrobat},
	abstract = {The rapid advancement of mobile computing technology and the rising usage of mobile apps made our daily life more productive. The mobile app should operate all the time bug-free in order to improve user satisfaction and offers great business value to the end user. At the same time, smartphones are full of special features that make testing of apps more challenging. Actually, the quality is a must for successful applications and it cannot be achieved without testing and verification. In this paper, we present the Behavior Driven Development (BDD) methodology and Cucumber framework to automate regression testing of Android apps. Particularly, the proposed methods use the visual programming language for smartphones (Catrobat) as a reference. The Catrobat program scripts communicate via a broadcast mechanism. The objective is to test the broadcast mechanism from different angles and track regression errors as well as specify and diagnose bugs with the help of executable specifications. The results show that the methods are able to effectively reveal deficiencies in the broadcast mechanism, and ensure that the app matches all expectations and needs of end users.},
	issn = {1865-7923},
	url = {https://onlinejour.journals.publicknowledgeproject.org/index.php/i-jim/article/view/9851},
	pages = {50--65},
}

@article{ALNAWAS2016313,
title = "The effect of benefits generated from interacting with branded mobile apps on consumer satisfaction and purchase intentions",
journal = "Journal of Retailing and Consumer Services",
volume = "31",
pages = "313 - 322",
year = "2016",
issn = "0969-6989",
doi = "https://doi.org/10.1016/j.jretconser.2016.04.004",
url = "http://www.sciencedirect.com/science/article/pii/S0969698916300364",
author = "Ibrahim Alnawas and Faisal Aburub",
keywords = "Usage and gratification approach, Experiential engagement, Mobile apps",
abstract = "This study extends the “Uses and Gratifications” approach (U&G) from a web context to a new one, i.e. mobile applications. It seeks to investigate the effect of the key benefits generated from interacting with branded mobile apps on consumer satisfaction and purchase intentions. A self-administrated questionnaire was used to collect the study data. The questionnaire was distributed to 358 participants inside seven major malls in a Middle Eastern country (Jordan). Purposive sample was employed. The data were analyzed using structural equation modeling (AMOS 18). Four key findings emerged from the current research. First, the study confirms the existence of four interaction-based benefits in the context of mobile apps, namely: learning benefits, social integrative benefits, personal integrative benefits and hedonic benefits. Second, apart from social integrative benefits, the other three benefits are found to influence consumer satisfaction to varying degrees. Third, with regard to purchase intentions, only learning benefits and hedonic benefits are found to generate that. Finally, the study confirms the relationship between consumer satisfaction and purchase intentions in a mobile context. The study contributes to the literature through adopting the U&G approach as a theoretical base to examine the key benefits that consumers gain when interacting with branded mobile apps."
}

@article{alsubaihin2019app_store_effects_on_software_engineering,
  title={App store effects on software engineering practices},
  author={A. {AlSubaihin} and F. {Sarro} and S. {Black} and L. {Capra} and M. {Harman}},
  abstract={In this paper, we study the app store as a phenomenon from the developers perspective to investigate the extent to which app stores affect software engineering tasks. Through developer interviews and questionnaires, we uncover findings that highlight and quantify the effects of three high-level app store themes: bridging the gap between developers and users, increasing market transparency and affecting mobile release management. Our findings have implications for testing, requirements engineering and mining software repositories research fields. These findings can help guide future research in supporting mobile app developers through a deeper understanding of the app store-developer interaction.},
  journal={IEEE Transactions on Software Engineering},
  year={2019},
  publisher={IEEE},
  volume = {47},
  number = {2},
  pages={300-319},
  doi={10.1109/TSE.2019.2891715}},
}

@ARTICLE{anderson2015_docker_software_engineering_podcast,
  author={Anderson, Charles},
  journal={IEEE Software}, 
  title={Docker [Software engineering]},
  year={2015},
  volume={32},
  number={3}, 
  pages={102-c3}, 
  abstract={
    In episode 217 of Software Engineering Radio, host Charles Anderson talks with James Turnbull, a software developer and security specialist who's vice president of services at Docker. Lightweight Docker containers are rapidly becoming a tool for deploying microservice-based architectures.},  
  keywords={},  
  doi={10.1109/MS.2015.62},  
  ISSN={1937-4194},  
  month={May},
  full_podcast = {https://www.se-radio.net/2015/01/episode-217-james-turnbull-on-docker/},
  quote = {
    The DevOps movement, for example, emerged from one of the classic stumbling blocks in a lot of organizations. Developers build code and applications and ship them to the operations people, only to discover that the code and applications don't run in production. This is the classic “it works on my machine; it's operations' problem now.”
  },
  thoughts = {This provides a suitable connection between DevOps and mobile app world, where developers don't seem to actively consider whether their code will work in other environments, other conditions, etc. Many organisations struggle to get their server-side software reliable across various test environments and then into production. Mobile appstore ecosystems exacerbate the challenges as the environments are more varied, owned by third parties and shared with code from many other third-parties (other apps on the device).
  
  The test channels provided by Google Play Console provide a bridge between the app development that happens within an organisation and one that starts to include external, real-world elements including the automated checks and robot tests, and alpha/beta testers with their individual microcosms (devices and device accounts).}
}

@article{automated_testing_android_apps_SLR_2019, 
author={P. {Kong} and L. {Li} and J. {Gao} and K. {Liu} and T. F. {Bissyandé} and J. {Klein}}, 
journal={IEEE Transactions on Reliability}, 
title={Automated Testing of Android Apps: A Systematic Literature Review}, 
year={2019}, 
volume={68}, 
number={1}, 
pages={45-66}, 
abstract={Automated testing of Android apps is essential for app users, app developers, and market maintainer communities alike. Given the widespread adoption of Android and the specificities of its development model, the literature has proposed various testing approaches for ensuring that not only functional requirements but also nonfunctional requirements are satisfied. In this paper, we aim at providing a clear overview of the state-of-the-art works around the topic of Android app testing, in an attempt to highlight the main trends, pinpoint the main methodologies applied, and enumerate the challenges faced by the Android testing approaches as well as the directions where the community effort is still needed. To this end, we conduct a systematic literature review during which we eventually identified 103 relevant research papers published in leading conferences and journals until 2016. Our thorough examination of the relevant literature has led to several findings and highlighted the challenges that Android testing researchers should strive to address in the future. After that, we further propose a few concrete research directions where testing approaches are needed to solve recurrent issues in app updates, continuous increases of app sizes, as well as the Android ecosystem fragmentation.}, 
keywords={mobile computing;program testing;software engineering;market maintainer communities;development model;Android testing researchers;Android ecosystem fragmentation;Android apps automated testing;Testing;Androids;Humanoid robots;Bibliographies;Ecosystems;Java;Systematics;Android;automated testing;literature review;survey}, 
doi={10.1109/TR.2018.2865733}, 
ISSN={0018-9529}, 
month={March},}

@article{avizienis2004_basic_concepts_and_taxonomy,
  key_paper_reason = {Seminal reference of key terms and concepts},
  author={A. {Avizienis} and J. -. {Laprie} and B. {Randell} and C. {Landwehr}},
  journal={IEEE Transactions on Dependable and Secure Computing},
  title={Basic concepts and taxonomy of dependable and secure computing},
  year={2004},
  volume={1},
  number={1},
  pages={11-33},
  abstract={
    This paper gives the main definitions relating to dependability, a generic concept including a special case of such attributes as reliability, availability, safety, integrity, maintainability, etc. Security brings in concerns for confidentiality, in addition to availability and integrity. Basic definitions are given first. They are then commented upon, and supplemented by additional definitions, which address the threats to dependability and security (faults, errors, failures), their attributes, and the means for their achievement (fault prevention, fault tolerance, fault removal, fault forecasting). The aim is to explicate a set of general concepts, of relevance across a wide range of situations and, therefore, helping communication and cooperation among a number of scientific and technical communities, including ones that are concentrating on particular types of system, of system failures, or of causes of system failures.
  },
  keywords={fault tolerant computing;data privacy;security of data;system recovery;software reliability;taxonomy;dependable computing;secure computing;system reliability;system availability;system safety;system integrity;system maintainability;fault prevention;fault tolerance;fault removal;fault forecasting;system security;system failures;system vulnerabilities;system attacks;Taxonomy;Availability;Fault tolerance;Safety;Maintenance;Communication system security;Uncertainty;Standardization;Books;Index Terms- Dependability;security;trust;faults;errors;failures;vulnerabilities;attacks;fault tolerance;fault removal;fault forecasting.},
  doi={10.1109/TDSC.2004.2},
  ISSN={1941-0018},
  month={Jan},
}

@article{ayyal2016automated_bidi_testing,
  title={Automated bidirectional languages localization testing for android apps with rich GUI},
  author={Ayyal Awwad, Aiman M and Slany, Wolfgang},
  journal={Mobile Information Systems},
  volume={2016},
  year={2016},
  publisher={Hindawi},
  doi = {10.1155/2016/2872067},
  pages = {1--13},
}

@article{bach2000_sbtm,
  title = {Session-Based Test Management},
  author = {Jonathan Bach},
  year = {2000},
  journal = {Software Testing and Quality Engineering magazine},
  publisher = {STQE},
  pages = {32--37},
  volume = {2000},
  issue = {06},
  url = {https://www.satisfice.com/download/session-based-test-management},
}

@ARTICLE{baez2021_chatbot_integrations,
  author={M. {Baez} and F. {Daniel} and F. {Casati} and B. {Benatallah}},
  journal={IEEE Internet Computing}, 
  title={Chatbot integration in few patterns}, 
  year={2021},
  volume={25},
  number={3},
  pages={52-59},
  abstract={Chatbots are software agents that are able to interact with humans in natural language. Their intuitive interaction paradigm is expected to significantly reshape the software landscape of tomorrow, while already today chatbots are invading a multitude of scenarios and contexts. This article takes a developer's perspective, identifies a set of architectural patterns that capture different chatbot integration scenarios, and reviews state-of-the-art development aids.},
  keywords={Computer architecture;Task analysis;Meteorology;Internet;Natural language processing;Software systems},
  doi={10.1109/MIC.2020.3024605},
  ISSN={1941-0131},
  month={},}

@article{bavota2014_impact_of_api_change_android,
    author={G. {Bavota} and M. {Linares-Vásquez} and C. E. {Bernal-Cárdenas} and M. D. {Penta} and R. {Oliveto} and D. {Poshyvanyk}},  
    journal={IEEE Transactions on Software Engineering},
    title={The Impact of API Change- and Fault-Proneness on the User Ratings of Android Apps},
    publisher = {IEEE},
    year={2015},
    volume={41},
    number={4},
    pages={384-407},
    abstract = {The mobile apps market is one of the fastest growing areas in the information technology. In digging their market share, developers must pay attention to building robust and reliable apps. In fact, users easily get frustrated by repeated failures, crashes, and other bugs; hence, they abandon some apps in favor of their competition. In this paper we investigate how the fault- and change-proneness of APIs used by Android apps relates to their success estimated as the average rating provided by the users to those apps. First, in a study conducted on 5,848 (free) apps, we analyzed how the ratings that an app had received correlated with the fault- and change-proneness of the APIs such app relied upon. After that, we surveyed 45 professional Android developers to assess (i) to what extent developers experienced problems when using APIs, and (ii) how much they felt these problems could be the cause for unfavorable user ratings. The results of our studies indicate that apps having high user ratings use APIs that are less fault- and change-prone than the APIs used by low rated apps. Also, most of the interviewed Android developers observed, in their development experience, a direct relationship between problems experienced with the adopted APIs and the users' ratings that their apps received.},
    
    keywords = {application program interfaces;data mining;mobile computing;program debugging;software fault tolerance;system recovery;API change-proneness;API fault-proneness;user ratings;Android Apps;mobile Apps market;information technology;software repository mining;Androids;Humanoid robots;Software;History;Computer bugs;Educational institutions;Electronic mail;Mining software repositories;empirical studies;android;API changes;Mining software repositories;empirical studies;android;API changes},
    doi={10.1109/TSE.2014.2367027},
    ISSN={1939-3520},
    month={April},
}

@article{bevan1999_89_quality_in_use_meeting_user_needs_for_quality,
    title = {Quality in use: Meeting user needs for quality},
    journal = {Journal of Systems and Software},
    volume = {49},
    number = {1},
    pages = {89-96},
    year = {1999},
    issn = {0164-1212},
    doi = {https://doi.org/10.1016/S0164-1212(99)00070-9},
    url = {https://www.sciencedirect.com/science/article/pii/S0164121299000709},
    author = {Nigel Bevan},
    abstract = {There is an increasing demand for software that matches real user needs in a working environment. The paper describes the new framework for software product quality developed for ISO/IEC 9126-1: internal quality (static properties of the code), external quality (behaviour of the software when it is executed) and quality in use (the extent to which the software meets the needs of the user). Quality in use is a broader view of the ergonomic concept of usability in ISO 9241-11 (1998). Achieving quality in use requires a user-centred design process which has cultural, strategic and technical implications.}
}

@article{blichfeldt2006creating,
  title={Creating a wider audience for action research: Learning from case-study research},
  author={Blichfeldt, Bodil Stilling and Andersen, Jesper Rank},
  journal={Journal of Research Practice},
  volume={2},
  number={1},
  pages={D2--D2},
  year={2006},
  url ={http://jrp.icaap.org/index.php/jrp/article/view/23/43},
}

@article{BROOMHEAD1986_217_extracting_qualitative_dynamics_from_experimental_data,
    title = {Extracting qualitative dynamics from experimental data},
    journal = {Physica D: Nonlinear Phenomena},
    volume = {20},
    number = {2},
    pages = {217-236},
    year = {1986},
    issn = {0167-2789},
    doi = {https://doi.org/10.1016/0167-2789(86)90031-X},
    url = {https://www.sciencedirect.com/science/article/pii/016727898690031X},
    author = {D.S. Broomhead and Gregory P. King},
    abstract = {We consider the notion of qualitative information and the practicalities of extracting it from experimental data. Our approach, based on a theorem of Takens, draws on ideas from the generalized theory of information known as singular system analysis due to Bertero, Pike and co-workers. We illustrate our technique with numerical data from the chaotic regime of the Lorenz model.}
}

@article{carlsson2013truth,
  title={The truth, the whole truth, and nothing but the truth—A multiple country test of an oath script},
  author={Carlsson, Fredrik and Kataria, Mitesh and Krupnick, Alan and Lampi, Elina and L{\"o}fgren, {\AA}sa and Qin, Ping and Sterner, Thomas},
  journal={Journal of Economic Behavior \& Organization},
  volume={89},
  pages={105--121},
  year={2013},
  publisher={Elsevier}
}

@article{catolino2019not,
  title={Not all bugs are the same: Understanding, characterizing, and classifying bug types},
  author={Catolino, Gemma and Palomba, Fabio and Zaidman, Andy and Ferrucci, Filomena},
  journal={Journal of Systems and Software},
  volume={152},
  pages={165--181},
  year={2019},
  publisher={Elsevier}
}

@article{cave2020covid,
  title={COVID-19 super-spreaders: Definitional quandaries and implications},
  author={Cave, Emma},
  journal={Asian bioethics review},
  volume={12},
  pages={235--242},
  year={2020},
  publisher={Springer}
}

@article{cruz2019_guess_what_test_your_app,
  title={To the attention of mobile software developers: guess what, test your app!},
  author={Cruz, Luis and Abreu, Rui and Lo, David},
  journal={Empirical Software Engineering},
  volume={24},
  number={4},
  pages={2438--2468},
  year={2019},
  publisher={Springer},
  doi = {10.1007/s10664-019-09701-0},
}

@article{davenport2006competing_on_analytics,
  title={Competing on analytics},
  author={Davenport, Thomas H and others},
  journal={Harvard Business Review},
  volume={84},
  number={1},
  pages={98},
  year={2006}
}

@article{derakhshanfar2020_search_based_crash_reduction,
  author = {Derakhshanfar, Pouria and Devroey, Xavier and Perrouin, Gilles and Zaidman, Andy and van Deursen, Arie},
  title = {Search-based crash reproduction using behavioural model seeding},
  journal = {Software Testing, Verification and Reliability},
  volume = {30},
  number = {3},
  pages = {e1733},
  keywords = {seed learning, crash reproduction, search-based software testing},
  doi = {https://doi.org/10.1002/stvr.1733},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1733},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/stvr.1733},
  note = {e1733 stvr.1733},
  abstract = {
    Summary Search-based crash reproduction approaches assist developers during debugging by generating a test case, which reproduces a crash given its stack trace. One of the fundamental steps of this approach is creating objects needed to trigger the crash. One way to overcome this limitation is seeding: using information about the application during the search process. With seeding, the existing usages of classes can be used in the search process to produce realistic sequences of method calls, which create the required objects. In this study, we introduce behavioural model seeding: a new seeding method that learns class usages from both the system under test and existing test cases. Learned usages are then synthesized in a behavioural model (state machine). Then, this model serves to guide the evolutionary process. To assess behavioural model seeding, we evaluate it against test seeding (the state-of-the-art technique for seeding realistic objects) and no seeding (without seeding any class usage). For this evaluation, we use a benchmark of 122 hard-to-reproduce crashes stemming from six open-source projects. Our results indicate that behavioural model seeding outperforms both test seeding and no seeding by a minimum of 6\% without any notable negative impact on efficiency.},
  year = {2020},
  thoughts = {
    crash reproduction is relevant to my research as it may help practitioners reproduce the causes of a crash so they can address it effectively.
  }
}


@article{diallo2017_what_is_a_fault_why_does_it_matter,
  title={What is a fault? and why does it matter?},
  author={Diallo, Nafi and Ghardallou, Wided and Desharnais, Jules and Frias, Marcelo and Jaoua, Ali and Mili, Ali},
  journal={Innovations in Systems and Software Engineering},
  volume={13},
  number={2-3},
  pages={219--239},
  year={2017},
  publisher={Springer},
  doi = {10.1007/s11334-017-0300-7},
}

@online{dobbing1998reliability,
  title={Reliability of Smart Instrumentation},
  author={Dobbing, A and Clark, N and Godfrey, D and Harris, P M  and Parkin, G and Stevens, M J and Wichmann, B A},
  organization = {National Physical Laboratory and Druck Ltd.},
  year={1998},
  numpages = {8},
}

@article{dromey1996_cornering_the_chimera,
  title={Cornering the chimera [software quality]},
  author={Dromey, R Geoff},
  journal={IEEE Software},
  volume={13},
  number={1},
  pages={33--43},
  year={1996},
  publisher={IEEE},
  abstract={Concrete and useful suggestions about what constitutes quality software have always been elusive. I suggest a framework for the construction and use of practical, testable quality models for requirements, design and implementation. Such information may be used directly to build, compare, and assess better quality software products.},
}

@article{fielding2012_triangulation_and_mixed_methods_designs,
    author = {Nigel G. Fielding},
    title ={Triangulation and Mixed Methods Designs: Data Integration With New Research Technologies},
    journal = {Journal of Mixed Methods Research},
    volume = {6},
    number = {2},
    pages = {124-136},
    year = {2012},
    doi = {10.1177/1558689812437101},
    URL = {https://doi.org/10.1177/1558689812437101},
    eprint = {https://doi.org/10.1177/1558689812437101},
    abstract = { Data integration is a crucial element in mixed methods analysis and conceptualization. It has three principal purposes: illustration, convergent validation (triangulation), and the development of analytic density or “richness.” This article discusses such applications in relation to new technologies for social research, looking at three innovative forms of data integration that rely on computational support: (a) the integration of geo-referencing technologies with qualitative software, (b) the integration of multistream visual data in mixed methods research, and (c) the integration of data from qualitative and quantitative methods.}
}


@article{floridi2019_five_risks_of_being_unethical,
author="Floridi, Luciano",
title="Translating Principles into Practices of Digital Ethics: Five Risks of Being Unethical",
journal="Philosophy {\&} Technology",
year="2019",
month="Jun",
day="01",
volume="32",
number="2",
pages="185--193",
issn="2210-5441",
doi="10.1007/s13347-019-00354-x",
url="https://doi.org/10.1007/s13347-019-00354-x"
}

@article{foidl2018_integrating_software_quality_models_into_risk_based_testing,
  title={Integrating software quality models into risk-based testing},
  author={Foidl, Harald and Felderer, Michael},
  journal={Software quality journal},
  volume={26},
  number={2},
  pages={809--847},
  year={2018},
  publisher={Springer},
  address={New York, USA},
  doi={https://doi.org/10.1007/s11219-016-9345-3},
}

@article{garvin1984_what_does_product_quality_really_mean,
  title={What does ``product quality” really mean?},
  author={Garvin, David A},
  organization = {Harvard University},
  journal={Sloan management review},
  volume={25},
  pages={19},
  year={1984}
}

@article{GAVIDIACALDERON2021_game_theoretic_analysis_of_software_development_practices,
    title = {Game-theoretic analysis of development practices: Challenges and opportunities},
    journal = {Journal of Systems and Software},
    volume = {159},
    pages = {110424},
    year = {2020},
    issn = {0164-1212},
    doi = {https://doi.org/10.1016/j.jss.2019.110424},
    url = {https://www.sciencedirect.com/science/article/pii/S0164121219301980},
    author = {Carlos Gavidia-Calderon and Federica Sarro and Mark Harman and Earl T. Barr},
    keywords = {Game theory, Empirical analysis, Technical debt, Software engineering practices},
    abstract = {
      Developers continuously invent new practices, usually grounded in hard-won experience, not theory. Game theory studies cooperation and conflict; its use will speed the development of effective processes. A survey of game theory in software engineering finds highly idealised models that are rarely based on process data. This is because software processes are hard to analyse using traditional game theory since they generate huge game models. We are the first to show how to use game abstractions, developed in artificial intelligence, to produce tractable game-theoretic models of software practices. We present Game-Theoretic Process Improvement (GTPI), built on top of empirical game-theoretic analysis. Some teams fall into the habit of preferring “quick-and-dirty” code to slow-to-write, careful code, incurring technical debt. We showcase GTPI’s ability to diagnose and improve such a development process. Using GTPI, we discover a lightweight intervention that incentivises developers to write careful code: add a singlecode reviewer who needs to catch only 25\% of kludges. This 25\% accuracy is key; it means that a reviewer does not need to examine each commit in depth, making this process intervention cost-effective.
  }
}


@article{gerlich2015app,
  title={App consumption: An exploratory analysis of the uses \& gratifications of mobile apps},
  author={Gerlich, R Nicholas and Drumheller, Kristina and Babb, Jeffry and De'Armond, De'Arno},
  journal={Academy of Marketing Studies Journal},
  volume={19},
  number={1},
  pages={69},
  year={2015},
  publisher={Jordan Whitney Enterprises, Inc}
}

@article{hall2009systematic,
  title={A systematic review of theory use in studies investigating the motivations of software engineers},
  author={Hall, Tracy and Baddoo, Nathan and Beecham, Sarah and Robinson, Hugh and Sharp, Helen},
  journal={ACM Transactions on Software Engineering and Methodology (TOSEM)},
  volume={18},
  number={3},
  pages={1--29},
  year={2009},
  publisher={ACM New York, NY, USA}
}

@ARTICLE{yet_to_cite_hamill2009_common_trends_in_software_fault_and_failure_data,
  author={Hamill, Maggie and Goseva-Popstojanova, Katerina},
  journal={IEEE Transactions on Software Engineering},  
  title={Common Trends in Software Fault and Failure Data},  
  year={2009}, 
  volume={35}, 
  number={4}, 
  pages={484-496},
  abstract={
    The benefits of the analysis of software faults and failures have been widely recognized. However, detailed studies based on empirical data are rare. In this paper, we analyze the fault and failure data from two large, real-world case studies. Specifically, we explore: 1) the localization of faults that lead to individual software failures and 2) the distribution of different types of software faults. Our results show that individual failures are often caused by multiple faults spread throughout the system. This observation is important since it does not support several heuristics and assumptions used in the past. In addition, it clearly indicates that finding and fixing faults that lead to such software failures in large, complex systems are often difficult and challenging tasks despite the advances in software development. Our results also show that requirement faults, coding faults, and data problems are the three most common types of software faults. Furthermore, these results show that contrary to the popular belief, a significant percentage of failures are linked to late life cycle activities. Another important aspect of our work is that we conduct intra- and interproject comparisons, as well as comparisons with the findings from related studies. The consistency of several main trends across software systems in this paper and several related research efforts suggests that these trends are likely to be intrinsic characteristics of software faults and failures rather than project specific.},  
  keywords={}, 
  doi={10.1109/TSE.2009.3},
  ISSN={1939-3520}, 
  month={July},
}

@ARTICLE{hao2011_privacy_preserving_etc_with_data_dynamics,  
  author={Hao, Zhuo and Zhong, Sheng and Yu, Nenghai}, 
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={A Privacy-Preserving Remote Data Integrity Checking Protocol with Data Dynamics and Public Verifiability},
  year={2011}, 
  volume={23}, 
  number={9}, 
  pages={1432-1437},
  abstract={
  Remote data integrity checking is a crucial technology in cloud computing. Recently, many works focus on providing data dynamics and/or public verifiability to this type of protocols. Existing protocols can support both features with the help of a third-party auditor. In a previous work, Sebé et al. propose a remote data integrity checking protocol that supports data dynamics. In this paper, we adapt Sebé et al.'s protocol to support public verifiability. The proposed protocol supports public verifiability without help of a third-party auditor. In addition, the proposed protocol does not leak any private information to third-party verifiers. Through a formal analysis, we show the correctness and security of the protocol. After that, through theoretical analysis and experimental results, we demonstrate that the proposed protocol has a good performance.},  
  keywords={},  
  doi={10.1109/TKDE.2011.62}, 
  ISSN={1558-2191}, 
  month={Sep.},
}

@article{harris2016identifying,
  title={Identifying factors influencing consumers’ intent to install mobile applications},
  author={Harris, Mark A and Brookshire, Robert and Chin, Amita Goyal},
  journal={International Journal of Information Management},
  volume={36},
  number={3},
  pages={441--450},
  year={2016},
  publisher={Elsevier}
}

@inproceedings{harty2021_logging_practices_with_mobile_analytics,
  author={Harty, Julian and Zhang, Haonan and Wei, Lili and Pascarella, Luca and Aniche, Maurício and Shang, Weiyi}, 
  booktitle={2021 IEEE/ACM 8th International Conference on Mobile Software Engineering and Systems (MobileSoft)}, 
  title={Logging Practices with Mobile Analytics: An Empirical Study on Firebase}, 
  year={2021}, 
  volume={}, 
  number={}, 
  pages={56-60}, 
  abstract={Software logs are of great value in both industrial and open-source projects. Mobile analytics logging enables developers to collect logs remotely from their apps running on end user devices at the cost of recording and transmitting logs across the Internet to a centralised infrastructure.This paper makes a first step in characterising logging practices with a widely adopted mobile analytics logging library, namely Firebase Analytics. We provide an empirical evaluation of the use of Firebase Analytics in 57 open-source Android applications by studying the evolution of code-bases to understand: a) the needs-in-common that push practitioners to adopt logging practices on mobile devices, and b) the differences in the ways developers use local and remote logging.Our results indicate mobile analytics logs are less pervasive and less maintained than traditional logging code. Based on our analysis, we believe logging using mobile analytics is more user centered compared to traditional logging, where the latter is mainly used to record information for debugging purposes.}, 
  keywords={}, 
  doi={10.1109/MobileSoft52590.2021.00013}, 
  ISSN={}, 
  month={May},
  address = {Madrid, Spain},
  publisher = {IEEE},
}

@ARTICLE{9397392,
  author={Hort, Max and Kechagia, Maria and Sarro, Federica and Harman, Mark},
  journal={IEEE Transactions on Software Engineering},
  title={A Survey of Performance Optimization for Mobile Applications}, 
  year={2021}, 
  volume={}, 
  number={}, 
  pages={1-1}, 
  abstract={
    Nowadays there is a mobile application for almost everything a user may think of, ranging from paying bills and gathering information to playing games and watching movies. In order to ensure user satisfaction and success of applications, it is important to provide high performant applications. This is particularly important for resource constraint systems such as mobile devices. Thereby, non-functional performance characteristics, such as energy and memory consumption, play an important role for user satisfaction. This paper provides a comprehensive survey of non-functional performance optimization for Android applications. We collected 155 unique publications, published between 2008 and 2020, that focus on the optimization of non-functional performance of mobile applications. We target our search at four performance characteristics, in particular: responsiveness, launch time, memory and energy consumption. For each performance characteristic, we categorize optimization approaches based on the method used in the corresponding publications. Furthermore, we identify research gaps in the literature for future work.},
  keywords={}, 
  doi={10.1109/TSE.2021.3071193}, 
  ISSN={1939-3520},
  month={},
  remarks = {This cites one of my papers, I understand. Worth weaving into the literature review as a recent and credible source.}
}

@article{HSIAO2016342,
title = "Exploring the influential factors in continuance usage of mobile social Apps: Satisfaction, habit, and customer value perspectives",
journal = "Telematics and Informatics",
volume = "33",
number = "2",
pages = "342 - 355",
year = "2016",
issn = "0736-5853",
doi = "https://doi.org/10.1016/j.tele.2015.08.014",
url = "http://www.sciencedirect.com/science/article/pii/S0736585315001136",
author = "Chun-Hua Hsiao and Jung-Jung Chang and Kai-Yu Tang",
keywords = "Social Apps, Continuance intention, Satisfaction, Habit, Customer value perspectives",
abstract = "The emergence of mobile application software (App) has explosively grown in conjunction with the worldwide use of smartphones in recent years. Among numerous categories of mobile Apps, social Apps were one of those with the greatest growth in 2013. Despite abundant research on users’ behavior intention of mobile App usage, few studies have focused on investigating key determinants of users’ continuance intention regarding social Apps. To fill this gap, we integrated customer value perspectives to explore the influential factors in the continuance intention of social App use. Moreover, users’ satisfaction and habit from both the marketing and psychology literature were also incorporated into the research model. A total of 378 valid questionnaires were collected by survey method, and structural equation modeling was employed in the subsequent data analysis. The results indicate that the continuance usage of social Apps is driven by users’ satisfaction, tight connection with others, and hedonic motivation to use the Apps. In addition, full mediation effects of satisfaction and habit were found between perceived usefulness and intention to continue use. These findings extend our understanding of users’ continuance intention in the context of social Apps. Discussion and implications are provided."
}

@article{humble2018_continuous_delivery_sounds_great,
    author = {Humble, Jez},
    title = {Continuous Delivery Sounds Great, but Will It Work Here?},
    year = {2018},
    issue_date = {April 2018},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {61},
    number = {4},
    issn = {0001-0782},
    url = {https://doi.org/10.1145/3173553},
    doi = {10.1145/3173553},
    abstract = {It's not magic, it just requires continuous, daily improvement at all levels.},
    journal = {Commun. ACM},
    month = mar,
    pages = {34–39},
    numpages = {6}
}

@article{Kechagia2015_charting_API_minefield_using_telemetry_data,
    author={Kechagia, Maria
    and Mitropoulos, Dimitris
    and Spinellis, Diomidis},
    title={Charting the API minefield using software telemetry data},
    journal={Empirical Software Engineering},
    year={2015},
    month={Dec},
    day={01},
    volume={20},
    number={6},
    pages={1785-1830},
    abstract={Programs draw significant parts of their functionality through the use of Application Programming Interfaces (APIs). Apart from the way developers incorporate APIs in their software, the stability of these programs depends on the design and implementation of the APIs. In this work, we report how we used software telemetry data to analyze the causes of API failures in Android applications. Specifically, we got 4.9 gb worth of crash data that thousands of applications sent to a centralized crash report management service. We processed that data to extract approximately a million stack traces, stitching together parts of chained exceptions, and established heuristic rules to draw the border between applications and the API calls. We examined a set of more than a half million stack traces associated with risky API calls to map the space of the most common application failure reasons. Our findings show that the top ones can be attributed to memory exhaustion, race conditions or deadlocks, and missing or corrupt resources. Given the classes of the crash causes we identified, we recommend API design and implementation choices, such as specific exceptions, default resources, and non-blocking algorithms, that can eliminate common failures. In addition, we argue that development tools like memory analyzers, thread debuggers, and static analyzers can prevent crashes through early code testing and analysis. Finally, some execution platform and framework designs for process and memory management can also eliminate some application crashes.},
    issn={1573-7616},
    doi={10.1007/s10664-014-9343-7},
    url={https://doi.org/10.1007/s10664-014-9343-7}
}



@article{khalid2016_examining_the_relationship_between_findbugs_warnings_and_app_ratings,  
  author={H. {Khalid} and M. {Nagappan} and A. E. {Hassan}},  
  journal={IEEE Software},
  title={Examining the Relationship between FindBugs Warnings and App Ratings}, year={2016},
  volume={33},
  number={4},
  pages={34-39},
  abstract={
    In the mobile-app ecosystem, user ratings of apps (a measure of user perception) are extremely important because they correlate strongly with downloads and hence revenue. A case study examined the relationship between ratings (and the associated review comments) and static-analysis warnings (collected using FindBugs) for 10,000 free-to-download Android apps. Three warning categories - bad practice, internationalization, and performance - were more frequent in low-rated apps and corresponded to the review comment complaints. Thus, these categories were closely related to the user experience. These results suggest that app developers could use static-analysis tools to identify the bugs behind the issues that users complain about, before releasing an app.},
  keywords={mobile computing;program diagnostics;FindBugs warnings;app ratings;mobile-app ecosystem;static-analysis warnings;free-to-download Android apps;warning categories;low-rated apps;user experience;static-analysis tools;Ecosystems;Androids;Humanoid robots;Computer bugs;Software development;Mobile communication;Computer applications;mobile apps;static analysis;user ratings;software quality assurance;FindBugs;Android;software engineering;software development}, doi={10.1109/MS.2015.29},
  ISSN={1937-4194},
  month={July},
}

@ARTICLE{khalid2015_what_do_mobile_app_users_complain_about,  
  author={H. {Khalid} and E. {Shihab} and M. {Nagappan} and A. E. {Hassan}},
  journal={IEEE Software},
  title={What Do Mobile App Users Complain About?},
  year={2015},
  volume={32},
  number={3},
  pages={70-77},
  abstract={
    Mobile-app quality is becoming an increasingly important issue. These apps are generally delivered through app stores that let users post reviews. These reviews provide a rich data source you can leverage to understand user-reported issues. Researchers qualitatively studied 6,390 low-rated user reviews for 20 free-to-download iOS apps. They uncovered 12 types of user complaints. The most frequent complaints were functional errors, feature requests, and app crashes. Complaints about privacy and ethical issues and hidden app costs most negatively affected ratings. In 11 percent of the reviews, users attributed their complaints to a recent app update. This study provides insight into the user-reported issues of iOS apps, along with their frequency and impact, which can help developers better prioritize their limited quality assurance resources.},  
  keywords={mobile computing;operating systems (computers);mobile App users;mobile app quality;app stores;iOS apps;user complaints;frequent complaints;functional errors;feature requests;app crashes;ethical issues;privacy issues;quality assurance resources;Computer crashes;Privacy;Mobile communication;Tagging;Computer applications;Software quality;Quality assurance;Software engineering;mobile applications;software quality;user reviews;quality assurance;software engineering},
  doi={10.1109/MS.2014.50},
  ISSN={1937-4194},
  month={May},
}

@article{kinshuman2011_debugging_in_the_very_large,
    author = {Kinshumann, Kinshuman and Glerum, Kirk and Greenberg, Steve and Aul, Gabriel and Orgovan, Vince and Nichols, Greg and Grant, David and Loihle, Gretchen and Hunt, Galen},
    title = {Debugging in the (Very) Large: Ten Years of Implementation and Experience},
    year = {2011},
    issue_date = {July 2011},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {54},
    number = {7},
    issn = {0001-0782},
    url = {https://doi.org/10.1145/1965724.1965749},
    doi = {10.1145/1965724.1965749},
    abstract = {Windows Error Reporting (WER) is a distributed system that automates the processing
    of error reports coming from an installed base of a billion machines. WER has collected
    billions of error reports in 10 years of operation. It collects error data automatically
    and classifies errors into buckets, which are used to prioritize developer effort
    and report fixes to users. WER uses a progressive approach to data collection, which
    minimizes overhead for most reports yet allows developers to collect detailed information
    when needed. WER takes advantage of its scale to use error statistics as a tool in
    debugging; this allows developers to isolate bugs that cannot be found at smaller
    scale. WER has been designed for efficient operation at large scale: one pair of database
    servers records all the errors that occur on all Windows computers worldwide.},
    journal = {Commun. ACM},
    month = jul,
    pages = {111–116},
    numpages = {6}
}

@article{kitchenham1996_software_quality_elusive_target,
  title={Software quality: the elusive target [special issues section]},
  author={Kitchenham, Barbara and Pfleeger, Shari Lawrence},
  journal={IEEE software},
  volume={13},
  number={1},
  pages={12--21},
  year={1996},
  publisher={IEEE},
  abstract={If you are a software developer, manager, or maintainer, quality is often on your mind. But what do you really mean by software quality? Is your definition adequate? Is the software you produce better or worse than you would like it to be? We put software quality on trial, examining both the definition and evaluation of our software products and processes.},
}

@article{yet_to_cite_Kong2021_anchor_locating_android_framework_specific_crashing_faults,
author={Kong, Pingfan
and Li, Li
and Gao, Jun
and Riom, Timoth{\'e}e
and Zhao, Yanjie
and Bissyand{\'e}, Tegawend{\'e} F.
and Klein, Jacques},
title={ANCHOR: locating android framework-specific crashing faults},
journal={Automated Software Engineering},
year={2021},
month={Jul},
day={12},
volume={28},
number={2},
pages={10},
abstract={Android framework-specific app crashes are hard to debug. Indeed, the callback-based event-driven mechanism of Android challenges crash localization techniques that are developed for traditional Java programs. The key challenge stems from the fact that the buggy code location may not even be listed within the stack trace. For example, our empirical study on 500 framework-specific crashes from an open benchmark has revealed that 37 percent of the crash types are related to bugs that are outside the stack traces. Moreover, Android programs are a mixture of code and extra-code artifacts such as the Manifest file. The fact that any artifact can lead to failures in the app execution creates the need to position the localization target beyond the code realm. In this paper, we propose Anchor , a two-phase suspicious bug location suggestion tool. Anchor specializes in finding crash-inducing bugs outside the stack trace. Anchor is lightweight and source code independent since it only requires the crash message and the apk file to locate the fault. Experimental results, collected via cross-validation and in-the-wild dataset evaluation, show that Anchor is effective in locating Android framework-specific crashing faults. Finally, we put our empirical study results openly accessible at https://github.com/anchor-locator/anchor.},
issn={1573-7535},
doi={10.1007/s10515-021-00290-1},
url={https://doi.org/10.1007/s10515-021-00290-1}
}

@article{krotov2020_tutorial_legality_and_ethics_of_web_scraping,
  title = {Tutorial: Legality and Ethics of Web Scraping},
  url = {https://aisel.aisnet.org/cais/vol47/iss1/22/},
  source = {https://digitalcommons.murraystate.edu/faculty/86/},
  year = {2020},
  author = {Krotov, Vlad and Johnson, Leigh and Silva, Leiser},
  journal = {Communications of the Association for Information Systems}, 
  volume = {47}, 
  pages = {555--581},
  doi = {https://doi.org/10.17705/1CAIS.04724},
  abstract = {
    Automatic retrieval of data from the Web (often referred to as Web Scraping) for industry and academic research projects is becoming a common practice. A variety of tools and technologies have been developed to facilitate Web Scraping. Unfortunately, the legality and ethics of using these tools for collecting data are often overlooked. Failure to pay due attention to these aspects of Web Scraping can result in serious ethical controversies and lawsuits. This paper reviews legal literature together with the literature on ethics and privacy to identify broad areas of concern together with a list of specific questions that need to be addressed by researchers and practitioners engaged in Web Scraping. Reflecting on these questions and concerns can potentially help the researchers decrease the likelihood of ethical and legal controversies in their work.
  },
}

@article{lee2014_determinants_of_mobile_app_success_evidence_from_the_app_store_market,
    author = { Gunwoong Lee  and T. S. Raghu },
    title = {Determinants of Mobile Apps' Success: Evidence from the App Store Market},
    journal = {Journal of Management Information Systems},
    volume = {31},
    number = {2},
    pages = {133-170},
    year  = {2014},
    publisher = {Routledge},
    doi = {10.2753/MIS0742-1222310206},
    url = {https://doi.org/10.2753/MIS0742-1222310206},
    eprint = {https://doi.org/10.2753/MIS0742-1222310206},
    abstract = {
      Mobile applications markets with app stores have introduced a new approach to define and sell software applications with access to a large body of heterogeneous consumer population. This research examines key seller- and app-level characteristics that impact success in an app store market. We tracked individual apps and their presence in the top-grossing 300 chart in Apple's App Store and examined how factors at different levels affect the apps' survival in the top 300 chart. We used a generalized hierarchical modeling approach to measure sales performance, and confirmed the results with the use of a hazard model and a count regression model. We find that broadening app offerings across multiple categories is a key determinant that contributes to a higher probability of survival in the top charts. App-level attributes such as free app offers, high initial ranks, investment in less-popular (less-competitive) categories, continuous quality updates, and high-volume and high-user review scores have positive effects on apps' sustainability. In general, each diversification decision across a category results in an approximately 15 percent increase in the presence of an app in the top charts. Survival rates for free apps are up to two times more than that for paid apps. Quality (feature) updates to apps can contribute up to a threefold improvement in survival rate as well. A key implication of the results of this study is that sellers must utilize the natural segmentation in consumer tastes offered by the different categories to improve sales performance.},
    related = {lotan2015_apple_apps_and_algorithmic_glitches},    
}

@ARTICLE{lim_investigating_country_differences,
  author={S. L. {Lim} and P. J. {Bentley} and N. {Kanakam} and F. {Ishikawa} and S. {Honiden}},  
  journal={IEEE Transactions on Software Engineering},   
  title={Investigating Country Differences in Mobile App User Behavior and Challenges for Software Engineering},   
  year={2015},  
  volume={41},  
  number={1},  
  pages={40-64},  
  abstract={Mobile applications (apps) are software developed for use on mobile devices and made available through app stores. App stores are highly competitive markets where developers need to cater to a large number of users spanning multiple countries. This work hypothesizes that there exist country differences in mobile app user behavior and conducts one of the largest surveys to date of app users across the world, in order to identify the precise nature of those differences. The survey investigated user adoption of the app store concept, app needs, and rationale for selecting or abandoning an app. We collected data from more than 15 countries, including USA, China, Japan, Germany, France, Brazil, United Kingdom, Italy, Russia, India, Canada, Spain, Australia, Mexico, and South Korea. Analysis of data provided by 4,824 participants showed significant differences in app user behaviors across countries, for example users from USA are more likely to download medical apps, users from the United Kingdom and Canada are more likely to be influenced by price, users from Japan and Australia are less likely to rate apps. Analysis of the results revealed new challenges to market-driven software engineering related to packaging requirements, feature space, quality expectations, app store dependency, price sensitivity, and ecosystem effect.},  keywords={consumer behaviour;mobile computing;smart phones;software engineering;market-driven software engineering;medical applications;data analysis;South Korea;South;Mexico;Australia;Spain;Canada;India;Russia;Italy;United Kingdom;Brazil;France;Germany;Japan;China;USA;applications stores;mobile devices;user behavior;mobile application;Mobile communication;Software;Smart phones;Software engineering;Data mining;Educational institutions;Requirements/specifications;market-driven software engineering;mobile application development;user requirements;survey research;app user behavior;software product lines;software ecosystems;Requirements/specifications;market-driven software engineering;mobile application development;user requirements;survey research;app user behavior;software product lines;software ecosystems},  
  doi={10.1109/TSE.2014.2360674},  
  ISSN={1939-3520},  
  month={Jan},
}

@article{luca2016study,
  title={A study on quality analysis measuring process},
  author={Luca, Liliana},
  journal={Revista Durabilitate si fiabilitate},
  number={2},
  pages={68--72},
  year={2016},
  quote_1 = {An important issue in the measurement process is optimal accuracy. Thus, in practice measurements should be considered the necessary and sufficient optimal precision. A much greater precision imposed in the measuring operation, causes great expense undue operator training and the means of measurement with an insufficient low accuracy, causing a low quality of measurement results.},
  quote_2 = {In [13] it is shown that obtaining a correct [Ishikawa] diagram is possible only through working in a team with experience},
  observations = {This paper's domain is in physical engineering, not software. There are some formatting and content issues in the paper online, it's not clear whether these are because of processing errors in making the paper available. The 10 references to the paper are all from physical engineering domains.},
}

@article{mantyla2014_how_are_software_defects_found,
  title={How are software defects found? The role of implicit defect detection, individual responsibility, documents, and knowledge},
  author={M{\"a}ntyl{\"a}, Mika V and Itkonen, Juha},
  journal={Information and Software Technology},
  volume={56},
  number={12},
  pages={1597--1612},
  year={2014},
  publisher={Elsevier}
}

@article{martin2016survey,
  title={A survey of app store analysis for software engineering},
  author={Martin, William and Sarro, Federica and Jia, Yue and Zhang, Yuanyuan and Harman, Mark},
  journal={IEEE transactions on software engineering},
  volume={43},
  number={9},
  pages={817--847},
  year={2016},
  publisher={IEEE}
}

@ARTICLE{martin2017_survey_in_app_store_analysis_for_software_engineering_IEEE_edition,  
  author={W. {Martin} and F. {Sarro} and Y. {Jia} and Y. {Zhang} and M. {Harman}},  
  journal={IEEE Transactions on Software Engineering},
  title={A Survey of App Store Analysis for Software Engineering},
  year={2017},
  volume={43},
  number={9},
  pages={817-847},
  abstract={
    App Store Analysis studies information about applications obtained from app stores. App stores provide a wealth of information derived from users that would not exist had the applications been distributed via previous software deployment methods. App Store Analysis combines this non-technical information with technical information to learn trends and behaviours within these forms of software repositories. Findings from App Store Analysis have a direct and actionable impact on the software teams that develop software for app stores, and have led to techniques for requirements engineering, release planning, software design, security and testing. This survey describes and compares the areas of research that have been explored thus far, drawing out common aspects, trends and directions future research should take to address open problems and challenges.},
  keywords={Software;Security;Software engineering;Market research;Ecosystems;Mobile communication;Google;App store;analysis;mining;API;feature;release planning;requirements engineering;reviews;security;ecosystem},
  doi={10.1109/TSE.2016.2630689},
  ISSN={1939-3520},
  month={Sep.},
}


@ARTICLE{martinez_fernandez2019_continuously_assessing_and_improving_software_quality_with_software_analytics_tools,  
  author={S. {Martínez-Fernández} and A. M. {Vollmer} and A. {Jedlitschka} and X. {Franch} and L. {López} and P. {Ram} and P. {Rodríguez} and S. {Aaramaa} and A. {Bagnato} and M. {Choraś} and J. {Partanen}},  
  journal={IEEE Access},   
  title={Continuously Assessing and Improving Software Quality With Software Analytics Tools: A Case Study},   
  year={2019},  
  volume={7},  
  number={},  
  pages={68219-68239},  
  abstract={In the last decade, modern data analytics technologies have enabled the creation of software analytics tools offering real-time visualization of various aspects related to software development and usage. These tools seem to be particularly attractive for companies doing agile software development. However, the information provided by the available tools is neither aggregated nor connected to higher quality goals. At the same time, assessing and improving the software quality has also been the key targets for the software engineering community, yielding several proposals for standards and software quality models. Integrating such quality models into software analytics tools could close the gap by providing the connection to higher quality goals. This paper aims at understanding whether the integration of quality models into software analytics tools provides understandable, reliable, useful, and relevant information at the right level of detail about the quality of a process or product and whether practitioners intend to use it. Over the course of more than a year, four companies involved in this case study deployed such a tool to assess and improve software quality in several projects. We used standardized measurement instruments to elicit the perception of 22 practitioners regarding their use of the tool. We complemented the findings with debriefing sessions held at the companies. In addition, we discussed challenges and lessons learned with four practitioners leading the use of the tool. The quantitative and qualitative analyses provided positive results, i.e., the practitioners' perception with regard to the tool's understandability, reliability, usefulness, and relevance was positive. Individual statements support the statistical findings, and constructive feedback can be used for future improvements. We conclude that the potential for future adoption of quality models within software analytics tools definitely exists and encourage other practitioners to use the presented seven challenges and seven lessons learned and adopt them in their companies.},  
  keywords={data analysis;data visualisation;software prototyping;software quality;software reliability;software tools;software analytics tools;agile software development;software quality;software engineering;data analytics technologies;tool understandability;tool reliability;tool usefulness;tool relevance;real-time visualization;Tools;Software quality;Companies;Real-time systems;Monitoring;Agile software development;case study;quality model;software analytics;software analytics tool;software quality},  
  doi={10.1109/ACCESS.2019.2917403},  
  ISSN={2169-3536},  
  month={},
}

@article{GoisMateus2019_an_empirical_study_on_the_quality_of_android_apps_in_kotlin,
  author="G{\'o}is Mateus, Bruno and Martinez, Matias",
  title="An empirical study on quality of Android applications written in Kotlin language",
  journal="Empirical Software Engineering",
  year="2019",
  month="Jun",
  day="25",
  abstract="During the last years, developers of mobile applications have the possibility to use new paradigms and tools for developing mobile applications. For instance, since 2017, Android developers have the official support to write Android applications using Kotlin language. Kotlin is programming language fully interoperable with Java that combines object-oriented and functional features.",
  issn="1573-7616",
  doi="10.1007/s10664-019-09727-4",
  url="https://doi.org/10.1007/s10664-019-09727-4",
  volume = {24},
  issue = {6},
  pages = {3356-3393},
}

@ARTICLE{maalej2016_towards_data_driven_requirements_engineering,
  author={W. {Maalej} and M. {Nayebi} and T. {Johann} and G. {Ruhe}},
  journal={IEEE Software},
  title={Toward Data-Driven Requirements Engineering},
  year={2016},
  volume={33},
  number={1},
  pages={48-54},
  abstract={
    Nowadays, users can easily submit feedback about software products in app stores, social media, or user groups. Moreover, software vendors are collecting massive amounts of implicit feedback in the form of usage data, error logs, and sensor data. These trends suggest a shift toward data-driven user-centered identification, prioritization, and management of software requirements. Developers should be able to adopt the requirements of masses of users when deciding what to develop and when to release. They could systematically use explicit and implicit user data in an aggregated form to support requirements decisions. The goal is data-driven requirements engineering by the masses and for the masses.},
  keywords={Requirements engineering;Software engineering;Stakeholders;Media;Feature extraction;Market research;app reviews;decision support;requirements engineering;software analytics;usage data;software engineering;software development},
  doi={10.1109/MS.2015.153},
  ISSN={1937-4194},
  month={Jan},
}

@article{marsh2002skin,
  title={A skin not a sweater: Ontology and epistemology in political science},
  author={Marsh, David and Furlong, Paul},
  journal={Theory and methods in political science},
  volume={2},
  pages={17--41},
  year={2002},
  publisher={Palgrave Basingstoke}
}

@article{mcilroy2016analyzing,
  title={Analyzing and automatically labelling the types of user issues that are raised in mobile app reviews},
  author={McIlroy, Stuart and Ali, Nasir and Khalid, Hammad and Hassan, Ahmed E},
  journal={Empirical Software Engineering},
  volume={21},
  number={3},
  pages={1067--1106},
  year={2016},
  publisher={Springer}
}

@article{menasria2018_purpose_driven_privacy_preservation_accelerometers,
  title={The purpose driven privacy preservation for accelerometer-based activity recognition},
  author={Menasria, Soumia and Wang, Jianxin and Lu, Mingming},
  journal={World Wide Web},
  volume={21},
  number={6},
  pages={1773--1785},
  year={2018},
  publisher={Springer},
  doi={10.1007/s11280-018-0604-z},
  abstract = {Accelerometer-based activity recognition (AAR) attracted a lot of attentions due to the wide spread of smartphones with energy-efficiency. However, since accelerometer data contains individual characteristics; AAR might raise privacy concerns. Although numerous privacy preservation approaches, such as ”privacy filtering, differential privacy, and inferential privacy”, have been proposed to conceal sensitive information, unfortunately they cannot address the privacy problem associated with AAR. In this paper, we report our efforts to control the use of the AAR while preserving the privacy. To achieve this task, our method leverages a connection to agglomerative information bottleneck, through which the amount of disclosed data can be compressed so that irrelevant private information can be reduced, and a connection to general privacy statistical inference framework, where both of the privacy leakage and utility accuracy are considered as mutual information. Our experimental results have shown that the proposed solution can greatly reduce privacy leakage while maintaining a relative good utility.},
}

@article{menzies2013_software_analytics_so_what,
  author={Menzies, Tim and Zimmermann, Thomas},
  journal={IEEE Software}, 
  title={Software Analytics: So What?}, 
  year={2013}, 
  volume={30},
  number={4}, 
  pages={31-37}, 
  abstract={
    The guest editors of this special issue of IEEE Software invited submissions that reflected the benefits (and drawbacks) of software analytics, an area of explosive growth. They had so many excellent submissions that they had to split this special issue into two volumes--you'll see even more content in the September/October issue. They divided the articles on conceptual grounds, so both volumes will feature equally excellent work. The Web extra at http://youtu.be/nO6X0azR0nw is a video interview in which IEEE Software editor in chief Forrest Shull speaks with Tim Menzies about the growing importance of software analytics.},  
  keywords={}, 
  doi={10.1109/MS.2013.86}, 
  ISSN={1937-4194}, 
  month={July},
}

@ARTICLE {menzies2018_unreasonable_effectiveness_of_software_analytics,
    author = {Tim Menzies},
    journal = {IEEE Software},
    title = {The Unreasonable Effectiveness of Software Analytics},
    year = {2018},
    volume = {35},
    number = {02},
    issn = {1937-4194},
    pages = {96-98},
    keywords = {software;complexity theory;software engineering;scrum (software development);frequency selective surfaces;task analysis;software tools},
    doi = {10.1109/MS.2018.1661323},
    publisher = {IEEE Computer Society},
    address = {Los Alamitos, CA, USA},
    month = {mar}
}


@article{menzies2019_badsmells_in_software_analytics,
  title={“Bad smells” in software analytics papers},
  author={Menzies, Tim and Shepperd, Martin},
  journal={Information and Software Technology},
  volume={112},
  pages={35--47},
  year={2019},
  publisher={Elsevier},
  abstract = {
    Context: There has been a rapid growth in the use of data analytics to underpin evidence-based software engineering. However the combination of complex techniques, diverse reporting standards and poorly understood underlying phenomena are causing some concern as to the reliability of studies.

    Objective: Our goal is to provide guidance for producers and consumers of software analytics studies (computational experiments and correlation studies).

    Method: We propose using “bad smells”, i.e., surface indications of deeper problems and popular in the agile software community and consider how they may be manifest in software analytics studies.

    Results: We list 12 “bad smells” in software analytics papers (and show their impact by examples).

    Conclusions: We believe the metaphor of bad smell is a useful device. Therefore we encourage more debate on what contributes to the validity of software analytics studies (so we expect our list will mature over time).
  }
}

@ARTICLE{miller2013_from_data_to_decisions_a_value_chain_for_big_data,  
  author={Miller, H. Gilbert and Mork, Peter}, 
  journal={IT Professional}, 
  title={From Data to Decisions: A Value Chain for Big Data}, 
  year={2013},
  volume={15}, 
  number={1}, 
  pages={57-59}, 
  abstract={
    With exponential growth in data, enterprises must act to make the most of the vast data landscape-to thoughtfully apply multiple technologies, carefully select key data for specific investigations, and innovatively tailor large integrated datasets to support specific queries and analyses. All these actions will flow from a data value chain-a framework to manage data holistically from capture to decision making and to support a variety of stakeholders and their technologies.
  }, 
  keywords={},
  doi={10.1109/MITP.2013.11}, 
  ISSN={1941-045X}, 
  month={Jan},
}

@ARTICLE {mills1987_cleanroom_software_engineering,
    author = {H. Mills and R. Linger and M. Dyer},
    journal = {IEEE Software},
    title = {Cleanroom Software Engineering},
    year = {1987},
    volume = {4},
    number = {05},
    issn = {1937-4194},
    pages = {19-25},
    keywords = {null},
    doi = {10.1109/MS.1987.231413},
    publisher = {IEEE Computer Society},
    address = {Los Alamitos, CA, USA},
    month = {sep}
}

@article{murphy2004_automating_software_failure_reporting,
    author = {Murphy, Brendan},
    title = {Automating Software Failure Reporting: We Can Only Fix Those Bugs We Know About.},
    year = {2004},
    issue_date = {November 2004},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {2},
    number = {8},
    issn = {1542-7730},
    url = {https://doi.org/10.1145/1036474.1036498},
    doi = {10.1145/1036474.1036498},
    abstract = {There are many ways to measure quality before and after software is released. For
    commercial and internal-use-only products, the most important measurement is the user’s
    perception of product quality. Unfortunately, perception is difficult to measure,
    so companies attempt to quantify it through customer satisfaction surveys and failure/behavioral
    data collected from its customer base. This article focuses on the problems of capturing
    failure data from customer sites. To explore the pertinent issues I rely on experience
    gained from collecting failure data from Windows XP systems, but the problems you
    are likely to face when developing internal (noncommercial) software should not be
    dissimilar.},
    journal = {Queue},
    month = nov,
    pages = {42–48},
    numpages = {7}
}

@ARTICLE{musa1990_software_reliability_engineering_technology_for_the_1990s,  
  author={Musa, J.D. and Everett, W.W.},  
  journal={IEEE Software},  
  title={Software-reliability engineering: technology for the 1990s}, 
  year={1990}, 
  volume={7}, 
  number={6}, 
  pages={36-43}, 
  abstract={
    It argued that software engineering is about to reach a new stage, the reliability stage, that stresses customers' operational needs and that software-reliability engineering will make this stage possible. Software-reliability engineering is defined as the applied science of predicting, measuring, and managing the reliability of software-based systems to maximize customer satisfaction. The basic concepts of software reliability-engineering and the reasons why it is important are examined. The application of software-reliability engineering at each stage of the life cycle is described.}, 
  keywords={},
  doi={10.1109/52.60588},
  ISSN={1937-4194},
  month={Nov},
}

@ARTICLE{musa1993_operational_profiles,  
    author = {J. D. {Musa}},  
    journal = {IEEE Software},   
    title = {Operational profiles in software-reliability engineering},
    year = {1993},  
    volume = {10},  
    number = {2},  
    pages = {14-32},
    
    abstract = {A systematic approach to organizing the process of determining the operational profile for guiding software development is presented. The operational profile is a quantitative characterization of how a system will be used that shows how to increase productivity and reliability and speed development by allocating development resources to function on the basis of use. Using an operational profile to guide testing ensures that if testing is terminated and the software is shipped because of schedule constraints, the most-used operations will have received the most testing and the reliability level will be the maximum that is practically achievable for the given test time. For guiding regression testing, it efficiently allocates test cases in accordance with use, so the faults most likely to be found, of those introduced by changes, are the ones that have the most effect on reliability.<>},
    
    keywords = {program testing;software reliability;software-reliability engineering;operational profile;guiding software development;quantitative characterization;productivity;speed development;schedule constraints;regression testing;Costs;Reliability engineering;Marketing and sales;Automatic testing;Productivity;Resource management;Capacitive sensors;Switching systems;Customer satisfaction;Operating systems},  doi = {10.1109/52.199724},  
    ISSN = {1937-4194},  
    month = {March},
}

 @article{_duplicated_in_logging_paper_Oliveira_Borges_Silva_Cacho_Castor_2018_android_error_handling, 
   title={Do android developers neglect error handling? a maintenance-Centric study on the relationship between android abstractions and uncaught exceptions},
   volume={136},
   ISSN={0164-1212},
   DOI={https://doi.org/10.1016/j.jss.2017.10.032},
   abstractNote={All the mainstream programming languages in widespread use for mobile app development provide error handling mechanisms to support the implementation of robust apps. Android apps, in particular, are usually written in the Java programming language. Java includes an exception handling mechanism that allows programs to signal the occurrence of errors by throwing exceptions and to handle these exceptions by catching them. All the Android-specific abstractions, such as activities and asynctasks, can throw exceptions when errors occur. When an app catches the exceptions that it or the libraries upon which it depends throw, it can resume its activity or, at least, fail in a graceful way. On the other hand, uncaught exceptions can lead an app to crash, particularly if they occur within the main thread. Previous work has shown that, in real Android apps available at the Play Store, uncaught exceptions thrown by Android-specific abstractions often cause these apps to fail. This paper presents an empirical study on the relationship between the usage of Android abstractions and uncaught exceptions. Our approach is quantitative and maintenance-centric. We analyzed changes to both normal and exception handling code in 112 versions extracted from 16 software projects covering a number of domains, amounting to more than 3 million LOC. Change impact analysis and exception flow analysis were performed on those versions of the projects. The main finding of this study is that, during the evolution of the analyzed apps, an increase in the use of Android abstractions exhibits a positive and statistically significant correlation with the number of uncaught exception flows. Since uncaught exceptions cause apps to crash, this result suggests that these apps are becoming potentially less robust as a consequence of exception handling misuse. Analysis of multiple versions of these apps revealed that Android developers usually employ abstractions that may throw exceptions without adding the appropriate handlers for these exceptions. This study highlights the need for better testing and verification tools with a focus on exception handling code and for a change of culture in Android development or, at least, in the design of its APIs.},
   journal={Journal of Systems and Software},
   author={Oliveira, Juliana and Borges, Deise and Silva, Thaisa and Cacho, Nelio and Castor, Fernando},
   year={2018},
   pages={1–18}
}


@article{ormen2015_smartphone_log_data_qualitative_perspective,
  title={Smartphone log data in a qualitative perspective},
  author={{\O}rmen, Jacob and Thorhauge, Anne Mette},
  journal={Mobile Media \& Communication},
  volume={3},
  number={3},
  pages={335--350},
  year={2015},
  publisher={SAGE Publications Sage UK: London, England},
  abstract = {Log data from smartphones have primarily been used in large-scale research designs to draw statistical inferences from hundreds or even thousands of participants. In this article, we argue that more qualitatively oriented designs can also benefit greatly from integrating these rich data sources into studies of smartphones in everyday life. Through an illustrative study, we explore a more nuanced perspective on what can be considered “log data” and how these types of data can be collected and analysed. A qualitative approach to log data analysis offers researchers new opportunities to situate smartphone use within people’s practices, norms, and routines. This is of relevance both to studies focusing on smartphones as cultural objects in everyday life and studies that use such devices as proxies for social behaviour more generally. We argue that log data, for instance in in-depth interviews, may serve as cues to instigate discussion and reflection as well as act as resources for contextualizing and organizing related empirical material. In the discussion, the advantages of a qualitative perspective for research designs are assessed in relation to issues of validity. Further perspectives on the promises of log data from various devices are proposed.},
}

@inproceedings{padmanabha2018_mitigating_the_latency_accuracy_tradeoff_in_moile_data_analytics_systems,
    author = {Padmanabha Iyer, Anand and Erran Li, Li and Chowdhury, Mosharaf and Stoica, Ion},
    title = {Mitigating the Latency-Accuracy Trade-off in Mobile Data Analytics Systems},
    year = {2018},
    isbn = {9781450359030},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3241539.3241581},
    doi = {10.1145/3241539.3241581},
    abstract = {
      An increasing amount of mobile analytics is performed on data that is procured in a real-time fashion to make real-time decisions. Such tasks include simple reporting on streams to sophisticated model building. However, the practicality of these analyses are impeded in several domains because they are faced with a fundamental trade-off between data collection latency and analysis accuracy. In this paper, we first study this trade-off in the context of a specific domain, Cellular Radio Access Networks (RAN). We find that the trade-off can be resolved using two broad, general techniques: intelligent data grouping and task formulations that leverage domain characteristics. Based on this, we present CellScope, a system that applies a domain specific formulation and application of Multi-task Learning (MTL) to RAN performance analysis. It uses three techniques: feature engineering to transform raw data into effective features, a PCA inspired similarity metric to group data from geographically nearby base stations sharing performance commonalities, and a hybrid online-offline model for efficient model updates. Our evaluation shows that CellScope's accuracy improvements over direct application of ML range from 2.5\texttimes{} to 4.4\texttimes{} while reducing the model update overhead by up to 4.8\texttimes{}. We have also used CellScope to analyze an LTE network of over 2 million subscribers, where it reduced troubleshooting efforts by several magnitudes. We then apply the underlying techniques in CellScope to another domain specific problem, mobile phone energy bug diagnosis, and show that the techniques are general.
    },
    booktitle = {Proceedings of the 24th Annual International Conference on Mobile Computing and Networking},
    pages = {513–528},
    numpages = {16},
    keywords = {mobile systems, data analytics, cellular networks},
    location = {New Delhi, India},
    series = {MobiCom '18}
}

@article{palvia2001socio,
  title={A socio-technical framework for quality assessment of computer information systems},
  author={Palvia, Shailendra C and Sharma, Ravi S and Conrath, David W},
  journal={Industrial Management \& Data Systems},
  year={2001},
  publisher={MCB UP Ltd},
  key_point_1 = {They use an Ishikawa diagram related to software quality},
  key_point_2 = {The paper makes 5 initial claims which still seem relevant today},
  key_point_3 = {They use a definition of Quality from ISO9000},
}

@article{petsas2017measurement,
  title={Measurement, modeling, and analysis of the mobile app ecosystem},
  author={Petsas, Thanasis and Papadogiannakis, Antonis and Polychronakis, Michalis and Markatos, Evangelos P and Karagiannis, Thomas},
  journal={ACM Transactions on Modeling and Performance Evaluation of Computing Systems (TOMPECS)},
  doi = {https://doi.org/10.1145/2993419},
  volume={2},
  number={2},
  pages={1--33},
  year={2017},
  publisher={ACM New York, NY, USA},
  abstract={
    Mobile applications (apps) have been gaining popularity due to the advances in mobile technologies and the large increase in the number of mobile users. Consequently, several app distribution platforms, which provide a new way for developing, downloading, and updating software applications in modern mobile devices, have recently emerged. To better understand the download patterns, popularity trends, and development strategies in this rapidly evolving mobile app ecosystem, we systematically monitored and analyzed four popular third-party Android app marketplaces. Our study focuses on measuring, analyzing, and modeling the app popularity distribution and explores how pricing and revenue strategies affect app popularity and developers’ income.

    Our results indicate that unlike web and peer-to-peer file sharing workloads, the app popularity distribution deviates from commonly observed Zipf-like models. We verify that these deviations can be mainly attributed to a new download pattern, which we refer to as the clustering effect. We validate the existence of this effect by revealing a strong temporal affinity of user downloads to app categories. Based on these observations, we propose a new formal clustering model for the distribution of app downloads and demonstrate that it closely fits measured data. Moreover, we observe that paid apps follow a different popularity distribution than free apps and show how free apps with an ad-based revenue strategy may result in higher financial benefits than paid apps. We believe that this study can be useful to appstore designers for improving content delivery and recommendation systems, as well as to app developers for selecting proper pricing policies to increase their income.},
}

@article{parasuraman_complacency_and_bias_in_human_use_of_automation,
    author = {Raja Parasuraman and Dietrich H. Manzey},
    title ={Complacency and Bias in Human Use of Automation: An Attentional Integration},
    journal = {Human Factors},
    volume = {52},
    number = {3},
    pages = {381-410},
    year = {2010},
    doi = {10.1177/0018720810376055},
    note ={PMID: 21077562},
    URL = {https://doi.org/10.1177/0018720810376055},
    eprint = {https://doi.org/10.1177/0018720810376055},
    abstract = { Objective: Our aim was to review empirical studies of complacency and bias in human interaction with automated and decision support systems and provide an integrated theoretical model for their explanation.Background: Automation-related complacency and automation bias have typically been considered separately and independently.Methods: Studies on complacency and automation bias were analyzed with respect to the cognitive processes involved.Results: Automation complacency occurs under conditions of multiple-task load, when manual tasks compete with the automated task for the operator’s attention. Automation complacency is found in both naive and expert participants and cannot be overcome with simple practice. Automation bias results in making both omission and commission errors when decision aids are imperfect.Automation bias occurs in both naive and expert participants, cannot be prevented by training or instructions, and can affect decision making in individuals as well as in teams.While automation bias has been conceived of as a special case of decision bias, our analysis suggests that it also depends on attentional processes similar to those involved in automation-related complacency.Conclusion: Complacency and automation bias represent different manifestations of overlapping automation-induced phenomena, with attention playing a central role. An integrated model of complacency and automation bias shows that they result from the dynamic interaction of personal, situational, and automation-related characteristics.Application: The integrated model and attentional synthesis provides a heuristic framework for further research on complacency and automation bias and design options for mitigating such effects in automated and decision support systems. }
}

 @article{pfleeger2000_risky_business, 
   title={Risky business: what we have yet to learn about risk management},
   volume={53},
   ISSN={0164-1212},
   DOI={https://doi.org/10.1016/S0164-1212(00)00017-0},
   abstractNote={
     This paper examines the way in which software practitioners are taught to perform risk management, and compares it with risk management in other fields. We find that there are three major problems with risk management: false precision, bad science, and the confusion of facts with values. All of these problems can lead to bad decisions, all in the guise of more objective decision-making. But we can learn from these problems and improve the way we do risk management.}, 
   number={3},
   journal={Journal of Systems and Software},
   author={Pfleeger, Shari Lawrence},
   year={2000},
   pages={265–273},
   url = {http://www.sciencedirect.com/science/article/pii/S0164121200000170},
   quotes = {
     `We must personalize the quantified risk to make it meaningful.',
   },
}

@article{ransdell1993educational_software_evaluation_research_validities,
  title={Educational software evaluation research: Balancing internal, external, and ecological validity},
  author={Ransdell, Sarah},
  journal={Behavior Research Methods, Instruments, \& Computers},
  volume={25},
  number={2},
  pages={228--232},
  year={1993},
  publisher={Springer},
  doi = {https://doi.org/10.3758/BF03204502},
  abstract = {The difficulties inherent in the evaluation of educational software are described in terms of the tradeoffs between internal, external, and ecological validity. Larger issues in evaluation research design and computer-based instruction are highlighted by primary and metaanalytic studies designed to reveal the effects of computer simulations in psychology classrooms and laboratories. The effectiveness of classroom and laboratory computer activities depends on how the inclusion of software, as well as the evaluation process itself, changes the entire instructional process.},
}


@article{razaghpanah2018apps,
  title={Apps, trackers, privacy, and regulators: A global study of the mobile tracking ecosystem},
  abstract={Third-party services form an integral part of the mobile ecosystem: they ease application development and enable features such as analytics, social network integration, and app monetization through ads. However, aided by the general opacity of mobile systems, such services are also largely invisible to users. This has negative consequences for user privacy as third-party services can potentially track users without their consent, even across multiple applications. Using real-world mobile traffic data gathered by the Lumen Privacy Monitor (Lumen), a privacy enhancing app with the ability to analyze network traffic on mobile devices in user space, we present insights into the mobile advertising and tracking ecosystem and its stakeholders. In this study, we develop automated methods to detect third-party advertising and tracking services at the traffic level. Using this technique we identify 2,121 such services, of which 233 were previously unknown to other popular advertising and tracking blacklists. We then uncover the business relationships between the providers of these services and characterize them by their prevalence in the mobile and Web ecosystem. Our analysis of the privacy policies of the largest advertising and tracking service providers shows that sharing harvested data with subsidiaries and third-party affiliates is the norm. Finally, we seek to identify the services likely to be most impacted by privacy regulations such as the European General Data Protection Regulation (GDPR) and ePrivacy directives.},
  author={Razaghpanah, Abbas and Nithyanand, Rishab and Vallina-Rodriguez, Narseo and Sundaresan, Srikanth and Allman, Mark and Kreibich, Christian and Gill, Phillipa},
  year={2018}
}

@article{rennie1998_freedom_and_responsibility_in_medical_publication,
    author = {Rennie, Drummond},
    title = "{Freedom and Responsibility in Medical Publication: Setting the Balance Right}",
    journal = {JAMA},
    volume = {280},
    number = {3},
    pages = {300-302},
    year = {1998},
    month = {07},
    abstract = "{
      I wish to discuss changes that we might make that would improve the ethical climate of the publication of research by making it both more open and more responsible. While none of the systems I briefly discuss have yet become common practice, they are all being tried in various ways, and they indicate the direction in which we ought to move.In the future, the following 4 systems will, I hope, be routine. First, authorship will be abolished in favor of contribution, and the work done by the contributors will be listed for the readers. Second, peer review will become open, and we will come to talk about anonymous review as a quaint anachronism. Third, scientists will take full responsibility for the aftercare of their papers, and, fourth, as a result, editors will enable and encourage readers to assume the responsibilities of reviewers. Changing our practice would in each case rectify the balance between rights and duties.}",
    issn = {0098-7484},
    doi = {10.1001/jama.280.3.300},
    url = {https://doi.org/10.1001/jama.280.3.300},
    eprint = {https://jamanetwork.com/journals/jama/articlepdf/187765/jpv71038.pdf},
}

@online{reid2012_iso29119_eurostar,
  title={ISO/IEC/IEEE 29119: the new international software testing standards},
  author={Reid, Stuart},
  year={2012},
  organization={Testing Solutions Group},
  address={London, UK},
  publisher={EuroSTAR Conferences},
}

@article{rowley2002_using_case_studies_in_research,
  title={Using case studies in research},
  author={Rowley, Jennifer},
  journal={Management research news},
  year={2002},
  pages = {16--27},
  volume = {25},
  number = {1},
  publisher={MCB UP Ltd},
  doi = {10.1108/01409170210782990},
  abstract = {
    Draws heavily on previous established research in an attempt to distil the key aspects of case study research in such a way as to encourage new researchers to grapple with and apply these. Explains when case study can be used, research design, data collection and data analysis, offering suggestions for drawing on the evidence in writing a report or dissertation. Briefly reviews alternative perspectives on the subject.
  }
}

@ARTICLE{saltzer2020_the_origin_of_the_mit_license,
  author={Saltzer, Jerome H.}, 
  journal={IEEE Annals of the History of Computing},
  title={The Origin of the “MIT License”},  
  year={2020},
  volume={42},
  number={4}, 
  pages={94-98},
  abstract={
    Discusses the origin and history of the “MIT License.” This license has become a popular way of releasing copyrighted computer programs for others to use without requiring signatures or a license fee. The quoted words refer to a group of software licenses that have common ancestry and guiding principles but varying wordings. There has been some recent discussion about the origin of the “MIT License,” but that discussion has been inconclusive because authoritative historical documentation has been missing or hard to find. This note provides some of that history and documentation. Being composed 35 years after the events involved, it relies primarily on my often-flaky recollections but it also offers some relevant supporting documentation found in my files and in online archives.},
  keywords={}, 
  doi={10.1109/MAHC.2020.3020234}, 
  ISSN={1934-1547}, 
  month={Oct},
  publisher = {IEEE},
}

@comment{Not yet cited the following},
@article{schelter2018_automating_large_scale_data_quality_verification,
author = {Schelter, Sebastian and Lange, Dustin and Schmidt, Philipp and Celikel, Meltem and Biessmann, Felix and Grafberger, Andreas},
title = {Automating Large-Scale Data Quality Verification},
year = {2018},
issue_date = {August 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3229863.3229867},
doi = {10.14778/3229863.3229867},
abstract = {Modern companies and institutions rely on data to guide every single business process and decision. Missing or incorrect information seriously compromises any decision process downstream. Therefore, a crucial, but tedious task for everyone involved in data processing is to verify the quality of their data. We present a system for automating the verification of data quality at scale, which meets the requirements of production use cases. Our system provides a declarative API, which combines common quality constraints with user-defined validation code, and thereby enables 'unit tests' for data. We efficiently execute the resulting constraint validation workload by translating it to aggregation queries on Apache Spark. Our platform supports the incremental validation of data quality on growing datasets, and leverages machine learning, e.g., for enhancing constraint suggestions, for estimating the 'predictability' of a column, and for detecting anomalies in historic data quality time series. We discuss our design decisions, describe the resulting system architecture, and present an experimental evaluation on various datasets.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1781–1794},
numpages = {14}
}

@article{schneble2018_cambridge_analytica_affair,
  title={The Cambridge Analytica affair and Internet-mediated research},
  author={Schneble, Christophe Olivier and Elger, Bernice Simone and Shaw, David},
  journal={EMBO reports},
  volume={19},
  number={8},
  pages={e46579},
  year={2018}
}

@article{sen2008_determinants_for_foss_license,
  author = { Ravi   Sen  and  Chandrasekar   Subramaniam  and  Matthew L.   Nelson },
  title = {Determinants of the Choice of Open Source Software License},
  journal = {Journal of Management Information Systems},
  volume = {25},
  number = {3},
  pages = {207-240},
  year  = {2008},
  publisher = {Routledge},
  doi = {10.2753/MIS0742-1222250306},
  URL = {https://doi.org/10.2753/MIS0742-1222250306},
  eprint = {https://doi.org/10.2753/MIS0742-1222250306},
  abstract = { 
    In this paper, we examine how the motivations and attitudes of open source software (OSS) developers affect their preference among the three common OSS license types—Strong-Copyleft, Weak-Copyleft, and Non-Copyleft. Despite the importance of the license type and developers to OSS projects, there is little understanding in open source literature of the license choice from a developer's perspective. The results from our empirical study of OSS developers reveal that the intrinsic motivation of challenge (problem solving) is associated with the developers' preference for licenses with moderate restrictions, while the extrinsic motivation of status (through peer recognition) is associated with developers' preference for licenses with least restrictions. We also find that when choosing an OSS license, a developer's attitude toward the software redistribution rights conflicts with his or her attitude toward preserving the social benefits of open source. A major implication of our findings is that OSS managers who want to attract a limited number of highly skilled programmers to their open source project should choose a restrictive OSS license. Similarly, managers of software projects for social programs could attract more developers by choosing a restrictive OSS license.},
}

@article{shahin2019empirical_study_architecting_cd,
  title={An empirical study of architecting for continuous delivery and deployment},
  author={Shahin, Mojtaba and Zahedi, Mansooreh and Babar, Muhammad Ali and Zhu, Liming},
  journal={Empirical Software Engineering},
  volume={24},
  number={3},
  pages={1061--1108},
  year={2019},
  publisher={Springer},
  doi={10.1007/s10664-018-9651-4},
  abstract = {Recently, many software organizations have been adopting Continuous Delivery and Continuous Deployment (CD) practices to develop and deliver quality software more frequently and reliably. Whilst an increasing amount of the literature covers different aspects of CD, little is known about the role of software architecture in CD and how an application should be (re-) architected to enable and support CD. We have conducted a mixed-methods empirical study that collected data through in-depth, semi-structured interviews with 21 industrial practitioners from 19 organizations, and a survey of 91 professional software practitioners. Based on a systematic and rigorous analysis of the gathered qualitative and quantitative data, we present a conceptual framework to support the process of (re-) architecting for CD. We provide evidence-based insights about practicing CD within monolithic systems and characterize the principle of “small and independent deployment units” as an alternative to the monoliths. Our framework supplements the architecting process in a CD context through introducing the quality attributes (e.g., resilience) that require more attention and demonstrating the strategies (e.g., prioritizing operations concerns) to design operations-friendly architectures. We discuss the key insights (e.g., monoliths and CD are not intrinsically oxymoronic) gained from our study and draw implications for research and practice.},
}

@article{shan2016finding,
  title={Finding resume and restart errors in Android applications},
  author={Shan, Zhiyong and Azim, Tanzirul and Neamtiu, Iulian},
  journal={ACM SIGPLAN Notices},
  volume={51},
  number={10},
  pages={864--880},
  year={2016},
  publisher={ACM New York, NY, USA},
  doi={https://doi.org/10.1145/3022671.2984011},
  abstract={Smartphone apps create and handle a large variety of ``instance'' data that has to persist across runs, such as the current navigation route, workout results, antivirus settings, or game state. Due to the nature of the smartphone platform, an app can be paused, sent into background, or killed at any time. If the instance data is not saved and restored between runs, in addition to data loss, partially-saved or corrupted data can crash the app upon resume or restart. While smartphone platforms offer API support for data-saving and data-retrieving operations, the use of this API is ad-hoc: left to the programmer, rather than enforced by the compiler. We have observed that several categories of bugs---including data loss, failure to resume/restart or resuming/restarting in the wrong state---are due to incorrect handling of instance data and are easily triggered by just pressing the `Home' or `Back' buttons. To help address this problem, we have constructed a tool chain for Android (the KREfinder static analysis and the KREreproducer input generator) that helps find and reproduce such incorrect handling. We have evaluated our approach by running the static analysis on 324 apps, of which 49 were further analyzed manually. Results indicate that our approach is (i) effective, as it has discovered 49 bugs, including in popular Android apps, and (ii) efficient, completing on average in 61 seconds per app. More generally, our approach helps determine whether an app saves too much or too little state.},
}

@article{shore2004_fail_fast_software_debugging,
  title={Fail fast [software debugging]},
  author={Shore, Jim},
  journal={IEEE Software},
  volume={21},
  number={5},
  pages={21--25},
  year={2004},
  publisher={IEEE},
  abstract={
    The most annoying aspect of software development is debugging. We don't mind the kinds of bugs that yield to a few minutes inspection. The bugs we hate are the ones that show up only after hours of successful operation, under unusual circumstances, or whose stack traces lead to dead ends. Fortunately, there's a simple technique that dramatically reduces the number of these bugs in our software. It won't reduce the overall number of bugs, at least not at first, but it'll make most defects much easier to find. The technique is to build our software to "fail fast".
  },  
  keywords={}, 
  doi={10.1109/MS.2004.1331296}, 
  ISSN={1937-4194},
  month={Sep.},
}

@article{sirianni1979_caesars_decision_to_cross_the_rubicon,
 ISSN = {07702817, 22959076},
 URL = {http://www.jstor.org/stable/41651462},
 online_copy = {https://www.persee.fr/doc/antiq_0770-2817_1979_num_48_2_1955},
 author = {Frank A. Sirianni},
 journal = {L'Antiquité Classique},
 number = {2},
 pages = {636--638},
 publisher = {L'Antiquité Classique},
 title = {CAESAR'S DECISION TO CROSS THE RUBICON},
 volume = {48},
 year = {1979}
}



@article{slany2014tinkering,
  title={Tinkering with Pocket Code, a Scratch-like programming app for your smartphone},
  author={Slany, Wolfgang},
  journal={Proceedings of Constructionism},
  year={2014},
  address={Vienna, Austria},
  numpages={10},
  number={3},
  volume={N/A},
}

@article{Sokolf6426_2013_first_do_no_harm_revisited,
	author = {Sokol, Daniel K},
	title = {{\textquotedblleft}First do no harm{\textquotedblright} revisited},
	volume = {347},
	elocation-id = {f6426},
	year = {2013},
	doi = {10.1136/bmj.f6426},
	publisher = {BMJ Publishing Group Ltd},
	URL = {https://www.bmj.com/content/347/bmj.f6426},
	eprint = {https://www.bmj.com/content/347/bmj.f6426.full.pdf},
	journal = {BMJ}
}
@article{Schuenemann2011_guidelines2_0_do_no_net_harm,
    author={Sch{\"u}nemann, Holger J.},
    title={Guidelines 2.0: Do No Net Harm---The Future of Practice Guideline Development in Asthma and Other Diseases},
    journal={Current Allergy and Asthma Reports},
    year={2011},
    month={Mar},
    day={17},
    volume={11},
    number={3},
    pages={261},
    abstract={Decisions are like double-edged swords: they always come with benefits and downsides. That is, any decision in life bears desirable and undesirable consequences, even if the latter only involves the time it takes to make or think about the decision, which can be considered the harm of decision making. Therefore, it is impossible to adhere to the Hippocratic Oath's concept of ``primum non nocere,'' which is frequently interpreted as ``never do harm.'' The guiding principle for health care decision making should be to ensure that there is, in summary, more benefit than harm---in other words, ``to do no net harm'' (``primum non net nocere''). Practice guidelines support decision making and, as a consequence, would require the explicit consideration of both desirable and undesirable consequences, and assigning due considerations depending on the magnitude and importance of the consequences. The Grading of Recommendations Assessment, Development and Evaluation (GRADE) Working Group (http://www.gradeworkinggroup.org) has made these considerations more explicit when developing health care recommendations. This article briefly summarizes the work of the GRADE working group based on examples of its application in the field of allergy and asthma, and provides an outlook for advances in the field of guideline development. These developments focus on funding of guidelines and handling conflict of interest, working with observational and diagnostic test accuracy studies, developing appropriate group processes, and the integration of values and preferences in the formulation of recommendations.},
    issn={1534-6315},
    doi={10.1007/s11882-011-0185-8},
    url={https://doi.org/10.1007/s11882-011-0185-8}
}



@ARTICLE {tantithamthavorn2021_actionable_analytics_tell_me_what_to_do,
    author = {C. Tantithamthavorn and J. Jiarpakdee and J. Grundy},
    journal = {IEEE Software},
    title = {Actionable Analytics: Stop Telling Me What It Is; Please Tell Me What To Do},
    year = {2021},
    volume = {38},
    number = {04},
    issn = {1937-4194},
    pages = {115-120},
    keywords = {project management;software development management;decision making;analytical models},
    doi = {10.1109/MS.2021.3072088},
    publisher = {IEEE Computer Society},
    address = {Los Alamitos, CA, USA},
    month = {jul}
}


@article{taylor2016no,
  title={No place to hide? The ethics and analytics of tracking mobility using mobile phone data},
  author={Taylor, Linnet},
  journal={Environment and Planning D: Society and Space},
  abstract={This paper examines the ethical and methodological problems with tracking human mobility using data from mobile phones, focusing on research involving low- and middle-income countries. Such datasets are becoming accessible to an increasingly broad community of researchers and data scientists, with a variety of analytical and policy uses proposed. This paper provides an overview of the state of the art in this area of research, then sets out a new analytical framework for such data sources that focuses on three pressing issues: first, interpretation and disciplinary bias; second, the potential risks to data subjects in low- and middle-income countries and possible ethical responses; and third, the likelihood of ‘function creep’ from benign to less benign uses. Using the case study of a data science challenge involving West African mobile phone data, I argue that human mobility is becoming legible in new, more detailed ways, and that this carries with it the dual risk of rendering certain groups invisible and of misinterpreting what is visible. Thus, this emerging ability to track movement in real time offers both the possibility of improved responses to conflict and forced migration, but also unprecedented power to surveil and control unwanted population movement.},
  volume={34},
  number={2},
  pages={319--336},
  year={2016},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{unterkalmsteiner2012_evaluation_and_measurement_of_spi_a_slr,  
  author={Unterkalmsteiner, Michael and Gorschek, Tony and Islam, A.K.M. Moinul and Cheng, Chow Kian and Permadi, Rahadian Bayu and Feldt, Robert}, 
  journal={IEEE Transactions on Software Engineering}, 
  title={Evaluation and Measurement of Software Process Improvement—A Systematic Literature Review},
  year={2012},  
  volume={38},
  number={2}, 
  pages={398-424},
  abstract={
    BACKGROUND-Software Process Improvement (SPI) is a systematic approach to increase the efficiency and effectiveness of a software development organization and to enhance software products. OBJECTIVE-This paper aims to identify and characterize evaluation strategies and measurements used to assess the impact of different SPI initiatives. METHOD-The systematic literature review includes 148 papers published between 1991 and 2008. The selected papers were classified according to SPI initiative, applied evaluation strategies, and measurement perspectives. Potential confounding factors interfering with the evaluation of the improvement effort were assessed. RESULTS-Seven distinct evaluation strategies were identified, wherein the most common one, “Pre-Post Comparison,” was applied in 49 percent of the inspected papers. Quality was the most measured attribute (62 percent), followed by Cost (41 percent), and Schedule (18 percent). Looking at measurement perspectives, “Project” represents the majority with 66 percent. CONCLUSION-The evaluation validity of SPI initiatives is challenged by the scarce consideration of potential confounding factors, particularly given that “Pre-Post Comparison” was identified as the most common evaluation strategy, and the inaccurate descriptions of the evaluation context. Measurements to assess the short and mid-term impact of SPI initiatives prevail, whereas long-term measurements in terms of customer satisfaction and return on investment tend to be less used.},  
  keywords={},  
  doi={10.1109/TSE.2011.26}, 
  ISSN={1939-3520},
  month={March},
}

@article{wang2011_enabling_public_accountability_and_data_dynamics_etc,  
  author={Wang, Qian and Wang, Cong and Ren, Kui and Lou, Wenjing and Li, Jin}, 
  journal={IEEE Transactions on Parallel and Distributed Systems}, 
  title={Enabling Public Auditability and Data Dynamics for Storage Security in Cloud Computing},  
  year={2011},
  volume={22},
  number={5}, 
  pages={847-859},
  abstract={
    Cloud Computing has been envisioned as the next-generation architecture of IT Enterprise. It moves the application software and databases to the centralized large data centers, where the management of the data and services may not be fully trustworthy. This unique paradigm brings about many new security challenges, which have not been well understood. This work studies the problem of ensuring the integrity of data storage in Cloud Computing. In particular, we consider the task of allowing a third party auditor (TPA), on behalf of the cloud client, to verify the integrity of the dynamic data stored in the cloud. The introduction of TPA eliminates the involvement of the client through the auditing of whether his data stored in the cloud are indeed intact, which can be important in achieving economies of scale for Cloud Computing. The support for data dynamics via the most general forms of data operation, such as block modification, insertion, and deletion, is also a significant step toward practicality, since services in Cloud Computing are not limited to archive or backup data only. While prior works on ensuring remote data integrity often lacks the support of either public auditability or dynamic data operations, this paper achieves both. We first identify the difficulties and potential security problems of direct extensions with fully dynamic data updates from prior works and then show how to construct an elegant verification scheme for the seamless integration of these two salient features in our protocol design. In particular, to achieve efficient data dynamics, we improve the existing proof of storage models by manipulating the classic Merkle Hash Tree construction for block tag authentication. To support efficient handling of multiple auditing tasks, we further explore the technique of bilinear aggregate signature to extend our main result into a multiuser setting, where TPA can perform multiple auditing tasks simultaneously. Extensive security and performance analysis show that the proposed schemes are highly efficient and provably secure.},  
  keywords={},  
  doi={10.1109/TPDS.2010.183},  
  ISSN={1558-2183},  
  month={May},
}

@article{weider1998_software_fault_prevention_in_coding_and_RCA,
  title={A software fault prevention approach in coding and root cause analysis},
  author={Weider, D Yu},
  journal={Bell Labs Technical Journal},
  volume={3},
  number={2},
  pages={3--21},
  year={1998},
  publisher={Nokia Bell Labs}
}

@techreport{wichmann2007software,
  title={Software Support for Metrology Best Practice Guide No. 1},
  author={Wichmann, Brian and Parkin, Graeme and Barker, Robin},
  journal={Validation of Software in Measurement Systems},
  year={2007},
  institution={{National Physical Laboratory (UK)}},
  url = {http://eprintspublications.npl.co.uk/3922/},
  pages = {1--43},
}

@inproceedings{yildirim2019_ux_analytics_for_android_platforms,
  title={User Experience Analytics for Android Platforms},
  author={Y{\i}ld{\i}r{\i}m, {\.I}lkan},
  pages = {553--555},
  booktitle={ISAS2019-ENS - 3rd International Symposium on Innovative Approaches in Scientific Studies (Engineering and Natural Sciences)},
  url = {http://www.set-science.com/manage/uploads/ISAS2019-ENS_0042/SETSCI_ISAS2019-ENS_0042_00105.pdf},
  year = {2019},
  publisher = {SET Technology - Turkey},
  more_info = {http://www.set-science.com/index.php?go=d1001a2417e2b87d5b7c53e16c5e1675&conf_id=42&paper_id=105},
  volume = {4},
  address = {Ankara, Turkey},
  commentz = {This does not appear to be structured or run as a IEEE/ACM style conference and the citation is hard to ascertain.}
}

@article{zeng2019studying_logging_practices_fdroid,
  title={Studying the characteristics of logging practices in mobile apps: a case study on f-droid},
  author={Zeng, Yi and Chen, Jinfu and Shang, Weiyi and Chen, Tse-Hsun Peter},
  journal={Empirical Software Engineering},
  volume={24},
  number={6},
  pages={3394--3434},
  year={2019},
  publisher={Springer}
}

@article{zou2016_uilog,
  title={Uilog: Improving log-based fault diagnosis by log analysis},
  author={Zou, De-Qing and Qin, Hao and Jin, Hai},
  journal={Journal of computer science and technology},
  volume={31},
  number={5},
  pages={1038--1052},
  year={2016},
  publisher={Springer},
  doi = {10.1007/s11390-016-1678-7},
  url = {https://doi.org/10.1007/s11390-016-1678-7},
  abstract = {In modern computer systems, system event logs have always been the primary source for checking system status. As computer systems become more and more complex, the interaction between software and hardware increases frequently. The components will generate enormous log information, including running reports and fault information. The sheer quantity of data is a great challenge for analysis relying on the manual method. In this paper, we implement a management and analysis system of log information, which can assist system administrators to understand the real-time status of the entire system, classify logs into different fault types, and determine the root cause of the faults. In addition, we improve the existing fault correlation analysis method based on the results of system log classification. We apply the system in a cloud computing environment for evaluation. The results show that our system can classify fault logs automatically and effectively. With the proposed system, administrators can easily detect the root cause of faults.},
  some_intersting_citations_on_log_design = {https://scholar.google.com/scholar?cites=6588326960360905008&as_sdt=2005&sciodt=0,5&hl=en},
}

@book{aurini2016_how_to_of_qualitative_research,
  title={The How To of Qualitative Research: Strategies for Executing High Quality Projects},
  author={Aurini, J.D. and Heath, M. and Howells, S.},
  isbn={9781473944275},
  url={https://books.google.co.uk/books?id=q-EODAAAQBAJ},
  year={2016},
  publisher={SAGE Publications},
  isbn = {978-1-4462-6709-7},
}


@book{benioff_trailblazer_2019,
  title = {Trailblazer: The Power of Business as the Greatest Platform for Change},
  author = {Benioff, Marc and Langley, Monica},
  year = {2019},
  publisher = {Simon and Schuster UK},
  address = {London, UK},
  ISBN = {9781984825193},
}

@book{chung2000_non_functional_requirements_in_software_engineering,
  title={Non-functional requirements in software engineering},
  author={Chung, Lawrence and Nixon, Brian A and Yu, Eric and Mylopoulos, John},
  volume={5},
  year={2000},
  publisher={Kluwer Academic Publishers},
  isbn = {0780792386667},
  address = {Norwell, MA, USA},
}

@book{clough2012_students_guide_to_methodology,
  title={A Student's Guide to Methodology},
  edition = {Third edition},
  author={Clough, Peter and Nutbrown, Cathy},
  year={2012},
  publisher={Sage},
  isbn = {978-1-4462-0861-8},
  pages = {288},
}

@book{davenport2010analytics_at_work,
  title={Analytics at work: Smarter decisions, better results},
  author={Davenport, Thomas H and Harris, Jeanne G and Morison, Robert},
  year={2010},
  publisher={Harvard Business Press},
  address={Boston, MA, USA.},
}

@book{davenport2017competing_on_analytics,
  title={Competing on analytics: Updated, with a new introduction: The new science of winning},
  author={Davenport, Thomas and Harris, Jeanne},
  year={2017},
  publisher={Harvard Business Press},
  address={Boston, MA, USA.},
}

@book{evans2004_achieving_software_quality_through_teamwork,
  title={Achieving software quality through teamwork},
  author={Evans, Isabel},
  year={2004},
  publisher={Artech House},
  pages = {324},
  ISBN = {9781580536622},
  address = {Norwood, MA 02062, USA.}
}

@book{goldratt2017_necessary_but_not_sufficient,
  title={Necessary but not sufficient: a theory of constraints business novel},
  author={{Eliyahu M. Goldratt with Eli Schragenheim and Carol A. Ptak}},
  year={2000},
  publisher={Routledge},
  address = {Great Barrington, MA, USA},
  isbn = {978-0884271703},
}

@book{gunther2013truth_about_better_decision_making,
  title={The Truth about Better Decision-making (collection)},
  author={Gunther, Robert E},
  year={2008},
  publisher={Pearson Education Limited},
  ISBN = {978-0-273-71806-2},
  address = {Harlow, England},
}

@book{harford2021_the_data_detective,
  title = {The Data Detective: Ten Easy Rules to Make Sense of Statistics},
  author = {Tim Harford},
  publisher = {Riverhead Books},
  isbn = {978-0593084595},

  interesting_amazon_review = {
  https://www.amazon.co.uk/gp/product/0593084594/ref=ox_sc_act_title_2?smid=AHRB2OK2Q2YCL&psc=1
  Trey Shipp, 2 February 2021
  Darrell Huff's classic book, How to Lie with Statistics, warns us not to get misled by statistics. In this book, Tim Harford tells us how to see the Truth with statistics. He gives us ten rules of thumb for thinking about reported numbers. But the real reason for reading the book is for all of the stories Harford tells to illustrate each rule. It's like the greatest hits from the BBC radio show he has hosted for years.

  Here is a simple summary of the ten rules:

    1. When considering new information, pay attention to how it makes you feel. Your emotions can influence you to dismiss accurate statistics that you do not like and to embrace false statistics that you do like
    2. Sometimes your personal experience (a worm's eye view) conflicts with a bird's-eye view statistic. For example, the subway may be only half full on average during the day but packed every time you ride it (during rush hour). Both perspectives help you understand the truth.
    3. Make sure you understand what is being counted. When counting beans, the definition of a bean matters.
    4. Look for information that can put a statistic into context, like the trend, the scale, or how it compares to other situations.
    5. Try to learn where the statistics came from (the backstory) – and what other data might have vanished into obscurity.
    6. Ask who is missing from the data, and would our conclusions differ if they were included.
    7. Ask tough questions about algorithms and the big datasets that drive them, recognizing that without intelligent openness they cannot be trusted.
    8. Pay more attention to the bedrock of official statistics – and the sometimes heroic statisticians who protect it.
    9. Look under the surface of any beautiful graph or chart. Don't let the beauty mislead you.
    10. Keep an open mind, asking how we might be mistaken and whether the facts have changed.

  Those ten tips sound boring, but Harford's stories are not. Most of them show the power of useful statistics. As Harford says: "Good statistics are not smoke and mirrors; in fact, they help us see more clearly. Good statistics are like a telescope for an astronomer, a microscope for a bacteriologist, or an X-ray for a radiologist."
  }
}

@book{harty_aymer_playbook_2016,
	edition = {2},
	title = {The {Mobile} {Analytics} {Playbook}},
	isbn = {978-0-9970694-0-2},
	url = {http://www.themobileanalyticsplaybook.com/},
	urldate = {2017-10-02},
	publisher = {HP Enterprises},
	author = {Harty, Julian and Aymer, Antoine},
	month = {Jan},
	year = {2016},
	address = {High Wycombe, UK},
}

@book{hubbard2014measure,
  title={How to Measure Anything: Finding the Value of Intangibles in Business, Third Edition},
  author={Hubbard, Douglas W},
  year={2014},
  publisher={John Wiley \& Sons},
  isbn = {978-1-118-53927-9},
  address = {Hoboken, New Jersey, USA},
}

@book{jain1991art,
  title={The Art Of Computer Systems Performance Analysis: Techniques For Experimental Measurement, Simulation, And Modeling},
  author={Jain, Raj},
  year={1991},
  publisher={john wiley \& sons},
  address = {New York, NY, USA.},
  isbn={978-0-471-50336-1}
}

@book{lepore1999_deming_and_goldratt,
  title = {Demin and Goldratt},
  author = {Domenico Lepore and Oded Cohen},
  year = {1999},
  publisher = {North River Press},
  isbn = {0-88427-163-3},
  address = {Great Barrington, MA, USA},
}

@book{marr2015bigdatabook,
  title={Big Data: Using SMART big data, analytics and metrics to make better decisions and improve performance},
  author={Marr, Bernard},
  year={2015},
  publisher={John Wiley \& Sons},
  address = {Chichester, United Kingdom}
}

@book{mill1884system,
  title={A system of logic, ratiocinative and inductive: Being a connected view of the principles of evidence and the methods of scientific investigation},
  author={Mill, John Stuart},
  volume={1},
  edition={8},
  year={1884},
  publisher={Longmans, Green, and Company},
  address={New York, NY, USA.},
  url={ftp://mirrors.xmission.com/gutenberg/2/7/9/4/27942/27942-pdf.pdf}
}

@book{phadke1995_quality_engineering_using_robust_design,
  title={Quality engineering using robust design},
  author={Phadke, Madhan Shridhar},
  year={1995},
  publisher={Prentice Hall PTR},
  address = {New Jersey, USA},
  ISBN = {0-13-745167-9}
}

https://www.springer.com/gb/book/9783319240442
@book{quality_control_in_R_book,
  title = {Quality Control in R},
  ISBN = {978-3-319-24044-2},
  year = {2015},
  author={Emilio L. Cano, Javier M. Moguerza, Mariano Prieto Corcoba},
  publisher = {Springer Nature},
  address = {Switzerland},
}

@book{riedesel2020_software_telemetry_meap_v04,
  title = {Software Telemetry (MEAP)},
  author = {Jamie Riedesel},
  year = {2020},
  publisher = {Manning},
  isbn = {9781617298141},
  address = {Unpublished},
  url = {https://www.manning.com/books/software-telemetry},
}

@book{riedesel2021_software_telemetry_meap_v06,
  title = {Software Telemetry (MEAP)},
  author = {Jamie Riedesel},
  year = {2021},
  publisher = {Manning},
  isbn = {9781617298141},
  address = {Unpublished},
  url = {https://www.manning.com/books/software-telemetry},
}

// Actually a court filing, recommended to create a book to cite it
https://latex.org/forum/viewtopic.php?t=32485
@book{rodriguez_et_al_v_google_llc_et_al_2020,
  title = {{20-CV-04688}},
  author = {{Rodriguez et al. v. Google LLC et al.}},
  year = {2020},
  publisher = {{UNITED STATES DISTRICT COURT NORTHERN DISTRICT OF CALIFORNIA}},
  address = {California, USA},
}

@book{rochkind1985_advanced_unit_programming,
  title={Advanced UNIX programming},
  author={Rochkind, Marc J},
  journal={Prentice Hall Software Series},
  year={1985},
  isbn={0-13-011800-1},
  publisher = {Prentice-Hall},
  address = {Englewood Cliffs, N.J. 07632},
  pages = {256},
}

@book{rudder2014dataclysm,
  title={Dataclysm: Love, Sex, Race, and Identity--What Our Online Lives Tell Us about Our Offline Selves},
  author={Rudder, Christian},
  year={2014},
  publisher={4th Estate},
  isbn = {978-0-00-749443-9},
  address = {London},
}

@book{rugg2006_gentle_guide_to_research_methods,
  title={A gentle guide to research methods},
  author={Rugg, Gordon and Petre, Marian},
  year={2006},
  publisher={McGraw-Hill Education (UK)},
  isbn = {978-0-335-21927-8},
  pages = {238},
}

@book{scheinkopf1999_thinking_for_a_change,
  title = {Thinking for a Change: Putting the TOC Thinking Processes to Use},
  author = {Lisa J. Scheinkopf},
  year = {1999},
  publisher = {CRC Press},
  address = {Boca Raton, FL, USA},
  isbn = {978-1574441017},
}

@book{sommerville1989_software_engineering,
  title={Software engineering - third edition},
  author={Sommerville, Ian},
  year={1989},
  publisher={Addison-Wesley Publishers Ltd.},
  address = {Wokingham, England},
  isbn={0-201-17568-1},
}

@book{trafford2008_stepping_stones_to_achieving_your_doctorate,
  title={Stepping stones to achieving your doctorate: By focusing on your viva from the start: Focusing on your viva from the start},
  author={Trafford, Vernon and Leshem, Shosh},
  year={2008},
  publisher={McGraw-Hill Education (UK)},
  isbn = {978-0-335225439},
  address = {New York, NY, USA},
}

@book{weinberg1992quality,
  title={Quality software management (Vol. 1) systems thinking},
  author={Weinberg, Gerald M},
  year={1992},
  publisher={Dorset House Publishing Co., Inc.},
  address = {New York, NY, USA},
}

@book{weinberg2006weinberg,
  title={Weinberg on Writing: The Fieldstone Method},
  author={Weinberg, Gerald M},
  abstract = {Most people never publish an article. Of those who do publish an article, most write only one. Most people never publish a report. Of those who do publish a report, most write only one. Most people never publish a book. Of those who do publish a book, most write only one. Most people never publish a script. Of those who do publish a script, most write only one. If you ask them why they don’t write more, they will say they are stuck, or “blocked.” But these are just labels, and explain nothing. Most often, they stop writing because they don’t know how to work with the essential randomness involved in the creative process.},
  year={2006},
  publisher={Dorset House Pub.}
}

@book{yin2018_case_study_research_and_applications_6th_edition,
  publisher = {SAGE Publications, Inc; Sixth edition},
  title = {Case Study Research and Applications - Design and Methods},
  edition = {\nth{6} edition},
  year = {2018},
  pages = {352},
  ISBN = {978-1506336169},
  address = {London},
  author = {Robert K. Yin},
}

@book{zeller2009_why_programs_fail,
  title = {Why Programs Fail 2nd Edition - A Guide to Systematic Debugging},
  edition = {2},
  year = {2009},
  author = {Andreas Zeller},
  ISBN = {9780123745156},
  doi = {10.1016/B978-0-12-374515-6.X0000-7},
  publisher = {{Elsevier Inc.}},
  address = {Burlington, MA 01803, USA},
}

@inbook{MacLean2015_pro_android_5_book,
  author="MacLean, Dave and Komatineni, Satya and Allen, Grant",
  title="Deploying Your Application: Google Play Store and Beyond",
  bookTitle="Pro Android 5",
  year="2015",
  publisher="Apress",
  address="Berkeley, CA",
  pages="677--696",
  abstract="Creating a great application that people will love is one thing, but you also need an easy way for people to find and download it. Google created the Play Store for this purpose. From an icon right on the device, users can click straight into the Play Store to browse, search, review, and download applications. Users can also access Play Store over the Internet to do those same things, although the downloading is not to the computer but rather apps are sent directly to the user's device. Many applications are free; for those that are not, the Play Store provides payment mechanisms for easy purchasing.",
  isbn="978-1-4302-4681-7",
  doi="10.1007/978-1-4302-4681-7_30",
  url="https://doi.org/10.1007/978-1-4302-4681-7_30",
  quotes = {
    If you really want to know how a user got to a crash, you’ll want to implement one of the mobile analytics packages into your app. These will generate event records as a user steps through your application, and will also report crashes. The breadcrumbs (event records) will let you know the steps a user took up to the point of the crash. This capability is separate from the Google Play Store, however.
  }
}

@inbook{yet_to_cite_10.1145/1134285.1134349,
    author = {Nagappan, Nachiappan and Ball, Thomas and Zeller, Andreas},
    title = {Mining Metrics to Predict Component Failures},
    year = {2006},
    isbn = {1595933751},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/1134285.1134349},
    abstract = {What is it that makes software fail? In an empirical study of the post-release defect
    history of five Microsoft software systems, we found that failure-prone software entities
    are statistically correlated with code complexity measures. However, there is no single
    set of complexity metrics that could act as a universally best defect predictor. Using
    principal component analysis on the code metrics, we built regression models that
    accurately predict the likelihood of post-release defects for new entities. The approach
    can easily be generalized to arbitrary projects; in particular, predictors obtained
    from one project can also be significant for new, similar projects.},
    booktitle = {Proceedings of the 28th International Conference on Software Engineering},
    pages = {452–461},
    numpages = {10}
}

@Inbook{Wohlin2003_empirical_research_methods_in_software_engineering,
  author="Wohlin, Claes and H{\"o}st, Martin and Henningsson, Kennet",
  editorz="Conradi, Reidar and Wang, Alf Inge",
  title="Empirical Research Methods in Software Engineering",
  bookTitle="Empirical Methods and Studies in Software Engineering: Experiences from ESERNET",
  year="2003",
  publisher="Springer Berlin Heidelberg",
  address="Berlin, Heidelberg",
  pages="7--23",
  abstract="Software engineering is not only about technical solutions. It is to a large extent also concerned with organizational issues, project management and human behaviour. For a discipline like software engineering, empirical methods are crucial, since they allow for incorporating human behaviour into the research approach taken. Empirical methods are common practice in many other disciplines. This chapter provides a motivation for the use of empirical methods in software engineering research. The main motivation is that it is needed from an engineering perspective to allow for informed and well-grounded decision. The chapter continues with a brief introduction to four research methods: controlled experiments, case studies, surveys and post-mortem analyses. These methods are then put into an improvement context. The four methods are presented with the objective to introduce the reader to the methods to a level that it is possible to select the most suitable method at a specific instance. The methods have in common that they all are concerned with quantitative data. However, several of them are also suitable for qualitative data. Finally, it is concluded that the methods are not competing. On the contrary, the different research methods can preferably be used together to obtain more sources of information that hopefully lead to more informed engineering decisions in software engineering.",
  isbn="978-3-540-45143-3",
  doi="10.1007/978-3-540-45143-3_2",
  url="https://doi.org/10.1007/978-3-540-45143-3_2"
}



@incollection{murphy_enabling_2019,
	address = {Berkeley, {CA}},
	title = {Enabling Productive Software Development by Improving Information Flow},
	isbn = {978-1-4842-4221-6},
	url = {https://doi.org/10.1007/978-1-4842-4221-6_24},
	abstract = {At its core, software development is an information-intensive knowledge generation and consumption activity. We are interested in how software tools can enable the productive development of software. Our hypothesis has been that software development productivity can be increased by improving the access and flow of information between the humans and tools involved in creating software systems. In this chapter, we review an evolution of technologies we have introduced based on this hypothesis. These technologies are in use by large software development organization and have been shown to improve software developer productivity. The description of these technologies also highlights how productivity can be considered at the individual, team and organizational level.},
	pages = {281--292},
	booktitle = {Rethinking Productivity in Software Engineering},
	publisher = {Apress},
	author = {Murphy, Gail C. and Kersten, Mik and Elves, Robert and Bryan, Nicole},
	editor = {Sadowski, Caitlin and Zimmermann, Thomas},
	year = {2019},
	doi = {10.1007/978-1-4842-4221-6_24},
}

@InProceedings{10.1007/978-3-319-49094-6_16,
author="Samuel, Triin
and Pfahl, Dietmar",
editor="Abrahamsson, Pekka
and Jedlitschka, Andreas
and Nguyen Duc, Anh
and Felderer, Michael
and Amasaki, Sousuke
and Mikkonen, Tommi",
title="Problems and Solutions in Mobile Application Testing",
booktitle="Product-Focused Software Process Improvement",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="249--267",
abstract="In recent years the amount of literature published about mobile application testing has significantly grown. However, it is unclear to what degree stated problems and proposed solutions are relevant to industry. To shed light on this issue, we conducted a literature survey to provide an overview of what current scientific literature considers problems and potential solutions in mobile application testing, and how often proposed solutions were reportedly evaluated in industry. Then we conducted a case study involving six software companies in Estonia to find out which of the problems are considered relevant by professionals, and which of the proposed solutions are considered novel and applicable. In total, we identified 49 potential problems or challenges in the mobile application testing domain and 39 potential solutions, some of which were implemented software tools while others were just theoretical concepts. Although some of the solutions were reportedly applied in practice, in most cases the literature did not give much information on the actual usage in industry of the proposed solutions. The case study revealed that while the relevance of each identified problem was highly variable from one company to another, there are some key problems that are generally considered vital both by research and industry. Regarding solution proposals, it turned out they are often described too much on the conceptual level or are too unrelated to the most urgent test-related problems of our case companies to be of interest to them.",
isbn="978-3-319-49094-6"
}

@inproceedings{10.1145/3239235.3267436,
    author = {Rodr\'{\i}guez-P\'{e}rez, Gema and Zaidman, Andy and Serebrenik, Alexander and Robles, Gregorio and Gonz\'{a}lez-Barahona, Jes\'{u}s M.},
    title = {What If a Bug Has a Different Origin? Making Sense of Bugs without an Explicit Bug Introducing Change},
    year = {2018},
    isbn = {9781450358231},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/3239235.3267436},
    doi = {10.1145/3239235.3267436},
    abstract = {Background: Many studies in the software research literature on bug fixing are built
    upon the assumption that "a given bug was introduced by the lines of code that were
    modified to fix it", or variations of it. Although this assumption seems very reasonable
    at first glance, there is little empirical evidence supporting it. A careful examination
    surfaces that there are other possible sources for the introduction of bugs such as
    modifications to those lines that happened before the last change an changes external
    to the piece of code being fixed. Goal: We aim at understanding the complex phenomenon
    of bug introduction and bug fix. Method: We design a preliminary approach distinguishing
    between bug introducing commits (BIC) and first failing moments (FFM). We apply this
    approach to Nova and ElasticSearch, two large and well-known open source software
    projects. Results: In our initial results we obtain that at least 24% bug fixes in
    Nova and 10% in ElasticSearch have not been caused by a BIC but by co-evolution, compatibility
    issues or bugs in external API. Merely 26--29% of BICs can be found using the algorithm
    based on the assumption that "a given bug was introduced by the lines of code that
    were modified to fix it". Conclusions: The approach allows also for a better framing
    of the comparison of automatic methods to find bug inducting changes. Our results
    indicate that more attention should be paid to whether a bug has been introduced and,
    when it was introduced.},
    booktitle = {Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
    articleno = {52},
    numpages = {4},
    keywords = {SZZ algorithm, bug-introducing change, empirical study},
    address = {Oulu, Finland},
    series = {ESEM '18}
}

@inproceedings{abreu2007_on_the_accuracy_of_spectrum_based_fault_localization,
  author={R. {Abreu} and P. {Zoeteweij} and A. J. C. {van Gemund}},
  booktitle={Testing: Academic and Industrial Conference Practice and Research Techniques - MUTATION (TAICPART-MUTATION 2007)},
  title={On the Accuracy of Spectrum-based Fault Localization},
  year={2007},
  volume={},
  number={},
  pages={89-98},
  abstract={
    Spectrum-based fault localization shortens the test- diagnose-repair cycle by reducing the debugging effort. As a light-weight automated diagnosis technique it can easily be integrated with existing testing schemes. However, as no model of the system is taken into account, its diagnostic accuracy is inherently limited. Using the Siemens Set benchmark, we investigate this diagnostic accuracy as a function of several parameters (such as quality and quantity of the program spectra collected during the execution of the system), some of which directly relate to test design. Our results indicate that the superior performance of a particular similarity coefficient, used to analyze the program spectra, is largely independent of test design. Furthermore, near- optimal diagnostic accuracy (exonerating about 80\% of the blocks of code on average) is already obtained for low-quality error observations and limited numbers of test cases. The influence of the number of test cases is of primary importance for continuous (embedded) processing applications, where only limited observation horizons can be maintained.},
  keywords={program debugging;program diagnostics;program testing;spectrum-based fault localization;test-diagnose-repair cycle;light-weight automated diagnosis technique;low-quality error observations;test data analysis;Software testing;Automatic testing;Fault diagnosis;Debugging;Computer industry;Benchmark testing;Failure analysis;Fault detection;Fault location;Mathematics},  
  doi={10.1109/TAIC.PART.2007.13},
  ISSN={},
  month={Sep},
  address = {Windsor, UK},
  publisher = {IEEE},
  organization = {IEEE},
}

@inproceedings{adams2016modern,
  title={Modern release engineering in a nutshell--why researchers should care},
  author={Adams, Bram and McIntosh, Shane},
  booktitle={2016 IEEE 23rd international conference on software analysis, evolution, and reengineering (SANER)},
  volume={5},
  pages={78--90},
  year={2016},
  organization={IEEE},
  publisher={IEEE},
  address={Suita, Japan},
  relevance_to_phd = {Release Engineering and Mobile App development have parallels.},
  quote_1 = { The main take-home message is that, while release engineering technology has flourished tremendously due to industry, empirical validation of best practices and the impact of the release engineering process on (amongst others) software quality is largely missing and provides major research opportunities.},
  quote_2 = {Even for the pioneers of modern release engineering, newer technologies like mobile apps still pose open challenges. Broader questions include: what will be the long-term effect on user-perceived quality of releases [43, 45], how quickly will technical debt ramp up when release cycles are so short and can end users keep up with a continuous stream of new releases?},
}

@inproceedings{adamsen2015systematic_catrobat,
  title={Systematic execution of {Android} test suites in adverse conditions},
  author={Adamsen, Christoffer Quist and Mezzetti, Gianluca and M{\o}ller, Anders},
  booktitle={Proceedings of the 2015 International Symposium on Software Testing and Analysis},
  pages={83--93},
  year={2015},
  publisher = {{ACM}},
  address = {Baltimore MD, USA},
}

@article{Amland_2000_rbt_financial_case_study, 
   title={Risk-based testing:: Risk analysis fundamentals and metrics for software testing including a financial application case study},
   volume={53},
   ISSN={0164-1212},
   DOI={https://doi.org/10.1016/S0164-1212(00)00019-4},
   abstractNote={
     The idea of risk-based testing is to focus testing and spend more time on critical functions. By combining the focused process with metrics it is possible to manage the test process by intelligent assessment and to communicate the expected consequences of decisions taken. This paper discusses an approach to risk-based testing and how risk-based testing was carried out in a large project in a financial institution. The paper concludes with how practical risk-based testing experience should inform theory and provide advice on organizational requirements that are necessary to achieve success.},
   number={3},
   journal={Journal of Systems and Software},
   author={Amland, Ståle},
   year={2000},
   pages={287–295}
}


@inproceedings{an2015_challenges_and_issues_of_mining_crash_reports,
  author={L. {An} and F. {Khomh}},  
  booktitle={2015 IEEE 1st International Workshop on Software Analytics (SWAN)},   
  title={Challenges and Issues of Mining Crash Reports},   
  year={2015},  
  volume={},  
  number={},  
  pages={5-8},  
  abstract={Automatic crash reporting tools built in many software systems allow software practitioners to understand the origin of field crashes and help them prioritise field crashes or bugs, locate erroneous files, and/or predict bugs and crash occurrences in subsequent versions of the software systems. In this paper, after illustrating the structure of crash reports in Mozilla, we discuss some techniques for mining information from crash reports, and highlight the challenges and issues of these techniques. Our aim is to raise the awareness of the research community about issues that may bias research results obtained from crash reports and provide some guidelines to address certain challenges related to mining crash reports.},  
  keywords={data mining;program debugging;program testing;crash report mining;crash reporting tool;software systems;Computer bugs;Data mining;Databases;Algorithm design and analysis;Software systems;Crash report;bug report;mining softwarerepositories.},  
  doi={10.1109/SWAN.2015.7070480},  
  ISSN={},  
  month={March},
}

@inproceedings{andrews1998_testing_using_log_file_analysis,
  title={Testing using log file analysis: tools, methods, and issues},
  author={Andrews, James H},
  booktitle={Proceedings 13th IEEE International Conference on Automated Software Engineering (Cat. No. 98EX239)},
  pages={157--166},
  year={1998},
  organization={IEEE},
  publisher={IEEE},
  address={Honolulu, HI, USA}
}

@inproceedings{avellis_harty_yu_towards_mobile_twin_peaks,
author = {Avellis, Giovanna and Harty, Julian and Yu, Yijun},
title = {Towards Mobile Twin Peaks for App Development},
year = {2017},
isbn = {9781538626696},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MOBILESoft.2017.10},
doi = {10.1109/MOBILESoft.2017.10},
booktitle = {Proceedings of the 4th International Conference on Mobile Software Engineering and Systems},
pages = {189–193},
numpages = {5},
keywords = {mobile analytics, ethnographic studies, app development, twin peaks, information retrieval},
address = {Buenos Aires, Argentina},
series = {MOBILESoft ’17}
}

@inproceedings{bagnato2020_challenges_and_benefits_from_using_software_analytics_in_softeam,
  title = {Challenges and Benefits from Using Software Analytics in Softeam},
  author = {Bagnato, Alessandra and Abherv{\'e}, Antonin and Mart{\'i}nez\-Fern{\'a}ndez, Silverio and Franch, Xavier},
  pages = {512},
  booktitle = {6th International Workshop on Rapid Continuous Software Engineering (RCoSE)},
  year = {2020},
  organization={IEEE},
  publisher = {IEEE},
  address = {Seoul, South Korea},  
}

@inproceedings{balagtas2009methodology,
  title={A methodology and framework to simplify usability analysis of mobile applications},
  author={Balagtas-Fernandez, Florence and Hussmann, Heinrich},
  booktitle={2009 IEEE/ACM International Conference on Automated Software Engineering},
  pages={520--524},
  year={2009},
  organization={IEEE},
  abstract = {Usability analysis is an important step in software development in order to improve certain aspects of the system. However, it is often a challenge especially when it comes to evaluating applications running on mobile devices because of the restrictions posed by the device and the lack of supporting tools and software available to collect the necessary usability data. This paper proposes a methodology and framework to aid developers in preparing the mobile system for usability analysis. The focus is on the simplification of the developer's task in preparing the system for evaluation and the processing of the collected usability data by automating some of the tasks involved in the process.},
}

@inproceedings{bishop1993variation,
  title={The variation of software survival time for different operational input profiles (or why you can wait a long time for a big bug to fail)},
  author={Bishop, Peter G},
  booktitle={FTCS-23 The Twenty-Third International Symposium on Fault-Tolerant Computing},
  pages={98--107},
  year={1993},
  organization={IEEE},
  publisher={IEEE},
  address={Toulouse, France},
}

@inproceedings{bohme2017_where_is_the_bug_and_how_is_it_fixed,
    author = {B\"{o}hme, Marcel and Soremekun, Ezekiel O. and Chattopadhyay, Sudipta and Ugherughe, Emamurho and Zeller, Andreas},
    title = {Where is the Bug and How is It Fixed? An Experiment with Practitioners},
    year = {2017},
    isbn = {9781450351058},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/3106237.3106255},
    doi = {10.1145/3106237.3106255},
    abstract = { Research has produced many approaches to automatically locate, explain, and repair
    software bugs. But do these approaches relate to the way practitioners actually locate,
    understand, and fix bugs? To help answer this question, we have collected a dataset
    named DBGBENCH --- the correct fault locations, bug diagnoses, and software patches
    of 27 real errors in open-source C projects that were consolidated from hundreds of
    debugging sessions of professional software engineers. Moreover, we shed light on
    the entire debugging process, from constructing a hypothesis to submitting a patch,
    and how debugging time, difficulty, and strategies vary across practitioners and types
    of errors. Most notably, DBGBENCH can serve as reality check for novel automated debugging
    and repair techniques. },
    booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
    pages = {117–128},
    numpages = {12},
    keywords = {Debugging in practice, User studies, User as tool benchmark, Evaluation},
    address = {Paderborn, Germany},
    series = {ESEC/FSE 2017}
}

@inproceedings{bohmer2011falling_asleep_with_angry_birds,
  title={Falling asleep with Angry Birds, Facebook and Kindle: a large scale study on mobile application usage},
  author={B{\"o}hmer, Matthias and Hecht, Brent and Sch{\"o}ning, Johannes and Kr{\"u}ger, Antonio and Bauer, Gernot},
  booktitle={Proceedings of the 13th international conference on Human computer interaction with mobile devices and services},
  pages={47--56},
  year={2011},
  address={Stockholm, Sweden},
  publisher={ACM},
}

@inproceedings{budgen1993_case_tools_masters_or_servants,
  author={D. {Budgen} and M. {Marashi} and A. {Reeves}},
  booktitle={1993 Software Engineering Environments},
  title={CASE tools: Masters or servants?},
  publisher = {IEEE},
  year={1993},
  volume={},
  number={},
  pages={156-165},
  abstract={Much of the recent research into the use of CASE tools for specification and design of software systems has focused on the integration of such tools with one another and with related tools. However, much less attention has been directed towards considering how well these tools integrate with current practices. In particular, for software design there is good reason to believe that "opportunistic" design practices are widely employed by software designers, and that few industrial designers use either design methods or CASE tools. In this paper we draw upon some of our own research experiences in a related area to suggest some reasons why this might be so, and to describe some ideas that we are currently exploring to improve our understanding of these reasons. Our conclusion is that the organization and form of the user interface for CASE tools needs to be influenced by the designer's cognitive processes concerning the design, as well as by the need for a consistent form of presentation.},
  keywords={computer aided software engineering;software tools;user interfaces;software systems specification;software systems design;opportunistic design practices;consistent presentation format;CASE tools;current practices;industrial designers;user interface;cognitive processes;Computer aided software engineering;Software design;Design methodology;User interfaces;Navigation;Software tools;Computer science;Software systems;Computer industry;Process design},
  doi={10.1109/SEE.1993.388412},
  ISSN={},
  month={July},
  address = {Reading, United Kingdom},
}

@inproceedings{buse_analytics_2010,
	title = {Analytics for Software Development},
	url = {http://research.microsoft.com/apps/pubs/default.aspx?id=136974},
	address={Santa Fe, New Mexico, USA},
	abstract = {{\textless}p{\textgreater}Despite large volumes of data and many types of metrics, software projects continue to be diffcult to predict and risky to conduct. In this paper we propose software analytics which holds out the promise of helping the managers of software projects turn their plentiful information resources, produced readily by current tools, into insights they can act on. We discuss how analytics works, why it's a good fit for software engineering, and the research problems that must be overcome in order to realize its promise.{\textless}/p{\textgreater}},
	booktitle = {Proceedings of the {FSE}/{SDP} Workshop on the Future of Software Engineering Research ({FoSER})},
	publisher = {Association for Computing Machinery, Inc.},
	author = {Buse, Raymond P. L. and Zimmermann, Thomas},
	date = {2010-11},
	year = {2010},
	pages = {77--80},
	file = {foser-2010-buse.pdf:/Users/julianharty/Zotero/storage/6RW75BVR/foser-2010-buse.pdf:application/pdf}
}

@inproceedings{buse2012_information_needs_for_software_development_analytics,  
  author={Buse, Raymond P. L. and Zimmermann, Thomas},  
  booktitle={2012 34th International Conference on Software Engineering (ICSE)},   
  title={Information needs for software development analytics},   
  year={2012},  
  volume={},  
  number={},  
  pages={987-996},  
  abstract={
    Software development is a data rich activity with many sophisticated metrics. Yet engineers often lack the tools and techniques necessary to leverage these potentially powerful information resources toward decision making. In this paper, we present the data and analysis needs of professional software engineers, which we identified among 110 developers and managers in a survey. We asked about their decision making process, their needs for artifacts and indicators, and scenarios in which they would use analytics. The survey responses lead us to propose several guidelines for analytics tools in software development including: Engineers do not necessarily have much expertise in data analysis; thus tools should be easy to use, fast, and produce concise output. Engineers have diverse analysis needs and consider most indicators to be important; thus tools should at the same time support many different types of artifacts and many indicators. In addition, engineers want to drill down into data based on time, organizational structure, and system architecture.},  
  keywords={}, 
  doi={10.1109/ICSE.2012.6227122},
  ISSN={1558-1225},
  month={June},
  organization = {IEEE},
  publisher = {IEEE},
  address = {Zurich, Switzerland},
}

@inproceedings{cai2019_large_scale_study_of_android_incompatibilities,
  author = {Cai, Haipeng and Zhang, Ziyi and Li, Li and Fu, Xiaoqin},
  title = {A Large-Scale Study of Application Incompatibilities in Android},
  year = {2019},
  isbn = {9781450362245},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3293882.3330564},
  doi = {10.1145/3293882.3330564},
  abstract = {
    The rapid expansion of the Android ecosystem is accompanied by continuing diversification of platforms and devices, resulting in increasing incompatibility issues which damage user experiences and impede app development productivity. In this paper, we conducted a large-scale, longitudinal study of compatibility issues in 62,894 benign apps developed in the past eight years, to understand the symptoms and causes of these issues. We further investigated the incompatibilities that are actually exercised at runtime through the system logs and execution traces of 15,045 apps. Our study revealed that, among others, (1) compatibility issues were prevalent and persistent at both installation and run time, with greater prevalence of run-time incompatibilities, (2) there were no certain Android versions that consistently saw more or less app incompatibilities than others, (3) installation-time incompatibilities were strongly correlated with the minSdkVersion specified in apps, while run-time incompatibilities were most significantly correlated with the underlying platform’s API level, and (4) installation-time incompatibilities were mostly due to apps’ use of architecture-incompatible native libraries, while run-time incompatibilities were mostly due to API changes during SDK evolution. We offered further insights into app incompatibilities, as well as recommendations on dealing with the issues for bother developers and end users of Android apps.},
booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysi (ISSTA 2019)},
  pages = {216–227},
  numpages = {12},
  keywords = {installation failure, run-time failure, Android, compatibility},
  address = {Beijing, China},
  series = {ISSTA 2019}
}

@inproceedings{canfora2013_automating_UX_experience_testing_on_smartphones,
  author={G. {Canfora} and F. {Mercaldo} and C. A. {Visaggio} and M. {DAngelo} and A. {Furno} and C. {Manganelli}},
  booktitle={2013 IEEE Sixth International Conference on Software Testing, Verification and Validation},
  title={A Case Study of Automating User Experience-Oriented Performance Testing on Smartphones},
  year={2013},
  volume={},
  number={},
  pages={66-69},
  abstract={
    We have developed a platform named Advanced Test Environment (ATE) for supporting the design and the automatic execution of UX tests for applications running on Android smartphones. The platform collects objective metrics used to estimate the UX. In this paper, we investigate the extent that the metrics captured by ATE are able to approximate the results that are obtained from UX testing with real human users. Our findings suggest that ATE produces UX estimations that are comparable to those reported by human users. We have also compared ATE with three widespread benchmark tools that are commonly used in the industry, and the results show that ATE outperforms these tools.},
  keywords={automatic testing;Linux;program testing;smart phones;software performance evaluation;advanced test environment;automatic UX test execution;UX test design;Android smartphones;objective metrics;ATE;UX testing;UX estimations;user experience-oriented performance testing automation;Conferences;Software testing;user experience;mobile applications;software testing;usability;smartphone;android},  
  doi={10.1109/ICST.2013.16},
  ISSN={2159-4848},
  month={March},
  publisher = {IEEE},
  address = {Luxembourg, Luxembourg},
}

@inproceedings{chen2014qoe,
  title={{QoE Doctor}: Diagnosing mobile app {QoE} with automated {UI} control and cross-layer analysis},
  author={Chen, Qi Alfred and Luo, Haokun and Rosen, Sanae and Mao, Z Morley and Iyer, Karthik and Hui, Jie and Sontineni, Kranthi and Lau, Kevin},
  booktitle={Proceedings of the 2014 Conference on Internet Measurement Conference},
  pages={151--164},
  year={2014},
  publisher = {ACM},
  address = {Vancouver BC, Canada},
  doi={https://doi.org/10.1145/2663716.2663726},
  abstract={
    Smartphones have become increasingly prevalent and important in our daily lives. To meet users' expectations about the Quality of Experience (QoE) of mobile applications (apps), it is essential to obtain a comprehensive understanding of app QoE and identify the critical factors that affect it. However, effectively and systematically studying the QoE of popular mobile apps such as Facebook and YouTube still remains a challenging task, largely due to a lack of a controlled and reproducible measurement methodology, and limited insight into the complex multi-layer dynamics of the system and network stacks.

    In this paper, we propose QoE Doctor, a tool that supports accurate, systematic, and repeatable measurements and analysis of mobile app QoE. QoE Doctor uses UI automation techniques to replay QoE-related user behavior, and measures the user-perceived latency directly from UI changes. To better understand and analyze QoE problems involving complex multi-layer interactions, QoE Doctor supports analysis across the application, transport, network, and cellular radio link layers to help identify the root causes. We implement QoE Doctor on Android, and systematically quantify various factors that impact app QoE, including the cellular radio link layer technology, carrier rate-limiting mechanisms, app design choices and user-side configuration options.},
}

@inproceedings{cobb2020_ux_s_with_online_status_indicators,
  title={User Experiences with Online Status Indicators},
  author={Cobb, Camille and Simko, Lucy and Kohno, Tadayoshi and Hiniker, Alexis},
  booktitle={Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
  publisher = {ACM},
  address = {Honolulu HI, USA},
  pages={1--12},
  year={2020},
  doi={https://doi.org/10.1145/3313831.3376240},
  abstract={Online status indicators (OSIs) improve online communication by helping users convey and assess availability, but they also let users infer potentially sensitive information about one another. We surveyed 200 smartphone users to understand the extent to which users are aware of information shared via OSIs and the extent to which this shapes their behavior. Despite familiarity with OSIs, participants misunderstand many aspects of OSIs, and they describe carefully curating and seeking to control their self-presentation via OSIs. Some users further report leveraging OSI-conveyed information for problematic and malicious purposes. Drawing on existing constructs of app dependence (i.e., when users contort their behavior to meet an app's demands) and app enablement (i.e., when apps enable users to engage in behaviors they feel good about), we demonstrate that current OSI design patterns promote app dependence, and we call for a shift toward OSI designs that are more enabling for users.},
}

@inproceedings{corner2018micromobile,
  note={Presents excellent approaches to getting a wide variety of people involved in micro research experiments inexpensively.},
  title={MicroMobile: Leveraging mobile advertising for large-scale experimentation},
  author={Corner, Mark D and Levine, Brian N},
  booktitle={Proceedings of the 16th Annual International Conference on Mobile Systems, Applications, and Services},
  pages={310--322},
  year={2018},
  organization={ACM}
}

@inproceedings{cramer2010_research_in_the_large_app_stores,
    author = {Cramer, Henriette and Rost, Mattias and Belloni, Nicolas and Bentley, Frank and Chincholle, Didier},
    title = {Research in the Large. Using App Stores, Markets, and Other Wide Distribution Channels in Ubicomp Research},
    year = {2010},
    isbn = {9781450302838},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/1864431.1864501},
    doi = {10.1145/1864431.1864501},
    booktitle = {Proceedings of the 12th ACM International Conference Adjunct Papers on Ubiquitous Computing - Adjunct},
    pages = {511–514},
    numpages = {4},
    keywords = {mass evaluation methods, mobile ecosystem, app stores, distribution channels, mobile interaction},
    address = {Copenhagen, Denmark},
    series = {UbiComp ’10 Adjunct}
}

@inproceedings{cummings2004automation,
  title={Automation bias in intelligent time critical decision support systems},
  author={Cummings, Mary},
  booktitle={AIAA 1st Intelligent Systems Technical Conference},
  pages={6},
  year={2004},
  publisher = {American Institute of Aeronautics and Astronautics},
  doi = {10.2514/6.2004-6313},
  address = {Chicago, Illinois, USA}
}

@inproceedings{deng2017_is_mutation_analysis_effective_at_testing_android_apps,
  author={L. {Deng} and J. {Offutt} and D. {Samudio}},
  booktitle={2017 IEEE International Conference on Software Quality, Reliability and Security (QRS)},
  title={Is Mutation Analysis Effective at Testing Android Apps?},
  year={2017},
  volume={},
  number={},
  pages={86-93},
  abstract={
    Not only is Android the most widely used mobile operating system, more apps have been released and downloaded for Android than for any other OS. However, quality is an ongoing problem, with many apps being released with faults, sometimes serious faults. Because the structure of mobile app software differs from other types of software, testing is difficult and traditional methods do not work. Thus we need different approaches to test mobile apps. In this paper, we identify challenges in testing Android apps, and categorize common faults according to fault studies. Then, we present a way to apply mutation testing to Android apps. Additionally, this paper presents results from two empirical studies on fault detection effectiveness using open-source Android applications: one for Android mutation testing, and another for four existing Android testing techniques. The studies use naturally occurring faults as well as crowdsourced faults introduced by experienced Android developers. Our results indicate that Android mutation testing is effective at detecting faults.},  
  keywords={Android (operating system);crowdsourcing;fault diagnosis;mobile computing;program diagnostics;program testing;public domain software;software reliability;mutation analysis;Android apps testing;mobile operating system;OS;mobile app software structure;fault detection;open-source Android applications;Android mutation testing;Androids;Humanoid robots;Testing;Crowdsourcing;XML;Wireless fidelity;Fault detection;Android;Software Testing;Mutation Testing;Empirical Evaluation;Crowdsourcing},  
  doi={10.1109/QRS.2017.19},  
  ISSN={},  
  month={July},
  publisher = {IEEE},
  address = {Prague, Czech Republic},
}

@inproceedings{diallo2015_correctness_and_relative_correctness,
  title={Correctness and relative correctness},
  author={Diallo, Nafi and Ghardallou, Wided and Mili, Ali},
  booktitle={2015 IEEE/ACM 37th IEEE International Conference on Software Engineering},
  volume={2},
  pages={591--594},
  year={2015},
  organization={IEEE},
  publisher={IEEE},
  address = {Florence, Italy},
}

@inproceedings{erdogdu2015_privacy_utility_tradeoff_under_continual_observation,
  author={M. A. {Erdogdu} and N. {Fawaz}},
  booktitle={2015 IEEE International Symposium on Information Theory (ISIT)},
  title={Privacy-utility trade-off under continual observation},
  publisher = {IEEE},
  address = {Hong Kong},
  year={2015},
  volume={},
  number={},
  pages={1801-1805},
  abstract={
    In the online setting, a user continuously releases a time-series that is correlated with his private data, to a service provider to derive some utility. Due to correlations, the continual observation of the time-series puts the user at risk of inference attacks against his private data. To protect the user's privacy, the time-series is randomized prior to its release according to a probabilistic privacy mapping. This mapping should be designed in a way that balances privacy and utility requirements over time. First, we formalize the framework for the design of utility-aware privacy mappings for time-series, under both online and batch models. We introduce two threat models, for which we respectively show that under the log-loss cost function, the information leakage can be modeled by the mutual or directed information between the randomized time-series and the private data. Second, we prove that the design of the privacy mapping can be cast as a convex optimization. We provide a sequential online scheme that allows to design privacy mappings at scale, that accounts for privacy risk from the history of released data and future releases to come. Third, we prove the equivalence of the optimal mappings under the batch and the online models, in the case of a Hidden Markov Model. Evaluations on real-world time-series data show that smart-meter data can be randomized to prevent disaggregation of per-device energy consumption, while maintaining the utility of the randomized series.},  
  keywords={convex programming;data privacy;time series;privacy-utility trade-off;user privacy protection;probabilistic privacy mapping;utility-aware privacy mapping;threat models;log-loss cost function;information leakage;randomized time-series;convex optimization;hidden Markov model;Privacy;Hidden Markov models;Data privacy;Aggregates;Data models;Distortion;Adaptation models},  
  doi={10.1109/ISIT.2015.7282766},
  ISSN={2157-8117},
  month={June},
}

@inproceedings{evans2020stuck,
  title={Stuck in Limbo with Magical Solutions: The Testers’ Lived Experiences of Tools and Automation},
  author={Evans, Isabel and Porter, Chris and Micallef, Mark and Harty, Julian},
  booktitle={Proceedings of the 15th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications},
  pages={195--202},
  year={2020},
  address = {Valletta, Malta},
  organization={SCITEPRESS-Science and Technology Publications},
  url = {http://oro.open.ac.uk/69980/},
  publisher = {{SciTePress Digital Library}},
  doi = {10.5220/0009091801950202},
}

@inproceedings{evans2020_test_tools_an_illusion_of_usability,  
  author={I. {Evans} and C. {Porter} and M. {Micallef} and J. {Harty}},
  booktitle={2020 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)},
  title={Test Tools: an illusion of usability?},
  year={2020},
  volume={},
  number={},
  pages={392-397},
  abstract={
    Software testing is vital, yet expensive and time-consuming. This essential part of the software development process includes testers performing many repeated actions in test execution and management. Use of automation and tools could reduce costs and timescale, while providing consistency by removing human error during repetitive activities. Challenges for successful tools and automation adoption have been identified both in academic research and in industry practice, including technical, managerial, skills-related and usability issues. We set out to investigate what usability improvements would aid successful tool adoption, and discovered that usability, while a necessary attribute, is not sufficient to ensure success, and the belief in usability as a sufficient cure for automation shelfware might be an illusory phenomenon which disguises potential difficulties when using tools longer term. This illusion of usability includes a belief that UI attractiveness is sufficient for tool usability, a belief that testers come from a narrow group of personas, and a belief that skill levels and requirements for tools are static. This may lead to frustration for testers, and therefore reluctance to use tools and automation. We summarise our findings and outline our proposed next research steps.},  
  keywords={human factors;program testing;software engineering;user interfaces;test tools;software testing;software development process;test execution;human error;automation shelfware;tool usability;UI attractiveness;Tools;Usability;Automation;Testing;Industries;ISO Standards;software testing;test tools;automation;usability;quality in use;user experience},  
  doi={10.1109/ICSTW50294.2020.00070},
  ISSN={},
  month={Oct},
  publisher = {IEEE},
  address = {Porto, Portugal},
}

@inproceedings{evans2021_scared_frustrated_and_quietly_proud,
    author = {Evans, Isabel and Porter, Chris and Micallef, Mark},
    title = {Scared, Frustrated and Quietly Proud: Testers’ Lived Experience of Tools and Automation},
    year = {2021},
    isbn = {9781450387576},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3452853.3452872},
    doi = {10.1145/3452853.3452872},
    abstract = {
      Software testing is vital, expensive, time-consuming yet a necessary part of software development. Testers perform repeated actions during testing, where automation and tools could reduce costs, timescale and human error. However, challenges to tools adoption have been identified in academic research and industry, which are blockers to success with automation. In attempting to find whether testers were experiencing tool usability shortcomings, we followed an exploratory research path, collecting stories from over 100 test practitioners. We discovered a richer, more complex story than we expected. We realised that usability – while necessary – is not sufficient to enable success, and that other human factors challenge successful automation projects. In answering privately to questions about their experiences of tools and automation, testers expressed themselves in language that was more emotional and linked to their lived experience (LX) than we expected. We uncovered frustrations and fear, as well as pride. In this paper we present our findings so far about TX: The testers’ lived experience of tools and automation, and we suggest steps for future research.
    },
    booktitle = {European Conference on Cognitive Ergonomics 2021},
    articleno = {16},
    numpages = {7},
    keywords = {lived experience, user experience, software testing, test automation, human factors, test tools, usability},
    location = {Siena, Italy},
    series = {ECCE 2021}
}

@inproceedings{fan2018large,
  title={Large-scale analysis of framework-specific exceptions in Android apps},
  author={Fan, Lingling and Su, Ting and Chen, Sen and Meng, Guozhu and Liu, Yang and Xu, Lihua and Pu, Geguang and Su, Zhendong},
  booktitle={2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE)},
  pages={408--419},
  year={2018},
  organization={IEEE},
  doi={10.1145/3180155.3180222},
  abstract={Mobile apps have become ubiquitous. For app developers, it is a key priority to ensure their apps' correctness and reliability. However, many apps still suffer from occasional to frequent crashes, weakening their competitive edge. Large-scale, deep analyses of the characteristics of real-world app crashes can provide useful insights to guide developers, or help improve testing and analysis tools. However, such studies do not exist - this paper fills this gap. Over a four-month long effort, we have collected 16,245 unique exception traces from 2,486 open-source Android apps, and observed that framework-specific exceptions account for the majority of these crashes. We then extensively investigated the 8,243 framework-specific exceptions (which took six person-months): (1) identifying their characteristics (e.g., manifestation locations, common fault categories), (2) evaluating their manifestation via state-of-the-art bug detection techniques, and (3) reviewing their fixes. Besides the insights they provide, these findings motivate and enable follow-up research on mobile apps, such as bug detection, fault localization and patch generation. In addition, to demonstrate the utility of our findings, we have optimized Stoat, a dynamic testing tool, and implemented ExLocator, an exception localization tool, for Android apps. Stoat is able to quickly uncover three previously-unknown, confirmed/fixed crashes in Gmail and Google+; ExLocator is capable of precisely locating the root causes of identified exceptions in real-world apps. Our substantial dataset is made publicly available to share with and benefit the community.},
}

@inproceedings{ferre2017_extending_mobile_app_analytics_for_usability_test_logging,
author="Ferre, Xavier
and Villalba, Elena
and Julio, H{\'e}ctor
and Zhu, Hongming",
editor="Bernhaupt, Regina
and Dalvi, Girish
and Joshi, Anirudha
and K. Balkrishan, Devanuj
and O'Neill, Jacki
and Winckler, Marco",
title="Extending Mobile App Analytics for Usability Test Logging",
booktitle="Human-Computer Interaction -- INTERACT 2017",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="114--131",
abstract="Mobile application development is characterized by reduced development cycles and high time-to-market pressure. Usability evaluation in mobile applications calls for the application of cost-effective methods, specially adapted to such constraints. We propose extending the Google Analytics for Mobile Applications basic service to store specific low-level user actions of interest for usability evaluation purposes. The solution can serve both for lab usability testing, automating quantitative data gathering, and for logging real use after application release. It is based on identification of relevant user tasks and the detailed events worth gathering, instrumentation of specific code for data gathering, and subsequent data extraction for calculating relevant usability--related variables. We validated our application in a real usability test by comparing the automatically gathered data with the information gathered by the human observer. Results shows both measurements are statistically exchangeable, opening promising new ways to perform usability testing cost-effectively and at greater scale.",
isbn="978-3-319-67687-6"
}

@inproceedings{flajolet2007_hyper_log_log,
  TITLE = {{HyperLogLog: the analysis of a near-optimal cardinality estimation algorithm}},
  AUTHOR = {Flajolet, Philippe and Fusy, {\'E}ric and Gandouet, Olivier and Meunier, Fr{\'e}d{\'e}ric},
  URL = {https://hal.inria.fr/hal-00406166},
  BOOKTITLE = {{AofA: Analysis of Algorithms}},
  ADDRESS = {Juan les Pins, France},
  EDITOR = {Jacquet, Philippe},
  PUBLISHER = {{Discrete Mathematics and Theoretical Computer Science}},
  SERIES = {DMTCS Proceedings},
  VOLUME = {DMTCS Proceedings vol. AH, 2007 Conference on Analysis of Algorithms (AofA 07)},
  PAGES = {137-156},
  YEAR = {2007},
  MONTH = Jun,
  KEYWORDS = {cardinality estimation ; Probabilistic algorithm},
  PDF = {https://hal.inria.fr/hal-00406166v2/file/dmAH0110.pdf},
  HAL_ID = {hal-00406166},
  HAL_VERSION = {v2},
}

@inproceedings{heule2013_hyper_log_log_in_practice,
    author = {Heule, Stefan and Nunkesser, Marc and Hall, Alexander},
    title = {HyperLogLog in Practice: Algorithmic Engineering of a State of the Art Cardinality Estimation Algorithm},
    year = {2013},
    isbn = {9781450315975},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/2452376.2452456},
    doi = {10.1145/2452376.2452456},
    abstract = {Cardinality estimation has a wide range of applications and is of particular importance in database systems. Various algorithms have been proposed in the past, and the HyperLogLog algorithm is one of them. In this paper, we present a series of improvements to this algorithm that reduce its memory requirements and significantly increase its accuracy for an important range of cardinalities. We have implemented our proposed algorithm for a system at Google and evaluated it empirically, comparing it to the original HyperLogLog algorithm. Like HyperLogLog, our improved algorithm parallelizes perfectly and computes the cardinality estimate in a single pass.},
    booktitle = {Proceedings of the 16th International Conference on Extending Database Technology},
    pages = {683–692},
    numpages = {10},
    address = {Genoa, Italy},
    series = {EDBT '13}
}

@inproceedings{khalid2014_prioritizing_the_devices_to_test_your_app_on_casestudy_android_games,
    author = {Khalid, Hammad and Nagappan, Meiyappan and Shihab, Emad and Hassan, Ahmed E.},
    title = {Prioritizing the Devices to Test Your App on: A Case Study of Android Game Apps},
    year = {2014},
    isbn = {9781450330565},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/2635868.2635909},
    doi = {10.1145/2635868.2635909},
    abstract = { Star ratings that are given by the users of mobile apps directly impact the revenue
    of its developers. At the same time, for popular platforms like Android, these apps
    must run on hundreds of devices increasing the chance for device-specific problems.
    Device-specific problems could impact the rating assigned to an app, given the varying
    capabilities of devices (e.g., hardware and software). To fix device-specific problems
    developers must test their apps on a large number of Android devices, which is costly
    and inefficient. Therefore, to help developers pick which devices to test their apps
    on, we propose using the devices that are mentioned in user reviews. We mine the user
    reviews of 99 free game apps and find that, apps receive user reviews from a large
    number of devices: between 38 to 132 unique devices. However, most of the reviews
    (80%) originate from a small subset of devices (on average, 33%). Furthermore, we
    find that developers of new game apps with no reviews can use the review data of similar
    game apps to select the devices that they should focus on first. Finally, among the
    set of devices that generate the most reviews for an app, we find that some devices
    tend to generate worse ratings than others. Our findings indicate that focusing on
    the devices with the most reviews (in particular the ones with negative ratings),
    developers can effectively prioritize their limited Quality Assurance (QA) efforts,
    since these devices have the greatest impact on ratings. },
    booktitle = {Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
    pages = {610–620},
    numpages = {11},
    keywords = {Mobile apps, Android fragmentation, Device prioritization},
    address = {Hong Kong, China},
    series = {FSE 2014}
}

@inproceedings{kinshuman2009_debugging_in_the_very_large,
    author = {Glerum, Kirk and Kinshumann, Kinshuman and Greenberg, Steve and Aul, Gabriel and Orgovan, Vince and Nichols, Greg and Grant, David and Loihle, Gretchen and Hunt, Galen},
    title = {Debugging in the (Very) Large: Ten Years of Implementation and Experience},
    year = {2009},
    isbn = {9781605587523},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/1629575.1629586},
    doi = {10.1145/1629575.1629586},
    abstract = {Windows Error Reporting (WER) is a distributed system that automates the processing
    of error reports coming from an installed base of a billion machines. WER has collected
    billions of error reports in ten years of operation. It collects error data automatically
    and classifies errors into buckets, which are used to prioritize developer effort
    and report fixes to users. WER uses a progressive approach to data collection, which
    minimizes overhead for most reports yet allows developers to collect detailed information
    when needed. WER takes advantage of its scale to use error statistics as a tool in
    debugging; this allows developers to isolate bugs that could not be found at smaller
    scale. WER has been designed for large scale: one pair of database servers can record
    all the errors that occur on all Windows computers worldwide.},
    booktitle = {Proceedings of the ACM SIGOPS 22nd Symposium on Operating Systems Principles},
    pages = {103–116},
    numpages = {14},
    keywords = {blue screen of death, statistics-based debugging., error reports, bucketing, classifying, minidump, labeling},
    address = {Big Sky, Montana, USA},
    series = {SOSP '09}
}

@inproceedings{kong2019_mining_android_crash_fixes,
  author = {Kong, Pingfan and Li, Li and Gao, Jun and Bissyand\'{e}, Tegawend\'{e} F. and Klein, Jacques},
  title = {Mining Android Crash Fixes in the Absence of Issue- and Change-Tracking Systems},
  year = {2019},
  isbn = {9781450362245},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3293882.3330572},
  doi = {10.1145/3293882.3330572},
  abstract = {
    Android apps are prone to crash. This often arises from the misuse of Android framework APIs, making it harder to debug since official Android documentation does not discuss thoroughly potential exceptions.Recently, the program repair community has also started to investigate the possibility to fix crashes automatically. Current results, however, apply to limited example cases. In both scenarios of repair, the main issue is the need for more example data to drive the fix processes due to the high cost in time and effort needed to collect and identify fix examples. We propose in this work a scalable approach, CraftDroid, to mine crash fixes by leveraging a set of 28 thousand carefully reconstructed app lineages from app markets, without the need for the app source code or issue reports. We developed a replicative testing approach that locates fixes among app versions which output different runtime logs with the exact same test inputs. Overall, we have mined 104 relevant crash fixes, further abstracted 17 fine-grained fix templates that are demonstrated to be effective for patching crashed apks. Finally, we release ReCBench, a benchmark consisting of 200 crashed apks and the crash replication scripts, which the community can explore for evaluating generated crash-inducing bug patches.},
  booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis (ISSTA 2019)},
  pages = {78–89},
  numpages = {12},
  keywords = {debugging, mining software repository, Android, crash, testing},
  address = {Beijing, China},
  series = {ISSTA 2019}
}


@inproceedings{harty2020_fast_abstract_data_dynamics_for_testing_systems,
  author={Harty, Julian},
  booktitle={2020 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)}, 
  title={Fast Abstract: Data Dynamics for Testing Systems}, 
  year={2020},
  volume={},
  number={},
  pages={491-492},
  abstract={Data is the lifeblood of business systems, and accordingly having useful data for testing is both an interesting research challenge and a headache for many involved in projects in industry. There are few good answers of where and how to source such data cost- and time- effectively. In the author's experience across various industries globally the state of practice is poor. The poor practices potentially compromises both the projects and potentially the reputation and revenues of businesses, especially given fines based on a percentage of turnover. This paper aims to stimulate discussion and research into ways to first understand and then devise safe yet potent data sets for testing systems where the data are appropriate to the needs and context of the software project.},
  keywords={},
  doi={10.1109/ICSTW50294.2020.9374728},
  ISSN={},
  month={Oct},
  address = {Porto, Portugal},
  publisher = {IEEE},
  organization = {IEEE},
  }


@inproceedings{febrero2017_software_reliability_as_user_perception,
  author={F. {Febrero} and M. A. {Moraga} and C. {Calero}},
  booktitle={2017 IEEE International Conference on Software Quality, Reliability and Security (QRS)},
  title={Software Reliability as User Perception: Application of the Fuzzy Analytic Hierarchy Process to Software Reliability Analysis},
  year={2017},
  volume={},
  number={},
  pages={224-231},
  abstract={
    Software Quality is a multidimensional concept for which Reliability is considered as a key attribute. Notwithstanding, due to its conceptual complexity, there is no common agreement on what Software Reliability is, thus different stakeholders use a variety of Software Reliability views. With the aim to improve our understanding of what Software Reliability means for industrial stakeholders as well as that of contributing to enhance the industrial applicability of Software Quality Models we propose approaching Software Reliability analysis by using structural model based on representative International Standards, which are industry-oriented. As analysis method, since interested on stakeholders' vision of Reliability we will apply the Fuzzy Analytic Hierarchical Process (FAHP) which is designed to manage human assessment, always characterized by a certain degree of vagueness and subjectivity. The rationality of the proposed model and feasibility of the analysis method is proved by the application on a very large industrial system which provides empirical evidence on the conceptual descriptiveness capturing stakeholders' views and industrial applicability in an efficient manner.},  
  keywords={analytic hierarchy process;fuzzy set theory;human factors;software reliability;user perception;fuzzy analytic hierarchy process;software reliability analysis;FAHP;Software reliability;Software;Stakeholders;Analytical models;Standards;Industries;Empirical Software Engineering;Fuzzy Analytical Hierarchical Process;Quality Analysis and Evaluation;Software Reliability},
  ISSN={},
  month={July},
  publisher = {IEEE},
  address = {Prague, Czech Republic},
  doi = {10.1109/QRS.2017.33},
}

@inproceedings{frankl1997choosing_testing_for_reliability,
  title={Choosing a testing method to deliver reliability},
  author={Frankl, Phyllis and Hamlet, Dick and Littlewood, Bev and Strigini, Lorenzo},
  booktitle={Proceedings of the 19th international conference on Software engineering},
  address = {Boston, Massachusetts, USA},
  publisher = {ACM},
  pages={68--78},
  year={1997}
}

@inproceedings{fu2013people,
  title={Why people hate your app: Making sense of user feedback in a mobile app store},
  author={Fu, Bin and Lin, Jialiu and Li, Lei and Faloutsos, Christos and Hong, Jason and Sadeh, Norman},
  booktitle={Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining},
  publisher = {ACM},
  address = {New York NY, USA},
  pages={1276--1284},
  year={2013}
}

@inproceedings{gao2014mobile,
  title={Mobile Testing-as-a-Service (MTaaS)--Infrastructures, Issues, Solutions and Needs},
  author={Gao, Jerry and Tsai, Wei-Tek and Paul, Ray and Bai, Xiaoying and Uehara, Tadahiro},
  booktitle={2014 IEEE 15th International Symposium on High-Assurance Systems Engineering},
  pages={158--167},
  year={2014},
  organization={IEEE},
  publisher = {IEEE},
  address = {Miami Beach, FL, USA},
  abstract = {With the rapid advance of mobile computing technology and wireless networking, there is a significant increase of mobile subscriptions. This drives a strong demand for development and validation of mobile APPs and SaaS applications on mobile web. This paper is written to offer informative and insightful discussion about mobile testing-as-a-service (MTaaS), including its basic concepts, motivations, distinct features and requirements, test environments, and different approaches. Moreover, it presents a test process in MTaaS and three different approaches. Furthermore, the paper proposes one mobile test cloud infrastructure for mobile TaaS, and discusses the required mobile test frameworks and environments. Finally, the paper addresses existing issues, challenges, and emergent needs.},
  convo={yes},
}

@inproceedings{geiger2018_a_graph_based_dataset_of_commit_history_of_realworld_android_apps,
    author = {Geiger, Franz-Xaver and Malavolta, Ivano and Pascarella, Luca and Palomba, Fabio and Di Nucci, Dario and Bacchelli, Alberto},
    title = {A Graph-Based Dataset of Commit History of Real-World Android Apps},
    year = {2018},
    isbn = {9781450357166},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/3196398.3196460},
    doi = {10.1145/3196398.3196460},
    abstract = {Obtaining a good dataset to conduct empirical studies on the engineering of Android apps is an open challenge. To start tackling this challenge, we present AndroidTimeMachine, the first, self-contained, publicly available dataset weaving spread-out data sources about real-world, open-source Android apps. Encoded as a graph-based database, AndroidTimeMachine concerns 8,431 real open-source Android apps and contains: (i) metadata about the apps' GitHub projects, (ii) Git repositories with full commit history and (iii) metadata extracted from the Google Play store, such as app ratings and permissions.},
    booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
    pages = {30–33},
    numpages = {4},
    keywords = {mining software repositories, dataset, Android},
    address = {Gothenburg, Sweden},
    series = {MSR '18}
}

@inproceedings{gray1986_why_do_computers_stop_and_what_can_be_done_about_it,
  author    = {Jim Gray},
  title     = {Why Do Computers Stop and What Can Be Done About It?},
  booktitle = {Fifth Symposium on Reliability in Distributed Software and Database
               Systems, {SRDS} 1986, Los Angeles, California, USA, January 13-15,
               1986, Proceedings},
  pages     = {3--12},
  publisher = {{IEEE} Computer Society},
  year      = {1986},
  month     = {Jan},
  address    = {Los Angeles, California, USA},
  timestamp = {Mon, 06 Nov 2017 16:35:11 +0100},
  biburl    = {https://dblp.org/rec/conf/srds/Gray86.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  exclusion_rationale = {
    Ancient work far removed from mobile analytics. It might be more relevant to cite: Why Do Internet Services Fail, and What Can Be Done About It? or even write something similar for why do mobile apps fail, etc.?
    
    BTW the technical report available online is dated 1985, I wonder if the 1986 paper is identical in terms of the contents?
  },
  inclusion_rationale = {
   It's the likely source of the concept of Heisenbug which I want to mention in the related work chapter.
  },
}

@inproceedings{greenheld2018_automating_developers_responses_to_app_reviews,
  author={G. {Greenheld} and B. T. R. {Savarimuthu} and S. A. {Licorish}},
  booktitle={2018 25th Australasian Software Engineering Conference (ASWEC)}, 
  title={Automating Developers' Responses to App Reviews}, 
  year={2018},
  volume={},
  number={},
  pages={66-70},
  publisher = {IEEE},
  address = {Adelaide, SA, Australia},
  abstract={App reviews are important as they contain valuable information for improving the quality of such software systems. To this end, most users providing app reviews expect a response; to the extent that prior research has shown that when developers respond to app reviews these responses improve app ratings and users' satisfaction. However, unfortunately, user reviews largely go unanswered for most apps due to the high prevalence of reviews. This challenge may be addressed by creating a system that automatically generates responses having learned from the responses already posted by developers for a given app. These generated responses may then be modified by developers if required. This work presents a system that recommends socially-acceptable responses based on principles adopted from three domains: information retrieval, social norms and userinterface design. We then evaluate the newly developed system against Google Play's de facto review response system, which requires developers to write responses manually. The evaluation of the two systems involved measuring participants' feedback on three aspects - usability, cognitive load and performance. The goal of the evaluation was to investigate whether users prefer the new system over the existing system. Our outcomes show that there were statistically significant differences between the two systems on all three aspects evaluated, with users preferring the newly proposed system over Google Play's response system. In particular, the proposed system has the potential to reduce the overall workload of developers considerably.},
  keywords={formal specification;information retrieval;Internet;mobile computing;user interfaces;software systems;app reviews;app ratings;user reviews;socially-acceptable responses;automating developers;Google Play response system;Google Play de facto review response system;Google;Task analysis;User interfaces;Information retrieval;Information science;Atmospheric measurements;Particle measurements;app review;response recommendation;social norms;user interface design;information retrieval},
  doi={10.1109/ASWEC.2018.00017},
  ISSN={2377-5408},
  month={Nov},
}

@inproceedings{godinho2016open,
  title={Open Device Lab: An Analysis of Available Devices in the Gaming Market.},
  abstract={Device Lab is a worldwide network of laboratories providing a community pool of internet connected devices. Quality assurance across real devices is needed to solve device-specific problems. The community proposal is to enable anyone a free-to-use lab to test their work. Building on the gaming market categories we carried out an analysis of community devices that can serve this market to evaluate their potential for collaboration. Our findings indicate that the majority of devices can serve the mobile game segment. An important alternative to resolve device-specific problems and improve gaming experience.},
  author={Godinho, Raquel Paiva and Contreras-Espinosa, Ruth S},
  booktitle={2016 8th International Conference on Games and Virtual Worlds for Serious Applications (VS-GAMES)},
  pages={1--4},
  year={2016},
  organization={IEEE}
}

@inproceedings{ghardallou2016debugging_without_testing,
  title={Debugging without testing},
  author={W. {Ghardallou} and N. {Diallo} and A. {Mili} and M. F. {Frias}},  
  booktitle={2016 IEEE International Conference on Software Testing, Verification and Validation (ICST)},
  pages={113--123},
  volume={},
  number={},
  year={2016},
  organization={IEEE},
  publisher = {IEEE},
  address = {Chicago, IL, USA.},
  abstract={It is so inconceivable to debug a program without testing it that these two words are used nearly interchangeably. Yet we argue that using the concept of relative correctness we can indeed remove a fault from a program and prove that the fault has been removed, by proving that the new program is more correct than the original. This is a departure from the traditional roles of proving and testing methods, whereby static proof methods are applied to a correct program to prove its correctness, and dynamic testing methods are applied to an incorrect program to expose its faults.},
  keywords={program debugging;program debugging;static proof methods;dynamic testing methods;Sufficient conditions;Mathematics;Conferences;Software testing;Debugging;Scalability;debugging;testing;correctness;relative correctness;faults;fault removal},  
  doi={10.1109/ICST.2016.12},  
  ISSN={},  
  month={April},
}

@inproceedings{gousios2012_ghtorrent_githubs_data_from_a_firehose,
  author={Gousios, Georgios and Spinellis, Diomidis},
  booktitle={2012 9th IEEE Working Conference on Mining Software Repositories (MSR)}, 
  title={GHTorrent: Github's data from a firehose}, 
  year={2012},
  volume={},
  number={},
  pages={12-21},
  abstract={A common requirement of many empirical software engineering studies is the acquisition and curation of data from software repositories. During the last few years, GitHub has emerged as a popular project hosting, mirroring and collaboration platform. GitHub provides an extensive REST API, which enables researchers to retrieve both the commits to the projects' repositories and events generated through user actions on project resources. GHTorrent aims to create a scalable off line mirror of GitHub's event streams and persistent data, and offer it to the research community as a service. In this paper, we present the project's design and initial implementation and demonstrate how the provided datasets can be queried and processed.},
  keywords={},
  doi={10.1109/MSR.2012.6224294},
  ISSN={2160-1860},
  month={June},
  publisher = {IEEE},
  address = {Zurich, Switzerland},
}

@comment{This is a provisional entry, hand crafted until the conference makes the paper available}
@inproceedings{guilardi_are_apps_ready_for_new_android_releases,
  title = {Are apps ready for new Android releases?},
  author = {Guilardi, Demetrio and Nicacio, Jalves and Napoleao, Bianca  and  Petrillo, Fabio},
  url = {https://conf.researchr.org/details/mobilesoft-2020/mobilesoft-2020-technical-papers/8/Are-apps-ready-for-new-Android-releases-},
  year = {2020},
  pages = {66 - 76},
  doi = {10.1145/3387905.3388598},
  abstract = {Context: Android operating system always brings new releases and updates to improve security, increase performance and bring a better user experience. When Google announces a new release, a whole chain of changes is triggered in cascade, causing many compatibility issues. Objective: This study focus at performing a quantitative and qualitative analysis on the state of apps readiness for new Android releases over time. Method: We performed an empirical study to map apps readiness to different Android versions. We developed a Repository Mining Tool to analyse 8420 open-source repositories, detecting 2118 Android projects and when they were adapted to different Android versions along their lifetimes. Results: Our results show that Android apps have became “less ready” over time. We found that 76.45\% of the analysed apps were ready for Android Lollipop 5.0 (API level 21) release, in October 2014. Though only 5.46\% were ready for Android 10 (API level 29), in September 2019. In addition, our results show that when apps are adapted to an Android version, 59.41\% perform the adaptation until the new Android release month, 95\% are adapted twelve months after the release, and 99.16\% are adapted two years later. Conclusion: Our findings reveal implications that affect not only the Android or mobile development research field and developers, they also reveal implications that points to Google’s policies and Android final users as well.},
  booktitle = {Proceedings of the 7th International Conference on Mobile Software Engineering and Systems},
  publisher = {ACM},
  address = {Virtual},
}

@inproceedings{Hamdi2021empirical,
  author={Hamdi, Oumayma and Ouni, Ali and AlOmar, Eman Abdullah and Ó Cinnéide, Mel and Mkaouer, Mohamed Wiem},  
  alternative_author={Hamdi, Oumayma and Ouni, Ali and AlOmar, Eman Abdullah and {\'O} Cinn{\'e}ide, Mel and Mkaouer, Mohamed Wiem},
  booktitle={2021 IEEE/ACM 8th International Conference on Mobile Software Engineering and Systems (MobileSoft)}, 
  title={An Empirical Study on the Impact of Refactoring on Quality Metrics in Android Applications},  
  year={2021}, 
  volume={},  
  number={},  
  pages={28-39},  
  abstract={
    Mobile applications must continuously evolve, sometimes under such time pressure that poor design or implementation choices are made, which inevitably result in structural software quality problems. Refactoring is the widely-accepted approach to ameliorating such quality problems. While the impact of refactoring on software quality has been widely studied in object-oriented software, its impact is still unclear in the context of mobile apps. This paper reports on the first empirical study that aims to address this gap. We conduct a large empirical study that analyses the evolution history of 300 open-source Android apps exhibiting a total of 42,181 refactoring operations. We analyze the impact of these refactoring operations on 10 common quality metrics using a causal inference method based on the Difference-in-Differences (DiD) model. Our results indicate that when refactoring affects the metrics it generally improves them. In many cases refactoring has no significant impact on the metrics, whereas one metric (LCOM) deteriorates overall as a result of refactoring. These findings provide practical insights into the current practice of refactoring in the context of Android app development.},  
  keywords={}, 
  doi={10.1109/MobileSoft52590.2021.00010}, 
  ISSN={}, 
  month={May},
  publisher = {IEEE},
  address = {Madrid, Spain},
}


@inproceedings{harty_better_android_apps_using_android_vitals,
    author = {Harty, Julian and M\"{u}ller, Matthias},
    title = {Better Android Apps Using Android Vitals},
    year = {2019},
    isbn = {9781450368582},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/3340496.3342761},
    doi = {10.1145/3340496.3342761},
    booktitle = {Proceedings of the 3rd ACM SIGSOFT International Workshop on App Market Analytics},
    pages = {26–32},
    numpages = {7},
    keywords = {Opensource, App development, Feedback and reputation, Android Vitals, Quality of apps},
    address = {Tallinn, Estonia},
    series = {WAMA 2019}
}

@inproceedings{harty2020_designing_engineering_onboarding,
    author = {Harty, Julian},
    title = {Designing Engineering Onboarding for 60+ Nationalities},
    year = {2020},
    isbn = {9781450370936},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/3372787.3390504},
    doi = {10.1145/3372787.3390504},
    abstract = {A large international engineering office in Germany needed to double in size in 12 months. We designed an onboarding programme within 3 months to help it do so efficaciously. We wanted to optimize for: fast iterations in the programme rollout, to keep the 'flywheel spinning' by reducing drag on current staff, rapid acceleration where new hires contributed quickly, and smooth integration where new hires adapted to the engineering, company, and country cultures.To reduce drag we onboarded in cohorts and involved existing practitioners in the design and discussion. To encourage contributions quickly we built contributions into the sessions, we also streamlined IT Support. To help new hires adopt the culture we encouraged help and mentoring within and across cohorts.For fast iterations, we incorporated existing islands of onboarding, involved local technical staff in design and delivery of hands-on training, and applied analytics to help improve the practice. And we launched early to bootstrap our learning and evaluation.Our approach worked; new hires were able to make meaningful contributions within a week and they scored the onboarding programme positively (8.5 NPS).},
    booktitle = {Proceedings of the 15th International Conference on Global Software Engineering},
    pages = {76–80},
    numpages = {5},
    keywords = {mob programming, collaboration in software development, onboarding, collaborative learning},
    address = {Seoul, Republic of Korea},
    series = {ICGSE '20}
}

@inproceedings{harty_google_play_console_insightful_development_using_android_vitals_and_pre_launch_reports,
author = {Harty, Julian},
title = {Google Play Console: Insightful Development Using Android Vitals and Pre-Launch Reports},
year = {2019},
publisher = {IEEE Press},
booktitle = {Proceedings of the 6th International Conference on Mobile Software Engineering and Systems},
pages = {62–65},
numpages = {4},
keywords = {software quality, Android vitals, mobile applications, Google dev console, pre-launch report, kiwix, metrics},
address = {Montreal, Quebec, Canada},
series = {MOBILESoft ’19}
}

@inproceedings{harty2020_how_can_software_testing_be_improved_by_analytics_to_deliver_better_apps,
  author = {Harty, Julian},
  booktitle={2020 IEEE 13th International Conference on Software Testing, Validation and Verification (ICST)},
  title={How Can Software Testing be Improved by Analytics to Deliver Better Apps?},
  year={2020},
  volume={},
  number={},
  pages={418-420}, 
  abstract={Many consider software testing to be necessary yet given the nature of testing and practical project constraints it cannot be comprehensive or complete. The resulting software has bugs including those that affect some users. Analytics of usage of apps may help illuminate testing that has been performed on existing releases and also inspire improvements to future testing. The Android ecosystem provides unusually rich analytics tools for developers of apps released in Google Play so my research focuses on this ecosystem to evaluate several analytics tools including Google Play Console, Android Vitals, which are integrated into the platform and the operating system, together with additional mobile analytics offerings from Google and Microsoft.},  
  keywords={mobile computing;program debugging;program testing;software tools;Android Vitals;software testing;mobile analytics;Google Play Console;Android ecosystem;future testing;Software;Androids;Humanoid robots;Tools;Google;Software testing;Android;Android Vitals;Apps;Crashlytics;Firebase;Mobile;Software Analytics;Software Testing},  
  doi={10.1109/ICST46399.2020.00052},  
  ISSN={2159-4848},
  month={Oct},
  address = {Porto, Portugal},
  publisher = {IEEE},
}

@inproceedings{harty_improving_app_quality_despite_flawed_mobile_analytics,
author = {Harty, Julian},
title = {Improving App Quality Despite Flawed Mobile Analytics},
year = {2020},
publisher = {IEEE Press},
booktitle = {Proceedings of the 7th International Conference on Mobile Software Engineering and Systems},
pages = {},
numpages = {2},
address = {Seoul, South Korea},
series = {MOBILESoft ’20}
}

@inproceedings{hecht2015approach,
  title={An approach to detect Android antipatterns},
  author={Hecht, Geoffrey},
  booktitle={2015 IEEE/ACM 37th IEEE International Conference on Software Engineering},
  volume={2},
  pages={766--768},
  year={2015},
  organization={IEEE},
  publisher = {IEEE},
  address = {Florence, Italy},
  convo={yes},
}

@inproceedings{hirsch2019approach_catrobat,
  title={An Approach to Test Classification in Big Android Applications},
  author={Hirsch, Thomas and Schindler, Christian and M{\"u}ller, Matthias and Schranz, Thomas and Slany, Wolfgang},
  booktitle={2019 IEEE 19th International Conference on Software Quality, Reliability and Security Companion (QRS-C)},
  pages={300--308},
  year={2019},
  organization={IEEE},
  publisher={IEEE},
  address={Sofia, Bulgaria}
}

@inproceedings{hsu2018_how_agile_impacts_a_software_corporation,  
  author={H. {Hsu} and Y. {Lin}},  
  booktitle={2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC)},   
  title={How Agile Impacts a Software Corporation: An Empirical Study},   
  year={2018},  
  volume={02},  
  number={},  
  pages={20-25},  
  abstract={Agile methods advocating early and frequent delivery, incremental and iterative development, and adaptive planning gradually becomes a popular software development methodology around the world. Many top software companies adopt agile methods and develop lots of remarkable software products. Although many researchers have studied agile methods using various approaches, whether and how agile methods help developers establish better software is still controversial. In this paper, we study the business data, development, and human resources records provided by Titansoft, a Singapore IT service corporation which is well-known in its successful Agile experiences. From analysis of the data, three primary impacts brought by agile adoption are revealed: (1) A temporary business stagnation; (2) Improvement in product quality; (3) Positive evolution in corporate culture. To the best of our knowledge, this paper is one of the few studies discussing how agile methods influence a software company in software development, business, and corporate culture aspects throughout a relatively long duration (more than 5 years).},  
  keywords={DP industry;project management;software development management;software engineering;software prototyping;software quality;incremental development;iterative development;popular software development methodology;software companies;agile methods;remarkable software products;agile adoption;Software;Companies;Programming;Collaboration;Electronic mail;Agile development, Data analysis, Coporate culture},  
  doi={10.1109/COMPSAC.2018.10197},  
  ISSN={0730-3157},  
  month={July},
}

@inproceedings{hu_tealeaf_cxmobile,  
  author={ {Yanke Hu}},  
  booktitle={2016 24th International Conference on Geoinformatics},   
  title={Tealeaf CxMobile - replaying real time customer experience},   
  year={2016},  
  volume={},  
  number={},  
  pages={1-4},
  address={Galway, Ireland},
  publisher={IEEE},
  abstract={With the rapid evolvement of mobile and cloud technology, improving and optimizing business mobile apps' customer experience has unprecedented importance. Mobile developers are expecting a tool that can help them know how customers are interacting with their apps, and how their apps react to the customers' behaviors. If there is any imperfection of the mobile app interface design or bugs being thrown out, they would want to know that as early as possible. This helps a lot to accelerate conversation rate and avoid business losses. There are numerous raw data analytics tools on the market to help those online businesses make improvements, but no tool can tell these businesses why and how their mobile customers are struggling. This paper introduces IBM Tealeaf CxMobile [1], which provides extensive analytics functions and expansive visibilities into the mobile customer experience of native mobile applications. IBM Tealeaf CxMobile is using the Modularization SDK methodology [2], which is essentially encapsulating core function modules into SDKs and centralize the backend reporting. With its unique Native Mobile Replay technology, IBM Tealeaf CxMobile can replay a user's journey through a native iOS or Android app in the way of “what you see is what you get” rather than mere data analysis. This paper will explain the principles of Tealeaf Native Replay and how it works on iOS and Android platforms. We also will give an example of integrating the Geo-Location logging functions into IBM Tealeaf CxMobile.},  
  
  keywords={Android (operating system);business data processing;customer services;data handling;iOS (operating system);mobile computing;optimisation;program debugging;real time customer experience;mobile technology;cloud technology;business mobile apps;customer experience;mobile developers;customer behaviors;mobile app interface design;raw data analytics tools;online businesses;IBM Tealeaf CxMobile;modularization SDK methodology;native iOS;Android app;geo-location logging functions;Business;Mobile communication;Airports;Mobile;Customer;Geo-Location;Enterprise;Analytics;Growth Hacking},  
  
  doi={10.1109/GEOINFORMATICS.2016.7578968},  
  ISSN={2161-0258},  
  month={Aug},
}

@inproceedings{huang2019_up_to_crash_3rdparty_libraries_on_android,
  author={Huang, Jie and Borges, Nataniel and Bugiel, Sven and Backes, Michael}, 
  booktitle={2019 IEEE European Symposium on Security and Privacy (EuroS P)}, 
  title={Up-To-Crash: Evaluating Third-Party Library Updatability on Android}, 
  year={2019}, 
  volume={}, 
  number={}, 
  publisher = {IEEE},
  address = {Stockholm, Sweden},
  pages={15-30}, 
  abstract={
    Buggy and flawed third-party libraries increase their host app's attack surface and put the users' privacy at risk. To avert this risk, libraries have to be kept updated to their newest versions by the app developers that integrate them into their projects. Recent researches revealed that the prevalence of outdated third-party libraries in Android apps is indeed a rampant problem, but also suggested that there is a great opportunity for drop-in replacements of outdated libraries, which would not even require cooperation by the app developers to update the libraries. However, all those conclusions are based on static app analysis, which can only provide an abstract view. In this work, we extend the updatability analysis to the runtime of apps. We implement a solution to update third-party libraries with drop-in replacements by their newer versions. To verify the feasibility of this developer-independent update mechanism, we dynamically test 3,000 real world apps for 3 popular libraries (78 library versions) for runtime failures stemming from incompatible library updates. To investigate the updatability of libraries in-depth, exploration enhanced dynamic testing is adopted to monitor the runtime behaviors of 15 apps before and after library updating. From our test, we find that the prior reported updatability rate is under real conditions overestimated by a factor of 1.57-2.06. Through root cause analysis, we find that the underlying problems prohibiting easy updates are intricate, such as deprecated functions, changed data structures, or entangled dependencies between different libraries and even the host app. We think our results not only put a more realistic light on the library updatability problem in Android, but also provide valuable insights for future solutions that provide automatic library updates or that try to support the app developers in better maintaining their external dependencies.},
  keywords={}, 
  doi={10.1109/EuroSP.2019.00012},
  ISSN={}, 
  month={June},
}


@inproceedings{jha2019_characterizing_android_specific_crash_bugs,
author = {Jha, Ajay Kumar and Lee, Sunghee and Lee, Woo Jin},
title = {Characterizing Android-Specific Crash Bugs},
year = {2019},
publisher = {IEEE Press},
abstract = {Android platform provides a unique framework for app development. Failure to comply
with the framework may result in serious bugs. Android platform is also evolving rapidly
and developers extensively use APIs provided by the framework, which may lead to serious
compatibility bugs if developers do not update the released apps frequently. Furthermore,
Android apps run on a wide range of memory-constrained devices, which may cause various
device-specific and memory-related bugs. There are several other Android-specific
issues that developers need to address during app development and maintenance. Failure
to address the issues may result in serious bugs manifested as crashes. In this paper,
we perform an empirical study to investigate and characterize various Android-specific
crash bugs, their prevalence, root causes, and solutions by analyzing 1,862 confirmed
crash reports of 418 open source Android apps. The investigation results can help
app developers in understanding, preventing, and fixing the Android-specific crash
bugs. Moreover, the results can help app developers and researchers in designing effective
bug detection tools for Android apps.},
booktitle = {Proceedings of the 6th International Conference on Mobile Software Engineering and Systems},
pages = {111–122},
numpages = {12},
keywords = {Android apps, crash bug analysis, mining crash bugs, characterizing crash bugs},
address = {Montreal, Quebec, Canada},
series = {MOBILESoft '19}
}

@inproceedings{johnson2013_why_dont_devs_use_static_analysis,
  title={Why don't software developers use static analysis tools to find bugs?},
  author={Johnson, Brittany and Song, Yoonki and Murphy-Hill, Emerson and Bowdidge, Robert},
  booktitle={2013 35th International Conference on Software Engineering (ICSE)},
  pages={672--681},
  year={2013},
  organization={IEEE},
  publisher = {IEEE},
  address = {San Francisco, CA. USA.},
}

@INPROCEEDINGS{joorabchi2013_real_challenges_in_mobile_app_development,
  author={Joorabchi, Mona Erfani and Mesbah, Ali and Kruchten, Philippe},  
  booktitle={2013 ACM / IEEE International Symposium on Empirical Software Engineering and Measurement}, 
  title={Real Challenges in Mobile App Development}, 
  year={2013},  
  volume={}, 
  number={}, 
  pages={15-24},
  publisher = {IEEE},
  address = {Baltimore, MD, USA},
  abstract={
    Context: Mobile app development is a relatively new phenomenon that is increasing rapidly due to the ubiquity and popularity of smartphones among end-users. Objective: The goal of our study is to gain an understanding of the main challenges developers face in practice when they build apps for different mobile devices. Method: We conducted a qualitative study, following a Grounded Theory approach, in which we interviewed 12 senior mobile developers from 9 different companies, followed by a semi-structured survey, with 188 respondents from the mobile development community. Results: The outcome is an overview of the current challenges faced by mobile developers in practice, such as developing apps across multiple platforms, lack of robust monitoring, analysis, and testing tools, and emulators that are slow or miss many features of mobile devices. Conclusion: Based on our findings of the current practices and challenges, we highlight areas that require more attention from the research and development community.},  
  keywords={},  
  doi={10.1109/ESEM.2013.9},
  ISSN={1949-3789}, 
  month={Oct},
}
  
@inproceedings{kalliamvakou2014_promises_and_perils_of_mining_github,
    author = {Kalliamvakou, Eirini and Gousios, Georgios and Blincoe, Kelly and Singer, Leif and German, Daniel M. and Damian, Daniela},
    title = {The Promises and Perils of Mining GitHub},
    year = {2014},
    isbn = {9781450328630},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/2597073.2597074},
    doi = {10.1145/2597073.2597074},
    abstract = { With over 10 million git repositories, GitHub is becoming one of the most important source of software artifacts on the Internet. Researchers are starting to mine the information stored in GitHub's event logs, trying to understand how its users employ the site to collaborate on software. However, so far there have been no studies describing the quality and properties of the data available from GitHub. We document the results of an empirical study aimed at understanding the characteristics of the repositories in GitHub and how users take advantage of GitHub's main features---namely commits, pull requests, and issues. Our results indicate that, while GitHub is a rich source of data on software development, mining GitHub for research purposes should take various potential perils into consideration. We show, for example, that the majority of the projects are personal and inactive; that GitHub is also being used for free storage and as a Web hosting service; and that almost 40\% of all pull requests do not appear as merged, even though they were. We provide a set of recommendations for software engineering researchers on how to approach the data in GitHub. },
    booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
    pages = {92–101},
    numpages = {10},
    keywords = {Mining software repositories, github, code reviews, git, bias},
    address = {Hyderabad, India},
    series = {MSR 2014}
}  
  
@inproceedings{kidwell2015_toward_fault_taxonomy_application_of_software_analytics,
  title={Toward a learned project-specific fault taxonomy: application of software analytics},
  author={Kidwell, Billy and Hayes, Jane Huffman},
  booktitle={2015 IEEE 1st International Workshop on Software Analytics (SWAN)},
  pages={1--4},
  year={2015},
  organization={IEEE},
  publisher = {IEEE},
  address = {Montreal, QC, Canada},
  doi = {10.1109/SWAN.2015.7070479},
  abstract = {This position paper argues that fault classification provides vital information for software analytics, and that machine learning techniques such as clustering can be applied to learn a project- (or organization-) specific fault taxonomy. Anecdotal evidence of this position is presented as well as possible areas of research for moving toward the posited goal.},
}

@inproceedings{kimbler_app_store_strategies_2010,  
    author={K. {Kimbler}},  
    booktitle={2010 14th International Conference on Intelligence in Next Generation Networks},   
    title={App store strategies for service providers},   
    year={2010},  
    volume={},  
    number={},  
    pages={1-5},  
    abstract={Apple App Store has introduced a substantially different model for digital good distribution to mobile users. Following Apple's spectacular success other smartphone and mobile OS vendors have started similar services. Mobile operators who invest hundreds of millions in 2,5G, 3G and now 4G technologies have a right to participate in the digital economy they enable. Can they quickly bite into the app store business and turn it to their advantage or they will be out of game again? This paper will try to answer these important questions by analysing the app store market and investigating potential app store strategies and scenarios for mobile operators.},  keywords={retailing;app store strategies;service providers;Apple app store;digital good distribution;smartphone;mobile OS vendors;digital economy;app store market;Mobile communication;Business;Smart phones;Mobile handsets;Games;app stores;business models;strategy},  doi={10.1109/ICIN.2010.5640947},
    month={Oct},
    address={Berlin, Germany},
    publisher={IEEE},
}

@inproceedings{lee2014_user_interaction_based_profiling_system_for_android_app_tuning,
    author = {Lee, Seokjun and Yoon, Chanmin and Cha, Hojung},
    title = {User Interaction-Based Profiling System for Android Application Tuning},
    year = {2014},
    isbn = {9781450329682},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/2632048.2636091},
    doi = {10.1145/2632048.2636091},
    abstract = {Quality improvement in mobile applications should be based on the consideration of several factors, such as users' diversity in spatio-temporal usage, as well as the device's resource usage, including battery life. Although application tuning should consider this practical issue, it is difficult to ensure the success of this process during the development stage due to the lack of information about application usage. This paper proposes a user interaction-based profiling system to overcome the limitations of development-level application debugging. In our system, the analysis of both device behavior and energy consumption is possible with fine-grained process-level application monitoring. By providing fine-grained information, including user interaction, system behavior, and power consumption, our system provides meaningful analysis for application tuning. The proposed method does not require the source code of the application and uses a web-based framework so that users can easily provide their usage data. Our case study with a few popular applications demonstrates that the proposed system is practical and useful for application tuning.},
    booktitle = {Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
    pages = {289–299},
    numpages = {11},
    keywords = {mobile application, tuning, debugging, webbased framework, profiling, user interaction},
    address = {Seattle, Washington},
    series = {UbiComp '14}
}

@INPROCEEDINGS{li2016_an_investigation_into_the_use_of_common_libraries_in_android_apps,  
    author={Li, Li and Bissyandé, Tegawendé F. and Klein, Jacques and Le Traon, Yves},
    booktitle={2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering (SANER)}, 
    title={An Investigation into the Use of Common Libraries in Android Apps}, 
    year={2016}, 
    volume={1}, 
    number={},
    publisher = {IEEE},
    address = {Osaka, Japan},
    pages={403-414}, 
    abstract={
      The packaging model of Android apps requires the entire code necessary for the execution of an app to be shipped into one single apk file. Thus, an analysis of Android apps often visits code which is not part of the functionality delivered by the app. Such code is often contributed by the common libraries which are used pervasively by all apps. Unfortunately, Android analyses, e.g., for piggybacking detection and malware detection, can produce inaccurate results if they do not take into account the case of library code, which constitute noise in app features. Despite some efforts on investigating Android libraries, the momentum of Android research has not yet produced a complete set of common libraries to further support in-depth analysis of Android apps. In this paper, we leverage a dataset of about 1.5 million apps from Google Play to harvest potential common libraries, including advertisement libraries. With several steps of refinements, we finally collect by far the largest set of 1,113 libraries supporting common functionality and 240 libraries for advertisement. We use the dataset to investigates several aspects of Android libraries, including their popularity and their proportion in Android app code. Based on these datasets, we have further performed several empirical investigations to confirm the motivations behind our work.},  
    keywords={},  
    doi={10.1109/SANER.2016.52}, 
    ISSN={},  
    month={March},
}

@inproceedings{li2020_where_shall_we_log,
  title = {Where Shall We Log? Studying and Suggesting Logging Locations in Code Blocks},
  author = {Zhenhao Li and Tse-Hsun (Peter) Chen and Weiyi Shang},
  year = {2020},
  publisher = {IEEE},
  pages = {12},
  booktitle={2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)},
  address = {Virtual Event, Australia},
}

@inproceedings{linares2013_api_change_and_fault_proneness_android,
  title={API change and fault proneness: a threat to the success of Android apps},
  author={Linares-V{\'a}squez, Mario and Bavota, Gabriele and Bernal-C{\'a}rdenas, Carlos and Di Penta, Massimiliano and Oliveto, Rocco and Poshyvanyk, Denys},
  booktitle={Proceedings of the 2013 9th joint meeting on foundations of software engineering},
  pages={477--487},
  year={2013},
  publisher = {ACM},
  address = {Sacramento, CA , USA},
  abstract = {
    During the recent years, the market of mobile software applications (apps) has maintained an impressive upward trajectory. Many small and large software development companies invest considerable resources to target available opportunities. As of today, the markets for such devices feature over 850K+ apps for Android and 900K+ for iOS. Availability, cost, functionality, and usability are just some factors that determine the success or lack of success for a given app. Among the other factors, reliability is an important criteria: users easily get frustrated by repeated failures, crashes, and other bugs; hence, abandoning some apps in favor of others.
    
    This paper reports a study analyzing how the fault- and change-proneness of APIs used by 7,097 (free) Android apps relates to applications' lack of success, estimated from user ratings. Results of this study provide important insights into a crucial issue: making heavy use of fault- and change-prone APIs can negatively impact the success of these apps.},
}

@inproceedings{linares2015_mining_android_app_execution_traces_etc,  
  author={M. {Linares-Vásquez} and M. {White} and C. {Bernal-Cárdenas} and K. {Moran} and D. {Poshyvanyk}},
  booktitle={2015 IEEE/ACM 12th Working Conference on Mining Software Repositories},
  title={Mining Android App Usages for Generating Actionable GUI-Based Execution Scenarios},
  year={2015},
  volume={},
  number={},
  pages={111-122},
  abstract={
    GUI-based models extracted from Android app execution traces, events, or source code can be extremely useful for challenging tasks such as the generation of scenarios or test cases. However, extracting effective models can be an expensive process. Moreover, existing approaches for automatically deriving GUI-based models are not able to generate scenarios that include events which were not observed in execution (nor event) traces. In this paper, we address these and other major challenges in our novel hybrid approach, coined as MONKEYLAB. Our approach is based on the Record→Mine→Generate→Validate framework, which relies on recording app usages that yield execution (event) traces, mining those event traces and generating execution scenarios using statistical language modeling, static and dynamic analyses, and validating the resulting scenarios using an interactive execution of the app on a real device. The framework aims at mining models capable of generating feasible and fully replayable (i.e., actionable) scenarios reflecting either natural user behavior or uncommon usages (e.g., corner cases) for a given app. We evaluated MONKEYLAB in a case study involving several medium-to-large open-source Android apps. Our results demonstrate that MONKEYLAB is able to mine GUI-based models that can be used to generate actionable execution scenarios for both natural and unnatural sequences of events on Google Nexus 7 tablets.},  
  keywords={Graphical user interfaces;Testing;Androids;Humanoid robots;Vocabulary;Analytical models;History;GUI models;mobile apps;mining execution traces and event logs;language models},  
  doi={10.1109/MSR.2015.18},  
  ISSN={2160-1860},  
  month={May},
  publisher = {IEEE},
  address = {Florence, Italy},
}

@inproceedings{linares2017_how_do_developers_test_android_apps,
  author={Linares-Vásquez, Mario and Bernal-Cardenas, Cárlos and Moran, Kevin and Poshyvanyk, Denys},
  booktitle={2017 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
  title={How do Developers Test Android Applications?}, 
  year={2017},
  volume={},
  number={},
  pages={613-622},
  abstract={Enabling fully automated testing of mobile applications has recently become an important topic of study for both researchers and practitioners. A plethora of tools and approaches have been proposed to aid mobile developers both by augmenting manual testing practices and by automating various parts of the testing process. However, current approaches for automated testing fall short in convincing developers about their benefits, leading to a majority of mobile testing being performed manually. With the goal of helping researchers and practitioners - who design approaches supporting mobile testing - to understand developer's needs, we analyzed survey responses from 102 open source contributors to Android projects about their practices when performing testing. The survey focused on questions regarding practices and preferences of developers/testers in-the-wild for (i) designing and generating test cases, (ii) automated testing practices, and (iii) perceptions of quality metrics such as code coverage for determining test quality. Analyzing the information gleaned from this survey, we compile a body of knowledge to help guide researchers and professionals toward tailoring new automated testing approaches to the need of a diverse set of open source developers.},
  keywords={},
  doi={10.1109/ICSME.2017.47},
  ISSN={},
  month={Sep.},
  publisher = {IEEE},
  address = {Shanghai, China},
}

@inproceedings{liu2015measurement,
  title={A measurement-based study on application popularity in android and {iOS} app stores},
  author={Liu, Wei and Zhang, Ge and Chen, Jun and Zou, Yuze and Ding, Wenchao},
  booktitle={Proceedings of the 2015 Workshop on Mobile Big Data},
  publisher = {ACM},
  organization = {ACM},
  address = {Hangzhou, China},
  pages={13--18},
  year={2015},
  doi = {https://doi.org/10.1145/2757384.2757392},
}

@inproceedings{lu2016_PRADA,
  author={X. {Lu} and X. {Liu} and H. {Li} and T. {Xie} and Q. {Mei} and D. {Hao} and G. {Huang} and F. {Feng}},
  booktitle={2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE)},
  title={PRADA: Prioritizing Android Devices for Apps by Mining Large-Scale Usage Data},
  year={2016},
  volume={},
  number={},
  pages={3-13},
  abstract={
    Selecting and prioritizing major device models are critical for mobile app developers to select testbeds and optimize resources such as marketing and quality-assurance resources. The heavily fragmented distribution of Android devices makes it challenging to select a few major device models out of thousands of models available on the market. Currently app developers usually rely on some reported or estimated general market share of device models. However, these estimates can be quite inaccurate, and more problematically, can be irrelevant to the particular app under consideration. To address this issue, we propose PRADA, the first approach to prioritizing Android device models for individual apps, based on mining large-scale usage data. PRADA adapts the concept of operational profiling (popularly used in software reliability engineering) for mobile apps - the usage of an app on a specific device model reflects the importance of that device model for the app. PRADA includes a collaborative filtering technique to predict the usage of an app on different device models, even if the app is entirely new (without its actual usage in the market yet), based on the usage data of a large collection of apps. We empirically demonstrate the effectiveness of PRADA over two popular app categories, i.e., Game and Media, covering over 3.86 million users and 14,000 device models collected through a leading Android management app in China.},  
  keywords={collaborative filtering;data mining;smart phones;PRADA;prioritizing android devices for apps;large-scale usage data mining;operational profiling;mobile apps;collaborative filtering technique;Android management app;Androids;Humanoid robots;Data models;Games;Testing;Data mining;Mobile communication;Mobile apps;Android fragmentation;prioritization;usage data},  
  doi={10.1145/2884781.2884828},
  ISSN={1558-1225},
  month={May},
  publisher = {IEEE},
  address = {Austin, TX, USA},
}

@inproceedings{luhana2018streamlining,
  title={Streamlining mobile app deployment with Jenkins and Fastlane in the case of Catrobat's pocket code},
  author={Luhana, Kirshan Kumar and Schindler, Christian and Slany, Wolfgang},
  booktitle={2018 IEEE International Conference on Innovative Research and Development (ICIRD)},
  pages={1--6},
  year={2018},
  organization={IEEE},
  publisher={IEEE},
  address={Bangkok},
}

@inproceedings{makhdoumi2014_from_information_bottleneck_to_the_privacy_funnel,
  author={A. {Makhdoumi} and S. {Salamatian} and N. {Fawaz} and M. {Médard}},
  booktitle={2014 IEEE Information Theory Workshop (ITW 2014)},
  title={From the Information Bottleneck to the Privacy Funnel},
  year={2014},
  volume={},
  number={},
  pages={501-505},
  abstract={
    We focus on the privacy-utility trade-off encountered by users who wish to disclose some information to an analyst, that is correlated with their private data, in the hope of receiving some utility. We rely on a general privacy statistical inference framework, under which data is transformed before it is disclosed, according to a probabilistic privacy mapping. We show that when the log-loss is introduced in this framework in both the privacy metric and the distortion metric, the privacy leakage and the utility constraint can be reduced to the mutual information between private data and disclosed data, and between non-private data and disclosed data respectively. We justify the relevance and generality of the privacy metric under the log-loss by proving that the inference threat under any bounded cost function can be upperbounded by an explicit function of the mutual information between private data and disclosed data. We then show that the privacy-utility tradeoff under the log-loss can be cast as the non-convex Privacy Funnel optimization, and we leverage its connection to the Information Bottleneck, to provide a greedy algorithm that is locally optimal. We evaluate its performance on the US census dataset. Finally, we characterize the optimal privacy mapping for the Gaussian Privacy Funnel.},
  keywords={data privacy;greedy algorithms;information bottleneck;privacy-utility trade-off;probabilistic privacy mapping;log-loss;privacy leakage;utility constraint;disclosed data;nonprivate data;privacy metric;bounded cost function;privacy-utility tradeoff;nonconvex privacy funnel optimization;greedy algorithm;US census dataset;optimal privacy mapping;Gaussian privacy funnel;Privacy;Data privacy;Optimization;Mutual information;Greedy algorithms;Distortion measurement},
  doi={10.1109/ITW.2014.6970882},
  ISSN={1662-9019},
  month={Nov},
}

@inproceedings{malavolta2020_android_runner,
    author = {Malavolta, Ivano and Grua, Eoin Martino and Lam, Cheng-Yu and de Vries, Randy and Tan, Franky and Zielinski, Eric and Peters, Michael and Kaandorp, Luuk},
    title = {A Framework for the Automatic Execution of Measurement-Based Experiments on Android Devices},
    year = {2020},
    isbn = {9781450381284},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3417113.3422184},
    doi = {10.1145/3417113.3422184},
    abstract = {Conducting measurement-based experiments is fundamental for assessing the quality of Android apps in terms of, e.g., energy consumption, CPU, and memory usage. However, orchestrating such experiments is not trivial as it requires large boilerplate code, careful setup of measurement tools, and the adoption of various empirical best practices scattered across the literature. All together, those factors are slowing down the scientific advancement and harming experiments' replicability in the mobile software engineering area.In this paper we present Android Runner (AR), a framework for automatically executing measurement-based experiments on native and web apps running on Android devices. In AR, an experiment is defined once in a descriptive fashion, and then its execution is fully automatic, customizable, and replicable. AR is implemented in Python and it can be extended with third-party profilers.AR has been used in more than 25 scientific studies primarily targeting performance and energy efficiency.},
    booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering Workshops},
    pages = {61–66},
    numpages = {6},
    location = {Virtual Event, Australia},
    series = {ASE '20}
}


@inproceedings{menzies2019take,
  title={Take Control:(On the Unreasonable Effectiveness of Software Analytics)},
  author={Menzies, Tim},
  booktitle={2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)},
  pages={265--266},
  year={2019},
  organization={IEEE},
  publisher={IEEE},
  address={Montreal, QC, Canada},
}

@inproceedings{merdes2006_ubiquitous_RATs_resource_aware_runtime_tests_improve_reliability,
  author = {Merdes, Matthias and Malaka, Rainer and Suliman, Dima and Paech, Barbara and Brenner, Daniel and Atkinson, Colin},
  title = {Ubiquitous RATs: How Resource-Aware Run-Time Tests Can Improve Ubiquitous Software Systems},
  year = {2006},
  isbn = {1595935851},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/1210525.1210538},
  doi = {10.1145/1210525.1210538},
  abstract = {
    In this paper we describe a new approach for increasing the reliability of ubiquitous software systems. This is achieved by executing tests at run-time. The individual software components are consequently accompanied by executable tests. We augment this well-known built-in test (BIT) paradigm by combining it with resource-awareness. Starting from the constraints for such resource-aware tests (RATs) we derive their design and describe a number of strategies for executing such tests under resource constraints as well as the necessary middleware. Our approach is especially beneficial to ubiquitous software systems due to their dynamic nature - which prevents a static verification of their reliability - and their inherent resource limitations.},
  booktitle = {Proceedings of the 6th International Workshop on Software Engineering and Middleware},
  pages = {55–62},
  numpages = {8},
  keywords = {MORABIT, resource-aware test (RAT), built-in test (BIT), ubiquitous software, run-time testing},
  address = {Portland, Oregon},
  series = {SEM '06},
  comments = {I've not cited this paper, it's here for future reference. If and when we use in-app analytics to drive testing this paper will be relevant.}
}

@inproceedings{mili2014_on_faults_and_faulty_programs,
  title={On faults and faulty programs},
  author={Mili, Ali and Frias, Marcelo F and Jaoua, Ali},
  booktitle={International Conference on Relational and Algebraic Methods in Computer Science},
  pages={191--207},
  year={2014},
  organization={Springer},
  publisher = {Springer},
  address = {Marienstatt, Germany},
  abstract = {A fault is an attribute of a program that precludes it from satisfying its specification; while this definition may sound clear-cut, it leaves many details unspecified. An incorrect program may be corrected in many different ways, involving different numbers of modifications. Hence neither the location nor the number of of faults may be defined in a unique manner; this, in turn, sheds a cloud of uncertainty on such concepts as fault density, and fault forecasting. In this paper, we present a more precise definition of a program fault, that has the following properties: it recognizes that the same incorrect behavior may be remedied in more than one way; it recognizes that removing a fault does not necessarily make the program correct, but may make it less incorrect (in a sense to be defined); it characterizes fault removals that make the program less incorrect, as opposed to fault removals that may remedy one aspect of program behavior at the expense of others; it recognizes that isolating a fault in a program is based on implicit assumptions about the remaining program parts; it identifies instances when a fault may be localized in a program with absolute certainty.},
  keywords = {faults, faulty programs, correctness, relative correctness, refinement, contingent fault, definite fault, fault removal, monotonic fault removal},
}

@inproceedings{miluzzo2010research_in_the_app_store_era,
  title={Research in the app store era: Experiences from the cenceme app deployment on the iphone},
  author={Miluzzo, Emiliano and Lane, Nicholas D and Lu, Hong and Campbell, Andrew T},
  booktitle = {UbiComp '10},
  pages = {4},
  year={2010},
  publisher = {ACM},
  address = {Copenhagen, Denmark},
}

@inproceedings{minelli2013_software_analytics_samoa,
  title={Software analytics for mobile applications--insights \& lessons learned},
  author={Minelli, Roberto and Lanza, Michele},
  booktitle={2013 17th European Conference on Software Maintenance and Reengineering},
  pages={144--153},
  year={2013},
  organization={IEEE},
  publisher={IEEE},
  address={Genova, Italy},
}

@inproceedings{moran2016_automatically_drr_android_app_crashes,  
  author={K. {Moran} and M. {Linares-Vásquez} and C. {Bernal-Cárdenas} and C. {Vendome} and D. {Poshyvanyk}},
  booktitle={2016 IEEE International Conference on Software Testing, Verification and Validation (ICST)},
  title={Automatically Discovering, Reporting and Reproducing Android Application Crashes},   
  year={2016},
  volume={},
  number={},
  pages={33-44},
  publisher = {IEEE},
  address = {Chicago, IL. USA.},
  abstract={Mobile developers face unique challenges when detecting and reporting crashes in apps due to their prevailing GUI event-driven nature and additional sources of inputs (e.g., sensor readings). To support developers in these tasks, we introduce a novel, automated approach called CRASHSCOPE. This tool explores a given Android app using systematic input generation, according to several strategies informed by static and dynamic analyses, with the intrinsic goal of triggering crashes. When a crash is detected, CRASHSCOPE generates an augmented crash report containing screenshots, detailed crash reproduction steps, the captured exception stack trace, and a fully replayable script that automatically reproduces the crash on a target device(s). We evaluated CRASHSCOPE's effectiveness in discovering crashes as compared to five state-of-the-art Android input generation tools on 61 applications. The results demonstrate that CRASHSCOPE performs about as well as current tools for detecting crashes and provides more detailed fault information. Additionally, in a study analyzing eight real-world Android app crashes, we found that CRASHSCOPE's reports are easily readable and allow for reliable reproduction of crashes by presenting more explicit information than human written reports.},
  keywords={Android (operating system);graphical user interfaces;mobile computing;program diagnostics;Android application crashes;mobile developers;GUI event-driven nature;CRASHSCOPE;systematic input generation;static analyses;dynamic analyses;crash reproduction steps;exception stack trace;fault information;human written reports;Computer crashes;Graphical user interfaces;Androids;Humanoid robots;Testing;Mobile communication;Systematics;android;crash reports;GUI-testing},
  doi={10.1109/ICST.2016.34},
  ISSN={},
  month={April},
}

@inproceedings{moran2017_crashscope_a_practical_tool_for_automated_testing_of_android_apps,  
  author={Moran, Kevin and Linares-Vasquez, Mario and Bernal-Cardenas, Carlos and Vendome, Christopher and Poshyvanyk, Denys},  
  booktitle={2017 IEEE/ACM 39th International Conference on Software Engineering Companion (ICSE-C)},   
  title={CrashScope: A Practical Tool for Automated Testing of Android Applications},   
  year={2017},  
  volume={},  
  number={},  
  pages={15-18},  
  abstract={
    Unique challenges arise when testing mobile applications due to their prevailing event-driven nature and complex contextual features (e.g. sensors, notifications). Current automated input generation approaches for Android apps are typically not practical for developers to use due to required instrumentation or platform dependence and generally do not effectively exercise contextual features. To better support developers in mobile testing tasks, in this demo we present a novel, automated tool called CRASHSCOPE. This tool explores a given Android app using systematic input generation, according to several strategies informed by static and dynamic analyses, with the intrinsic goal of triggering crashes. When a crash is detected, CRASHSCOPE generates an augmented crash report containing screen shots, detailed crash reproduction steps, the captured exception stack trace, and a fully replay able script that automatically reproduces the crash on a target device(s). Results of preliminary studies show that CRASHSCOPE is able to uncover about as many crashes as other state of the art tools, while providing detailed useful crash reports and test scripts to developers. Website: www.android-dev-tools.com/crashscope-home Video url: https://youtu.be/ii6S1JF6xDw.},
  keywords={},  
  doi={10.1109/ICSE-C.2017.16},  
  ISSN={},
  month={May},
  publisher = {IEEE},
  address = {Buenos Aires, Argentina},
}

@inproceedings{mueller2019_pocketcode,  
  author={Müller, Matthias and Schindler, Christian and Slany, Wolfgang},  
  booktitle={2019 IEEE/ACM 6th International Conference on Mobile Software Engineering and Systems (MOBILESoft)},   
  title={Pocket Code - A Mobile Visual Programming Framework for App Development},   
  year={2019},  
  volume={},  
  number={},  
  pages={140-143},  
  abstract={
    Software development more and more focuses on mobile and connected IoT solutions. Whereas a variety of frameworks is available for professional developers, also the need comes up to deliver apps fast for personal use or rapid prototyping. Innovative ideas, in whatever context, must be realizable without having a vast amount of resources or deep domain-knowledge needed. In this work we present Pocket Code, a free open source mobile visual coding framework for Android and iOS with various hardware extensions, enabling everyone to create powerful apps directly on mobiles in short time. Especially the included visual coding-bricks for Arduino boards and Raspberry Pis also allow more sophisticated programs not only in an educational context to be developed with Pocket Code. In contrast to existing solutions, the presented app does not require any PC setting and is therefore making the development of apps mobile and broadening it to an even wider audience.},
  keywords={},
  doi={10.1109/MOBILESoft.2019.00027},
  ISSN={},
  month={May},
  publisher = {IEEE},
  address = {Montreal, QC, Canada},
}

@inproceedings{nagappan2016_future_trends_in_sw_eng_for_mobile_apps,  
  author={M. {Nagappan} and E. {Shihab}},
  booktitle={2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering (SANER)},   
  title={Future Trends in Software Engineering Research for Mobile Apps},   
  year={2016},  
  volume={5},  
  number={},  
  pages={21-32},  
  abstract={
    There has been tremendous growth in the use of mobile devices over the last few years. This growth has fueled the development of millions of software applications for these mobile devices often called as 'apps'. Current estimates indicate that there are hundreds of thousands of mobile app developers. As a result, in recent years, there has been an increasing amount of software engineering research conducted on mobile apps to help such mobile app developers. In this paper, we discuss current and future research trends within the framework of the various stages in the software development life-cycle: requirements (including non-functional), design and development, testing, and maintenance. While there are several non-functional requirements, we focus on the topics of energy and security in our paper, since mobile apps are not necessarily built by large companies that can afford to get experts for solving these two topics. For the same reason we also discuss the monetizing aspects of a mobile app at the end of the paper. For each topic of interest, we first present the recent advances done in these stages and then we present the challenges present in current work, followed by the future opportunities and the risks present in pursuing such research.},  
  keywords={Mobile communication;Software engineering;Feature extraction;Software;Smart phones;Google;Mobile apps;Mining app markets},  
  doi={10.1109/SANER.2016.88},
  ISSN={},
  month={March},
  publisher = {IEEE},
  address = {Osaka, Japan},
}

@inproceedings{nayebi2016release,
  title={Release Practices for Mobile Apps--What do Users and Developers Think?},
  author={Nayebi, Maleknaz and Adams, Bram and Ruhe, Guenther},
  booktitle={2016 {IEEE} 23rd international conference on software analysis, evolution, and reengineering {(SANER)}},
  volume={1},
  pages={552--562},
  year={2016},
  organization={IEEE},
  publisher = {IEEE},
  address = {Suita, Japan},
}

@inproceedings{nayebi2017version,
  title={Which version should be released to app store?},
  author={Nayebi, Maleknaz and Farahi, Homayoon and Ruhe, Guenther},
  booktitle={2017 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
  pages={324--333},
  year={2017},
  organization={IEEE},
  publisher = {IEEE},
  address = {Toronto, ON, Canada}
}

@inproceedings{nitze2015_a_survey_on_mobile_users_sq_perceptions_and_expectations,  
  author={Nitze, André and Schmietendorf, Andreas},  
  booktitle={2015 IEEE Eighth International Conference on Software Testing, Verification and Validation Workshops (ICSTW)}, 
  title={A survey on mobile users' software quality perceptions and expectations}, 
  year={2015},
  volume={},
  number={}, 
  pages={1-2},
  abstract={
    Software quality can be looked at from many perspectives. Software quality assurance techniques can improve internal and external quality properties of applications. However, the users' perceptions of and expectations on the resulting software product are pivotal to the products' economic success. In this contribution the results of a survey on quality aspects of mobile applications (apps) with 144 participants are presented. The results are discussed in terms of implications for developers and product managers of mobile application development projects. This work is based on a previous evaluation of mobile quality assurance aspects of mobile application development.},  
  keywords={},
  doi={10.1109/ICSTW.2015.7107417},
  ISSN={}, 
  month={April},
  organization={IEEE},
  publisher = {IEEE},
  address = {Graz, Austria},
}

@inproceedings{nurmuradov2017_caret-hm-heatmapping-android-emulator,
    author = {Nurmuradov, Dmitry and Bryce, Renee},
    title = {Caret-HM: Recording and Replaying Android User Sessions with Heat Map Generation Using UI State Clustering},
    year = {2017},
    isbn = {9781450350761},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/3092703.3098231},
    doi = {10.1145/3092703.3098231},
    abstract = { The Caret-HM framework allows Android developers to record and replay user sessions
    and convert them into heatmaps. One advantage of our framework over existing solutions
    is that it allows developers to control the environment while simplifying the recording
    process by giving users access to their applications via a web browser. The heatmap
    generation using Android user sessions and clustering UI states is a unique feature
    of our framework. Heat maps allow developers to identify the usage of application
    features for testing and guiding business decisions. We provide a qualitative comparison
    to the existing solutions. The video with demonstration is available at https://www.youtube.com/watch?v=eMSNAKM1Bj4
    },
    booktitle = {Proceedings of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis},
    pages = {400–403},
    numpages = {4},
    keywords = {User Sessions, Web-based Android Emulator, Mobile, Heat Map, Android},
    address = {Santa Barbara, CA, USA},
    series = {ISSTA 2017}
}

@inproceedings{olesen2020_10_years_of_hackathons,
  author = {Falk Olesen, Jeanette and Halskov, Kim},
  title = {10 Years of Research With and On Hackathons},
  year = {2020},
  isbn = {9781450369749},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3357236.3395543},
  doi = {10.1145/3357236.3395543},
  abstract = {Hackathon formats have been praised for their potential for promoting innovative thinking and making in a short time-frame. For this reason, hackathons have also been embraced by many researchers who use hackathons as part of their research in various ways. Through an extensive review of 381 publications published during a 10 year time span, we document the multiple ways in which hackathons are embraced and used by researchers The paper contributes to a better understanding of hackathons as part of research by providing a broad overview as a resource for researchers. We identify three main motivations for using hackathons as part of research: 1) Structuring learning, 2) structuring processes, and 3) enabling participation. For each of the motivations, we identify research with hackathons, and research on hackathons as two main categories. Drawing on several examples from the review we discuss benefits and challenges of using hackathons as part of research.},
  booktitle = {Proceedings of the 2020 ACM Designing Interactive Systems Conference},
  pages = {1073–1088},
  numpages = {16},
  keywords = {research method, literature review, hackathons},
  address = {Eindhoven, Netherlands},
  series = {DIS '20}
}

@inproceedings{paakkonen2009_communication_in_testing,
  author    = {Tuula P{\"{a}}{\"{a}}akk{\"{o}}nen and
               Jorma Sajaniemi},
  title     = {Communication in Testing: Improvements for Testing Management},
  booktitle = {Proceedings of the 21st Annual Workshop of the Psychology of Programming
               Interest Group, {PPIG} 2009, Limerick, Ireland, June 24-26, 2009},
  pages     = {12},
  publisher = {Psychology of Programming Interest Group},
  address   = {Limerick, Ireland},
  year      = {2009},
  url       = {http://ppig.org/library/paper/communication-testing-improvements-testing-management},
  timestamp = {Fri, 08 Jun 2018 11:42:28 +0200},
  biburl    = {https://dblp.org/rec/conf/ppig/PaaakkonenS09.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  abstract = {
    Testing in companies with highly competitive environments has many opportunities and challenges. Testing is diversified, both contextually and geographically: there are many testing goals, many ongoing parallel projects, many testing phases and all this needs to be managed so that the full view of testing is visible for management and reporting. Testing is also linked to project communication and psychology: communication needs to be constructive, and testers and test leaders should have good interpersonal skills. It really does matter how a failure or test report is formulated.
    In order to localize testing problems in a large software-intensive company, we conducted a current state analysis via web-based questionnaire among experienced testing practitioners, most having at least 5 years experience in these tasks. The scope of the survey was decided to keep broad with the goal of finding future improvements within tool, process and method development. In this paper, we will concentrate on those survey results that have psychological underlying. Based on the results, we suggest a set of improvements for testing and, especially, test reporting. Implementation of these improvements is still underway, but the findings and suggestions provide insight into psychology of testing.
  },
  pdf = {https://ppig.org/files/2009-PPIG-21st-paakkonen.pdf},
}

@inproceedings{parate2016_RECKON_an_analytics_framework_for_app_developers_HP_AppPulseMobile,
    author = {Parate, Abhinav and Jain, Puneet and Kim, Kyu-Han},
    title = {RECKON: An Analytics Framework for App Developers},
    year = {2016},
    isbn = {9781450342544},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/2980147.2980154},
    doi = {10.1145/2980147.2980154},
    abstract = {This paper argues that there is a need to expand the capabilities of mobile app analytics beyond providing low-level insights about how efficiently a user can execute an action on a mobile app, to deeper insights about how efficiently a user can complete a task using the app (e.g., flight reservation). This paper presents RECKON, a framework for mobile app analytics, that provides insights about end-to-end user experience in completing tasks, without explicit effort from the app developers. RECKON identifies and extracts task-level information from an unlabeled data stream of user actions. Based on the extracted information, RECKON outputs various helpful metrics to the developers. We have implemented and evaluated RECKON with a popular travel assistant app with 122, 243 users for 30 days. Our evaluation results validate RECKON's performance in obtaining critical mobile app insights as well as demonstrate its wide applicability for developers.},
    booktitle = {Proceedings of the 2nd Workshop on Experiences in the Design and Implementation of Smart Objects},
    pages = {23–28},
    numpages = {6},
    keywords = {developer, app analytics, task, user interface},
    address = {New York City, New York},
    series = {SmartObjects '16}
}

@inproceedings{partachi2020_flexme_untangling_commits,
    author = {P\^{a}rundefinedachi, Profir-Petru and Dash, Santanu Kumar and Allamanis, Miltiadis and Barr, Earl T.},
    title = {Flexeme: Untangling Commits Using Lexical Flows},
    year = {2020},
    isbn = {9781450370431},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/3368089.3409693},
    doi = {10.1145/3368089.3409693},
    abstract = {Today, most developers bundle changes into commits that they submit to a shared code
    repository. Tangled commits intermix distinct concerns, such as a bug fix and a new
    feature. They cause issues for developers, reviewers, and researchers alike: they
    restrict the usability of tools such as git bisect, make patch comprehension more
    difficult, and force researchers who mine software repositories to contend with noise.
    We present a novel data structure, the ��-NFG, a multiversion Program Dependency Graph
    augmented with name flows. A ��-NFG directly and simultaneously encodes different
    program versions, thereby capturing commits, and annotates data flow edges with the
    names/lexemes that flow across them. Our technique, Flexeme, builds a ��-NFG from
    commits, then applies Agglomerative Clustering using Graph Similarity to that ��-NFG
    to untangle its commits. At the untangling task on a C# corpus, our implementation,
    Heddle, improves the state-of-the-art on accuracy by 0.14, achieving 0.81, in a fraction
    of the time: Heddle is 32 times faster than the previous state-of-the-art.},
    booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
    pages = {63–74},
    numpages = {12},
    keywords = {graph kernels, clustering, commint untangling},
    address = {Virtual Event, USA},
    series = {ESEC/FSE 2020}
}

@inproceedings{patro2013_capturing_mobile_experience_in_the_wild,
    author = {Patro, Ashish and Rayanchu, Shravan and Griepentrog, Michael and Ma, Yadi and Banerjee, Suman},
    title = {Capturing Mobile Experience in the Wild: A Tale of Two Apps},
    year = {2013},
    isbn = {9781450321013},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/2535372.2535391},
    doi = {10.1145/2535372.2535391},
    abstract = {We present a long term and large scale study of the experience of mobile users through
    two popular but contrasting applications in the wild. To conduct this study, we implemented
    a measurement framework and library, called Insight, which has been deployed on these
    two applications that are available through Apple's App Store and Google's Android
    Market. One of them, Parallel Kingdom (PK), is a popular massively multiplayer online
    role-playing game (MMORPG) which has over a million unique users distributed more
    than 120 countries. The other application, StudyBlue (SB), is an educational application
    with over 160,000 unique users. Our study spans most of the life of the PK game (more
    than 3 years) while our deployment with SB has been running for over a year now. We
    use Insight to collect diverse information about network behavior, application usage
    and footprints, platform statistics, user actions, and various factors affecting application
    revenues.},
    booktitle = {Proceedings of the Ninth ACM Conference on Emerging Networking Experiments and Technologies},
    pages = {199–210},
    numpages = {12},
    keywords = {network performance, application usage, mobile applications, mmorpg},
    address = {Santa Barbara, California, USA},
    series = {CoNEXT '13}
}

@inproceedings{pielot2015attention,
  title={When attention is not scarce-detecting boredom from mobile phone usage},
  author={Pielot, Martin and Dingler, Tilman and Pedro, Jose San and Oliver, Nuria},
  booktitle={Proceedings of the 2015 ACM international joint conference on pervasive and ubiquitous computing},
  pages={825--836},
  organization = {ACM},
  publisher = {ACM},
  address = {Osaka, Japan},
  year={2015}
}

@inproceedings{poeplau2014_execute_this_unsafe_android,
  title={Execute this! analyzing unsafe and malicious dynamic code loading in android applications.},
  author={Poeplau, Sebastian and Fratantonio, Yanick and Bianchi, Antonio and Kruegel, Christopher and Vigna, Giovanni},
  year={2014},
  pages={23-26},
  volume = {14},
  organization = {NDSS},
  publisher = {Internet Society},
  address = {San Diego, CA, USA},
  isbn = {1-891562-35-5},
}

@inproceedings{pokhrel2020_digitaltwin_for_cybersecurity,
  title={Digital Twin for Cybersecurity Incident Prediction: A Multivocal},
  author={Pokhrel, Abhishek and Katta, Vikash and Colomo-Palacios, Ricardo},
  year = {2020},
  booktitle={2nd International Workshop on Software Engineering Research \& Practices for the Internet of Things (SERP4IoT)},
  pages={671--678},
  organization={IEEE},
  publisher = {IEEE},
  address = {Seoul, South Korea},
}

@inproceedings{prochlo2017_strong_privacy_analytics_in_the_crowd_46411,
    author = {Bittau, Andrea and Erlingsson, \'{U}lfar and Maniatis, Petros and Mironov, Ilya and Raghunathan, Ananth and Lie, David and Rudominer, Mitch and Kode, Ushasree and Tinnes, Julien and Seefeld, Bernhard},
    title = {Prochlo: Strong Privacy for Analytics in the Crowd},
    year = {2017},
    isbn = {9781450350853},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/3132747.3132769},
    also_available_at = {https://arxiv.org/abs/1710.00901},
    doi = {10.1145/3132747.3132769},
    abstract = {The large-scale monitoring of computer users' software activities has become commonplace, e.g., for application telemetry, error reporting, or demographic profiling. This paper describes a principled systems architecture---Encode, Shuffle, Analyze (ESA)---for performing such monitoring with high utility while also protecting user privacy. The ESA design, and its Prochlo implementation, are informed by our practical experiences with an existing, large deployment of privacy-preserving software monitoring.With ESA, the privacy of monitored users' data is guaranteed by its processing in a three-step pipeline. First, the data is encoded to control scope, granularity, and randomness. Second, the encoded data is collected in batches subject to a randomized threshold, and blindly shuffled, to break linkability and to ensure that individual data items get "lost in the crowd" of the batch. Third, the anonymous, shuffled data is analyzed by a specific analysis engine that further prevents statistical inference attacks on analysis results.ESA extends existing best-practice methods for sensitive-data analytics, by using cryptography and statistical techniques to make explicit how data is elided and reduced in precision, how only common-enough, anonymous data is analyzed, and how this is done for only specific, permitted purposes. As a result, ESA remains compatible with the established workflows of traditional database analysis.Strong privacy guarantees, including differential privacy, can be established at each processing step to defend against malice or compromise at one or more of those steps. Prochlo develops new techniques to harden those steps, including the Stash Shuffle, a novel scalable and efficient oblivious-shuffling algorithm based on Intel's SGX, and new applications of cryptographic secret sharing and blinding. We describe ESA and Prochlo, as well as experiments that validate their ability to balance utility and privacy.},
    booktitle = {Proceedings of the 26th Symposium on Operating Systems Principles},
    pages = {441–459},
    numpages = {19},
    address = {Shanghai, China},
    series = {SOSP '17},
}

@inproceedings{rekimoto1999_time_machine_computing,
    author = {Rekimoto, Jun},
    title = {Time-Machine Computing: A Time-Centric Approach for the Information Environment},
    year = {1999},
    isbn = {1581130759},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/320719.322582},
    doi = {10.1145/320719.322582},
    abstract = {This paper describes the concept of Time-Machine Computing (TMC), a time-centric approach to organizing information on computers. A system based on Time-Machine Computing allows a user to visit the past and the future states of computers. When a user needs to refer to a document that he/she was working on at some other time, he/she can travel in the time dimension and the system restores the computer state at that time. Since the user's activities on the system are automatically archived, the user's daily workspace is seamlessly integrated into the information archive. The combination of spatial information management of the desktop metaphor and time traveling allows a user to organize and archive information without being bothered by folder hierarchies or the file classification problems that are common in today's desktop environments. TMC also provides a mechanism for linking multiple applications and external information sources by exchanging time information. This paper describes the key features of TMC, a time-machine desktop environment called “TimeScape,” and several time-oriented application integration examples.},
    booktitle = {Proceedings of the 12th Annual ACM Symposium on User Interface Software and Technology},
    pages = {45–54},
    numpages = {10},
    keywords = {desktop environment, information visualization, time-machine computing, document management, time traveling, inter-application communication},
    address = {Asheville, North Carolina, USA},
    series = {UIST '99}
}

@inproceedings{riganelli2019benchmark_android_data_loss_bugs,
  title={A benchmark of data loss bugs for android apps},
  author={Riganelli, Oliviero and Mobilio, Marco and Micucci, Daniela and Mariani, Leonardo},
  booktitle={2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR)},
  pages={582--586},
  year={2019},
  organization={IEEE},
  publisher = {IEEE},
  address = {Montreal, QC, Canada},
}

@inproceedings{sama2009using_jinjector,
  title={Using code instrumentation to enhance testing on J2ME: a lesson learned with JInjector},
  author={Sama, Michele and Harty, Julian},
  booktitle={Proceedings of the 10th workshop on Mobile Computing Systems and Applications},
  pages={1--6},
  year={2009},
  organization = {ACM},
  publisher = {ACM},
  address = {Santa Cruz, CA, USA}, 
  doi = {10.1145/1514411.1514424},
}

@inproceedings{sarro2015_feature_lifecycles_in_appstores,
    author={F. {Sarro} and A. A. {Al-Subaihin} and M. {Harman} and Y. {Jia} and W. {Martin} and Y. {Zhang}},  
    booktitle={2015 IEEE 23rd International Requirements Engineering Conference (RE)},   
    title={Feature lifecycles as they spread, migrate, remain, and die in App Stores},
    publisher = {IEEE},
    address = {Ottawa, ON, Canada},
    year={2015},  
    volume={},  
    number={},  
    pages={76-85},  
    abstract={We introduce a theoretical characterisation of feature lifecycles in app stores, to help app developers to identify trends and to find undiscovered requirements. To illustrate and motivate app feature lifecycle analysis, we use our theory to empirically analyse the migratory and non-migratory behaviours of 4,053 non-free features from two App Stores (Samsung and BlackBerry). The results reveal that, in both stores, intransitive features (those that neither migrate nor die out) exhibit significantly different behaviours with regard to important properties, such as their price. Further correlation analysis also highlights differences between trends relating price, rating, and popularity. Our results indicate that feature lifecycle analysis can yield insights that may also help developers to understand feature behaviours and attribute relationships.},  keywords={formal specification;smart phones;application feature lifecycle analysis;empirical analysis;migratory behaviours;nonmigratory behaviours;nonfree features;Samsung App Stores;BlackBerry App Stores;intransitive features;correlation analysis;price factor;rating factor;popularity factor;feature behaviours;attribute relationships;Feature extraction;Data mining;Databases;Software;HTML;Market research;Natural language processing},  
    doi={10.1109/RE.2015.7320410},  
    ISSN={2332-6441},  
    month={Aug},
    relevant_text = {The App Store marketplace can be thought of as a highly user-participatory cyclic development model that partly involves “requirements for the masses; requirements from the masses”. In this adaptive development space, users express their needs and desires by voting apps up and down and contributing product reviews. Users also tacitly express support for a feature by downloading apps that offer it. Developers may observe this behaviour and respond accordingly by adding popular features, where appropriate, to their own products. In this way, the developers triage the perceived desires of their users and make strategic decisions as to which features to adopt [7 - A case study of post-deployment user feedback triage].
    
    Capturing user reactions helps developers to select and prioritise feature inclusions in the next releases [4 - Analysis of user comments: An approach for software requirements evolution].}
}

@inproceedings{10.1145/2632168.2632169,
    author = {Sasnauskas, Raimondas and Regehr, John},
    title = {Intent Fuzzer: Crafting Intents of Death},
    year = {2014},
    isbn = {9781450329347},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/2632168.2632169},
    doi = {10.1145/2632168.2632169},
    abstract = { We present a fuzzing framework for Intents: the core IPC mechanism for intra- and inter-app communication in Android. Since intents lie at a trust boundary between apps, their correctness is important and thorough testing is warranted. The key challenge is to balance the tension between generating intents that applications expect, permitting deep penetration into application logic, and generating intents that trigger interesting bugs that have not been previously uncovered. Our work strikes this balance using a novel combination of static analysis and random test-case generation. Our intent fuzzer crashed dozens of Google core and top Google Play apps, resulting in app restarts or even in a complete OS reboot. },
    booktitle = {Proceedings of the 2014 Joint International Workshop on Dynamic Analysis (WODA) and Software and System Performance Testing, Debugging, and Analytics (PERTEA)},
    pages = {1–5},
    numpages = {5},
    keywords = {fuzz testing, static analysis, Android IPC, random testing},
    address = {San Jose, CA, USA},
    series = {WODA+PERTEA 2014}
}


@inproceedings{scaffidi2007developing,
  title={Developing confidence in software through credentials and low-ceremony evidence},
  author={Scaffidi, Christopher and Shaw, Mary},
  booktitle={International Workshop on Living with Uncertainties},
  year={2007},
  address = {Atlanta, GA, USA},
  numpages = {3},
  publisher = {IEEE/ACM},
  source_of_paper = {http://se.cs.toronto.edu/IWLU/papers/Confidence_Scaffidi.pdf},
  month = "1",
  url = "https://kilthub.cmu.edu/articles/journal_contribution/Developing_Confidence_in_Software_through_Credentials_and_Low-Ceremony_Evidence/6621953",
  doi = "10.1184/R1/6621953.v1"
}

@inproceedings{schranz2019contributors_catrobat,
  title={Contributors’ impact on a FOSS project’s quality},
  author={Schranz, Thomas and Schindler, Christian and M{\"u}ller, Matthias and Slany, Wolfgang},
  booktitle={Proceedings of the 2nd ACM SIGSOFT International Workshop on Software Qualities and Their Dependencies},
  pages={35--38},
  year={2019},
  address={Tallinn, Estonia},
  publisher={{ACM}},
}

@inproceedings{seneviratne2015_a_measurement_study_of_tracking_in_paid_mobile_apps,
    author = {Seneviratne, Suranga and Kolamunna, Harini and Seneviratne, Aruna},
    title = {A Measurement Study of Tracking in Paid Mobile Applications},
    year = {2015},
    isbn = {9781450336239},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/2766498.2766523},
    doi = {10.1145/2766498.2766523},
    abstract = {Smartphone usage is tightly coupled with the use of apps that can be either free or paid. Numerous studies have investigated the tracking libraries associated with free apps. Only a limited number of these have focused on paid apps. As expected, these investigations indicate that tracking is happening to a lesser extent in paid apps, yet there is no conclusive evidence. This paper provides the first large-scale study of paid apps. We analyse top paid apps obtained from four different countries: Australia, Brazil, Germany, and US, and quantify the level of tracking taking place in paid apps in comparison to free apps. Our analysis shows that 60\% of the paid apps are connected to trackers that collect personal information compared to 85\%--95\% in free apps. We further show that approximately 20\% of the paid apps are connected to more than three trackers. With tracking being pervasive in both free and paid apps, we then quantify the aggregated privacy leakages associated with individual users. Using the data of user installed apps of over 300 smartphone users, we show that 50\% of the users are exposed to more than 25 trackers which can result in significant leakages of privacy.},
    booktitle = {Proceedings of the 8th ACM Conference on Security \& Privacy in Wireless and Mobile Networks},
    articleno = {7},
    numpages = {6},
    address = {New York, New York},
    series = {WiSec '15}
}


@inproceedings{shen2017_towards_release_strategy_optimization_for_apps_in_google_play,
    author = {Shen, Sheng and Lu, Xuan and Hu, Ziniu and Liu, Xuanzhe},
    title = {Towards Release Strategy Optimization for Apps in Google Play},
    year = {2017},
    isbn = {9781450353137},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/3131704.3131710},
    doi = {10.1145/3131704.3131710},
    abstract = {In the appstore-centric ecosystem, app developers have an urgent requirement to optimize their release strategy to maximize user adoption of their apps. To address this problem, we introduce an approach to assisting developers to select the proper release opportunity based on the purpose of the update and current condition of the app. Before that, we propose the update interval to characterize release patterns of apps, and find significance of the updates through empirical analysis. We mined the release-history data of 17,820 apps from 33 categories in Google Play, over a period of 105 days. With 41,028 releases identified from these apps, we reveal important characteristics of update intervals and how these factors can influence update effects. We suggest developers to synthetically consider app ranking, rating trend, and update purpose in addition to the timing of releasing an app version. We propose a Multinomial Naive Bayes model to help decide an optimal release opportunity to gain better user adoption.},
    booktitle = {Proceedings of the 9th Asia-Pacific Symposium on Internetware},
    articleno = {1},
    numpages = {10},
    keywords = {update interval, release strategy, app store, Mobile apps},
    address = {Shanghai, China},
    series = {Internetware'17}
}

@inproceedings{shklovski2014_leakiness_and_creepiness_in_app_space_perceptions_of_privacy_and_mobile_app_use,
    author = {Shklovski, Irina and Mainwaring, Scott D. and Sk\'{u}lad\'{o}ttir, Halla Hrund and Borgthorsson, H\"{o}skuldur},
    title = {Leakiness and Creepiness in App Space: Perceptions of Privacy and Mobile App Use},
    year = {2014},
    isbn = {9781450324731},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2556288.2557421},
    doi = {10.1145/2556288.2557421},
    abstract = {Mobile devices are playing an increasingly intimate role in everyday life. However,
    users can be surprised when informed of the data collection and distribution activities
    of apps they install. We report on two studies of smartphone users in western European
    countries, in which users were confronted with app behaviors and their reactions assessed.
    Users felt their personal space had been violated in "creepy" ways. Using Altman's
    notions of personal space and territoriality, and Nissenbaum's theory of contextual
    integrity, we account for these emotional reactions and suggest that they point to
    important underlying issues, even when users continue using apps they find creepy.},
    booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
    pages = {2347–2356},
    numpages = {10},
    keywords = {bodily integrity, mobile devices, creepiness, data privacy, learned helplessness},
    location = {Toronto, Ontario, Canada},
    series = {CHI '14},
    remarks = {
      The differences between users saying they distrust the app store, and their behaviours which imply they DO trust the apps not to misbehave by continuing to use apps even if they feel the data collection is 'creepy'.
    }
}

@inproceedings{Silva:2019:RCS:3339076.3339130,
     author = {Silva, Rodrigo F. G. and Roy, Chanchal K. and Rahman, Mohammad Masudur and Schneider, Kevin A. and Paixao, Klerisson and de Almeida Maia, Marcelo},
     title = {Recommending Comprehensive Solutions for Programming Tasks by Mining Crowd Knowledge},
     booktitle = {Proceedings of the 27th International Conference on Program Comprehension},
     series = {ICPC '19},
     year = {2019},
     address = {Montreal, Quebec, Canada},
     pages = {358--368},
     numpages = {11},
     url = {https://doi.org/10.1109/ICPC.2019.00054},
     doi = {10.1109/ICPC.2019.00054},
     acmid = {3339130},
     publisher = {IEEE Press},
     address = {Piscataway, NJ, USA},
     keywords = {mining crowd knowledge, stack overflow, word embedding},
} 

@inproceedings{spadini2018_pydriller,
  author = {Spadini, Davide and Aniche, Maur\'{\i}cio and Bacchelli, Alberto},
  title = {PyDriller: Python Framework for Mining Software Repositories},
  year = {2018},
  isbn = {9781450355735},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3236024.3264598},
  doi = {10.1145/3236024.3264598},
  abstract = {Software repositories contain historical and valuable information about the overall development of software systems. Mining software repositories (MSR) is nowadays considered one of the most interesting growing fields within software engineering. MSR focuses on extracting and analyzing data available in software repositories to uncover interesting, useful, and actionable information about the system. Even though MSR plays an important role in software engineering research, few tools have been created and made public to support developers in extracting information from Git repository. In this paper, we present PyDriller, a Python Framework that eases the process of mining Git. We compare our tool against the state-of-the-art Python Framework GitPython, demonstrating that PyDriller can achieve the same results with, on average, 50\% less LOC and significantly lower complexity.  URL: https://github.com/ishepard/pydriller  Materials: https://doi.org/10.5281/zenodo.1327363  Pre-print: https://doi.org/10.5281/zenodo.1327411},
  booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages = {908–911},
  numpages = {4},
  keywords = {Mining Software Repositories, Python, Git, GitPython},
  address = {Lake Buena Vista, FL, USA},
  series = {ESEC/FSE 2018}
}

@inproceedings{steglich2019revisiting,
  title={Revisiting the mobile software ecosystems literature},
  author={Steglich, Caio and Marczak, Sabrina and Guerra, Luiz Pedro and Mosmann, Luiz Henrique and Perin, Marcelo and Figueira Filho, Fernando and de Souza, Cleidson},
  booktitle={2019 IEEE/ACM 7th International Workshop on Software Engineering for Systems-of-Systems (SESoS) and 13th Workshop on Distributed Software Development, Software Ecosystems and Systems-of-Systems (WDES)},
  pages={50--57},
  year={2019},
  organization={IEEE},
  publisher={IEEE},
  address={Montreal, QC, Canada},
  doi={10.1109/SESoS/WDES.2019.00015},
  abstract={Software Ecosystems are comprised of a technology platform, business models, internal and external developers, and engaging users. The popularity of smartphones brought along the mobile software ecosystems, such as iOS and Android, which are composed of a platform, a community of users and developers, mobile applications, and online application store, and evangelists that often promote the ecosystem. Given the recent nature of the topic, this paper aims to revisit the state-of-the-art through a systematic literature mapping. We found 63 publications on the topic of mobile software ecosystems that were categorized by year (almost 50\% of the publications are from 2015 and on), by author (a few collaboration clusters were identified), and by the mobile ecosystems characteristics (most publications discuss business or technical aspects) and elements (applications and the platform are the most discussed topics followed by the developers and the users). Our results provide an up-to-date map of the topic for those interested in mobile software ecosystems.}
}

@inproceedings{strahm2018_mobile_app_onboarding,
    author = {Strahm, Brendan and Gray, Colin M. and Vorvoreanu, Mihaela},
    title = {Generating Mobile Application Onboarding Insights Through Minimalist Instruction},
    year = {2018},
    isbn = {9781450351980},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/3196709.3196727},
    doi = {10.1145/3196709.3196727},
    abstract = {Mobile application designers use onboarding task flows to help first time users learn and engage with key application functionality. Although some guidelines for designing onboarding flows have been offered by practitioners, a systematic, research-informed approach is needed. In this paper, we present the creation of a method for designing mobile application onboarding experiences. We used the minimalist instruction framework to engage twelve university students in an iterative set of design and evaluation activities. Participants interacted with a physical prototype of an educational badging mobile application through a semi-structured exploration and reflection activity, bookended by structured mini-interviews. We found that this method facilitated engagement with participants' meaning-making processes, resulting in useful design insights and the creation of an onboarding task flow. Research opportunities for integrating instructional design and learning approaches in HCI in the context of onboarding are considered.},
    booktitle = {Proceedings of the 2018 Designing Interactive Systems Conference},
    pages = {361–372},
    numpages = {12},
    keywords = {mobile., onboarding, user experience, minimalist instruction, design methods},
    address = {Hong Kong, China},
    series = {DIS '18}
}

@inproceedings{syer2013_empirical_findings_for_mobile_apps,
    author = {Syer, Mark D. and Nagappan, Meiyappan and Hassan, Ahmed E. and Adams, Bram},
    title = {Revisiting Prior Empirical Findings for Mobile Apps: An Empirical Case Study on the 15 Most Popular Open-Source Android Apps},
    year = {2013},
    publisher = {IBM Corp.},
    abstract = {Our increasing reliance on mobile devices has led to the explosive development of millions of mobile apps across multiple platforms that are used by millions of people around the world every day. However, most software engineering research is performed on large desktop or server-side software applications (e.g., Eclipse and Apache). Unlike the software applications that we typically study, mobile apps are 1) designed to run on devices with limited, but diverse, resources (e.g., limited screen space and touch interfaces with diverse gestures) and 2) distributed through centralized "app stores," where there is a low barrier to entry and heavy competition. Hence, mobile apps may differ from traditionally studied desktop or server side applications, the extent that existing software development "best practices" may not apply to mobile apps. Therefore, we perform an exploratory study, comparing mobile apps to commonly studied large applications and smaller applications along two dimensions: the size of the code base and the time to fix defects. Finally, we discuss the impact of our findings by identifying a set of unique software engineering challenges posed by mobile apps.},
    booktitle = {Proceedings of the 2013 Conference of the Center for Advanced Studies on Collaborative Research},
    pages = {283–297},
    numpages = {15},
    address = {Ontario, Canada},
    series = {CASCON '13},
}

@INPROCEEDINGS{upadhyay2017_articulating_the_construction_of_a_web_scraper_for_massive_data_extraction,  
  author={Upadhyay, Shreya and Pant, Vishal and Bhasin, Shivansh and Pattanshetti, Mahantesh K.},  
  booktitle={2017 Second International Conference on Electrical, Computer and Communication Technologies (ICECCT)}, 
  title={Articulating the construction of a web scraper for massive data extraction},  
  year={2017},  
  volume={}, 
  number={}, 
  publisher = {IEEE},
  address = {Coimbatore, India},
  pages={1-4},
  abstract={
    Massive volumes of data are generated by various users, entities, applications and disseminated online. This copious volume of big data is distributed across millions of websites and is available for various applications. Search engines do provide a simple mechanism to access this data. Accessing this data using search engines requires a user to spend time and resources to manually click and download. Clearly, such a manual approach is not scalable for a vast majority of real life applications at the enterprise and organization level. There exist a number of automated approaches to data extraction from the web. Most of these approaches are ad-hoc and domain specific. Therefore, the need for a robust, automated, easy to use framework for extracting content from the web with a minimal human effort across domains appears enticing. The architecture proposed by the authors for a web scraper addresses this gap to harvest data from the web. The proposed web scraping framework offers an easy and feasible approach for parsing and extracting data on a large scale from multiple websites with minimal human intervention. This paper provides an insight into issues relevant to constructing a web scraper and concludes by describing the implementation of a web scraper for harvesting learning objects for an eLearning application.},
  keywords={},
  doi={10.1109/ICECCT.2017.8117827}, 
  ISSN={},  
  month={Feb},
}

@INPROCEEDINGS{ujwal2017_classification_based_adaptive_web_scraper,  
  author={Ujwal, B.V.S. and Gaind, Bharat and Kundu, Abhishek and Holla, Anusha and Rungta, Mukund},
  booktitle={2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA)},
  title={Classification-Based Adaptive Web Scraper},
  year={2017}, 
  volume={},
  number={}, 
  publisher = {IEEE},
  address = {Cancun, Mexico},
  pages={125-132},
  doi={10.1109/ICMLA.2017.0-168},
}

@inproceedings{viennot2014_a_measurement_study_of_google_play,
  author = {Viennot, Nicolas and Garcia, Edward and Nieh, Jason},
  title = {A Measurement Study of Google Play},
  year = {2014},
  isbn = {9781450327893},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/2591971.2592003},
  doi = {10.1145/2591971.2592003},
  abstract = {
    Although millions of users download and use third-party Android applications from the Google Play store, little information is known on an aggregated level about these applications. We have built PlayDrone, the first scalable Google Play store crawler, and used it to index and analyze over 1,100,000 applications in the Google Play store on a daily basis, the largest such index of Android applications. PlayDrone leverages various hacking techniques to circumvent Google's roadblocks for indexing Google Play store content, and makes proprietary application sources available, including source code for over 880,000 free applications. We demonstrate the usefulness of PlayDrone in decompiling and analyzing application content by exploring four previously unaddressed issues: the characterization of Google Play application content at large scale and its evolution over time, library usage in applications and its impact on application portability, duplicative application content in Google Play, and the ineffectiveness of OAuth and related service authentication mechanisms resulting in malicious users being able to easily gain unauthorized access to user data and resources on Amazon Web Services and Facebook.},
  booktitle = {The 2014 ACM International Conference on Measurement and Modeling of Computer Systems},
  pages = {221–233},
  numpages = {13},
  keywords = {google play, authentication, oauth, android, mobile computing, decompilation, security, clone detection},
  address = {Austin, Texas, USA},
  series = {SIGMETRICS '14},
  status = {MUST-DO include in related works.},
}

@inproceedings{wang2017_exploratory_study_of_the_mobile_app_ecosystem,
    author = {Wang, Haoyu and Liu, Zhe and Guo, Yao and Chen, Xiangqun and Zhang, Miao and Xu, Guoai and Hong, Jason},
    title = {An Explorative Study of the Mobile App Ecosystem from App Developers’ Perspective},
    abstract = {With the prevalence of smartphones, app markets such as Apple App Store and Google Play has become the center stage in the mobile app ecosystem, with millions of apps developed by tens of thousands of app developers in each major market. This paper presents a study of the mobile app ecosystem from the perspective of app developers. Based on over one million Android apps and 320,000 developers from Google Play, we analyzed the Android app ecosystem from different aspects. Our analysis shows that while over half of the developers have released only one app in the market, many of them have released hundreds of apps. We classified developers into different groups based on the number of apps they have released, and compared their characteristics. Specially, we have analyzed the group of aggressive developers who have released more than 50 apps, trying to understand how and why they create so many apps. We also investigated the privacy behaviors of app developers, showing that some developers have a habit of producing apps with low privacy ratings. Our study shows that understanding the behavior of mobile developers can be helpful to not only other app developers, but also to app markets and mobile users.},
    year = {2017},
    isbn = {9781450349130},
    publisher = {International World Wide Web Conferences Steering Committee},
    address_of_publisher = {Republic and Canton of Geneva, CHE},
    url = {https://doi.org/10.1145/3038912.3052712},
    doi = {10.1145/3038912.3052712},
    booktitle = {Proceedings of the 26th International Conference on World Wide Web},
    pages = {163–172},
    numpages = {10},
    keywords = {app clone, app developers, app ecosystem, google play, mobile apps, mobile privacy, android},
    address = {Perth, Australia},
    series = {WWW ’17}
}
  
@inproceedings{wang2018_beyond_google_play,
  title={Beyond google play: A large-scale comparative study of Chinese android app markets},
  author={Wang, Haoyu and Liu, Zhe and Liang, Jingyue and Vallina-Rodriguez, Narseo and Guo, Yao and Li, Li and Tapiador, Juan and Cao, Jingcun and Xu, Guoai},
  booktitle={Proceedings of the Internet Measurement Conference 2018},
  pages={293--307},
  year={2018},
  publisher = {ACM},
  address = {Boston MA, USA},
  relevance_to_phd={In China there are many app stores, both vendor-specific and from large tech companies. These are used by 100M's of users. This paper researches these app stores in depth: 6M apps from 16 Chinese app markets + Google Play. Do any of these app stores provide an Android Vitals like service? Two have a "Quality Rating". Will some of those apps' usage be sent to Google Play?
  Their figure 2 helps provide backing evidence that the apps we assessed, particularly with Kiwix, cover the vast majority of the distribution.},
  quote_1 = {"538,283 developers in Google Play in 2018"},
  quote_2 = {Discusses "user download" counts in Google Play and other app stores. Figure 2 provides a distribution of downloads across markets, and correlates to my assessment of where&when Android Vitals provides reports.},
  quote_3 = {We further analyzed the release or update time of these apps across markets. This is also a metric used for estimating whether developers actively maintain their apps, a strong signal for code quality},
}

@inproceedings{wang2019understanding,
  title={Understanding the evolution of mobile app ecosystems: A longitudinal measurement study of {Google Play}},
  author={Wang, Haoyu and Li, Hao and Guo, Yao},
  booktitle={The World Wide Web Conference},
  pages={1988--1999},
  year={2019},
  publisher = {ACM},
  address = {San Francisco CA, USA},
  doi={https://doi.org/10.1145/3308558.3313611},
  abstract = {The continuing expansion of mobile app ecosystems has attracted lots of efforts from the research community. However, although a large number of research studies have focused on analyzing the corpus of mobile apps and app markets, little is known at a comprehensive level on the evolution of mobile app ecosystems. Because the mobile app ecosystem is continuously evolving over time, understanding the dynamics of app ecosystems could provide unique insights that cannot be achieved through studying a single static snapshot. In this paper, we seek to shed light on the dynamics of mobile app ecosystems. Based on 5.3 million app records (with both app metadata and apks) collected from three snapshots of Google Play over more than three years, we conduct the first study on the evolution of app ecosystems from different aspects. Our results suggest that although the overall ecosystem shows promising progress in regard of app popularity, user ratings, permission usage and privacy policy declaration, there still exists a considerable number of unsolved issues including malicious apps, update issues, third-party tracking threats, improper app promotion behaviors, and spamming/malicious developers. Our study shows that understanding the evolution of mobile app ecosystems can help developers make better decision on developing and releasing apps, provide insights for app markets to identifying misbehaviors, and help mobile users to choose desired apps.},
}

 @inproceedings{Yang_Prasad_Xie_2013_grey_box_automated_gui_model_generation_for_mobile_apps, 
   address={Rome, Italy},
   title={A Grey-Box Approach for Automated GUI-Model Generation of Mobile Applications},
   ISBN={978-3-642-37057-1},
   abstract = {
     As the mobile platform continues to pervade all aspects of human activity, and mobile applications, or mobile apps for short, on this platform tend to be faulty just like other types of software, there is a growing need for automated testing techniques for mobile apps. Model-based testing is a popular and important testing approach that operates on a model of an app’s behavior. However, such a model is often not available or of insufficient quality. To address this issue, we present a novel grey-box approach for automatically extracting a model of a given mobile app. In our approach, static analysis extracts the set of events supported by the Graphical User Interface (GUI) of the app. Then dynamic crawling reverse-engineers a model of the app, by systematically exercising these events on the running app. We also present a tool implementing this approach for the Android platform. Our empirical evaluation of this tool on several Android apps demonstrates that it can efficiently extract compact yet reasonably comprehensive models of high quality for such apps.},
  booktitle={Fundamental Approaches to Software Engineering},
  publisher={Springer Berlin Heidelberg},
  author={Yang, Wei and Prasad, Mukul R. and Xie, Tao},
  editor={Cortellessa, Vittorio and Varró, Dániel},
  year={2013},
  pages={250–265}
}


@inproceedings{yang2013testing,
  title={Testing for poor responsiveness in Android applications},
  author={Yang, Shengqian and Yan, Dacong and Rountev, Atanas},
  abstract={An important category of defects in Android applications are related to poor responsiveness. When the user interface thread performs expensive operations, the application is sluggish and may fail with an “Application Not Responding” error. Poor responsiveness has serious negative consequences for user perception and marketplace success. We propose a systematic technique to uncover and quantify common causes of poor responsiveness in Android software. When test cases are executed against the application GUI, artificial long delays are inserted at typical problematic operations (e.g., at calls that access the network). This test amplification approach may exhibit increased response times for GUI events, which demonstrates the effects of expensive operations on poor responsiveness observed by the user. The proposed approach successfully uncovered 61 responsiveness problems in eight open-source Android applications, due to inappropriate usage of resources such as network, flash storage, on-device database, and bitmaps.},
  booktitle={2013 1st International Workshop on the Engineering of Mobile-Enabled Systems (MOBS)},
  pages={1--6},
  year={2013},
  organization={IEEE}
}

@inproceedings{yasumatsu2019_software_library_update_practices_mobile_app_devs,
    author = {Yasumatsu, Tatsuhiko and Watanabe, Takuya and Kanei, Fumihiro and Shioji, Eitaro and Akiyama, Mitsuaki and Mori, Tatsuya},
    title = {Understanding the Responsiveness of Mobile App Developers to Software Library Updates},
    year = {2019},
    isbn = {9781450360999},
    publisher = {Association for Computing Machinery},
    url = {https://doi.org/10.1145/3292006.3300020},
    doi = {10.1145/3292006.3300020},
    booktitle = {Proceedings of the Ninth ACM Conference on Data and Application Security and Privacy},
    pages = {13–24},
    numpages = {12},
    keywords = {mobile app developers, android security, software library, mobile apps measurement},
    address = {Richardson, Texas, USA},
    series = {CODASPY ’19}
}

@inproceedings{_duplicated_in_logging_paper_zhou2020_mobilogleak,
  author={R. {Zhou} and M. {Hamdaqa} and H. {Cai} and A. {Hamou-Lhadj}},
  booktitle={2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER)},
  title={MobiLogLeak: A Preliminary Study on Data Leakage Caused by Poor Logging Practices},   
  year={2020},
  volume={},
  number={},
  pages={577-581},
  abstract={Logging is an essential software practice that is used by developers to debug, diagnose and audit software systems. Despite the advantages of logging, poor logging practices can potentially leak sensitive data. The problem of data leakage is more severe in applications that run on mobile devices, since these devices carry sensitive identification information ranging from physical device identifiers (e.g., IMEI MAC address) to communications network identifiers (e.g., SIM, IP, Bluetooth ID), and application-specific identifiers related to the location and the users' accounts. This preliminary study explores the impact of logging practices on data leakage of such sensitive information. Particularly, we want to investigate whether log-related statements inserted into an application code could lead to data leakage. While studying logging practices in mobile applications is an active research area, to our knowledge, this is the first study that explores the interplay between logging and security in the context of mobile applications for Android. We propose an approach called MobiLogLeak, an approach that identifies log statements in deployed apps that leak sensitive data. MobiLogLeak relies on taint flow analysis. Among 5,000 Android apps that we studied, we found that 200 apps leak sensitive data through logging.},
  keywords={mobile computing;program diagnostics;security of data;MobiLogLeak;data leakage;software practice;software systems;sensitive data;mobile devices;sensitive identification information;physical device identifiers;communications network identifiers;application-specific identifiers;log-related statements;application code;mobile applications;log statements;logging practices;security;taint flow analysis;Android apps;Taint Flow Analysis;Mobile Applications;Data Leakage;Logging Practices},  
  doi={10.1109/SANER48275.2020.9054831},
  ISSN={1534-5351},
  month={Feb},
  publisher = {IEEE},
  address = {London, ON, Canada},
}

@InProceedings{zhou2017_user_perceived_control_trust_etc_smartphone,
    author="Zhou, Yun
    and Raake, Alexander
    and Xu, Tao
    and Zhang, Xuyun",
    editor="Wen, Sheng
    and Wu, Wei
    and Castiglione, Aniello",
    title="Users' Perceived Control, Trust and Expectation on Privacy Settings of Smartphone",
    booktitle="Cyberspace Safety and Security",
    year="2017",
    publisher="Springer International Publishing",
    address="Cham",
    pages="427--441",
    abstract="A common issue is that a large number of authorized apps use important and sensitive personal information without arousing users' full awareness. Existing schemes for privacy protection on smartphones try to provide users with privacy settings to control privacy leakage. Privacy settings on smartphone are intended to inform users about risks of privacy leakage and let users take over control of smartphone. Therefore, it is essential to understand and measure how much users perceive and trust these settings. To this end, we design and conduct a fine-grained online survey with 222 respondents. We collect the demographics as well as users' smartphone usage, covering not only participants' basic background information like age, gender, job, but also time of smartphone use per day, respective importance and sensitivity level of personal data, and their smartphone OSs. In this paper, we investigate users' current privacy perception and protection on smartphone in different groups, discussing participants' responses to (1) Rating the importance and sensitivity of personal information; (2) Trust on existing privacy protection; (3) Perceived control on smartphone; (4) Frequency of searching privacy knowledge; (5) Concerns about manufacturer and third-party company's behaviors on personal data and decision.",
    isbn="978-3-319-69471-9"
}

@inproceedings{zhu2015_learning_to_log,
	title = {Learning to Log: Helping Developers Make Informed Logging Decisions},
	isbn = {978-1-4799-1934-5},
	url = {http://ieeexplore.ieee.org/document/7194593/},
	doi = {10.1109/ICSE.2015.60},
	shorttitle = {Learning to Log},
	booktitle = {2015 IEEE/ACM 37th IEEE International Conference on Software Engineering},
	address = {Florence, Italy},
	abstract = {Logging is a common programming practice of practical importance to collect system runtime information for postmortem analysis. Strategic logging placement is desired to cover necessary runtime information without incurring unintended consequences (e.g., Performance overhead, trivial logs). However, in current practice, there is a lack of rigorous specifications for developers to govern their logging behaviours. Logging has become an important yet tough decision which mostly depends on the domain knowledge of developers. To reduce the effort on making logging decisions, in this paper, we propose a "learning to log" framework, which aims to provide informative guidance on logging during development. As a proof of concept, we provide the design and implementation of a logging suggestion tool, Log Advisor, which automatically learns the common logging practices on where to log from existing logging instances and further leverages them for actionable suggestions to developers. Specifically, we identify the important factors for determining where to log and extract them as structural features, textual features, and syntactic features. Then, by applying machine learning techniques (e.g., Feature selection and classifier learning) and noise handling techniques, we achieve high accuracy of logging suggestions. We evaluate Log Advisor on two industrial software systems from Microsoft and two open-source software systems from Git Hub (totally 19.1M {LOC} and 100.6K logging statements). The encouraging experimental results, as well as a user study, demonstrate the feasibility and effectiveness of our logging suggestion tool. We believe our work can serve as an important first step towards the goal of "learning to log".},
	pages = {415--425},
	publisher = {{IEEE}},
	author = {Zhu, Jieming and He, Pinjia and Fu, Qiang and Zhang, Hongyu and Lyu, Michael R. and Zhang, Dongmei},
	urldate = {2017-02-07},
	date = {2015-05},
	year = {2015},
	keywords = {{ICST}2018, mobicom2017}
}

@misc{amland2002_slides,
  title = {ET Workshop v. 1.20 - Test Management},
  year = {2002},
  author = {Ståle Amland},
  organization = {Amland Consulting},
  url = {http://www.testingeducation.org/course_notes/amland_stale/cm_200212_exploratorytesting/exploratorytesting_5_management.pdf},
  part_of = {http://www.testingeducation.org/course_notes/amland_stale/cm_200212_exploratorytesting/},
}

@misc{android_android_vitals_guide,
  title = {Use Android vitals to improve your app's performance, stability, and size},
  url = {https://developer.android.com/distribute/best-practices/develop/android-vitals},
  organization = {{Android Developers}},
  author = {{Android Developers}},
  year = {2020},
  note = {Last retrieved 13 July 2021},  
}

@misc{android_app_bundle,
  title = {Android App Bundle - Android Developers},
  url = {https://developer.android.com/platform/technology/app-bundle},
  organization = {{Android Developers}},
  author = {{Android Developers}},
  year = {2021},
  note = {Last retrieved 13 July 2021},  
}

@misc{android_guidelines_core_app_quality,
  url = {https://developer.android.com/docs/quality-guidelines/core-app-quality},
  title = {Core app quality - Android Developers},
    organization = {{Android Developers}},
  author = {{Android Developers}},
  year = {2021},
  note = {Last retrieved 14 July 2021}, 
  abstract = {
    Core app quality - Last updated: May 17, 2021
    This checklist defines a set of core quality criteria and associated tests to help you assess the quality of your app. Some of these criteria might be easy to miss, and the tests help you remember to include them in your test plans.

    The checklist highlights the minimum quality that all apps should meet. Your testing will likely go well beyond what's described here.

    Each item in the quality checklist has a unique ID which you might find helpful to use when you communicate with your team. You can also view the previous version of these guidelines [https://developer.android.com/docs/quality-guidelines/2021/02].
  },
  extract = {
    Stability	PS-S1	CR-all SD-1	The app does not crash or block the UI thread causing ANR (Android Not Responding”) errors. Utilize Google Play’s pre-launch report to identify potential stability issues. After deployment, pay attention to the Android Vitals page in the Google Play developer console.},
}

@misc{android_dropboxmanager,
  title = {DropBoxManager - Android Developers},
  url = {https://developer.android.com/reference/android/os/DropBoxManager},
  year = {2021},
  organization = {{Android Developers}},
  author = {{Android Developers}},
  abstract = {
    Enqueues chunks of data (from various sources -- application crashes, kernel log records, etc.). The queue is size bounded and will drop old data if the enqueued data exceeds the maximum size. You can think of this as a persistent, system-wide, blob-oriented "logcat".

    DropBoxManager entries are not sent anywhere directly, but other system services and debugging tools may scan and upload entries for processing.
  },
  note = {Last retrieved 31 July 2020},  
}

@misc{android_in_app_updates,
  title = {In-app updates | Android Developers},
  url = {https://developer.android.com/guide/playcore/in-app-updates},
  year = {2021},
  organization = {{Android Developers}},
  author = {{Android Developers}},  
}

@misc{android_processes_and_application_lifecycle,
  url = {https://developer.android.com/guide/components/activities/process-lifecycle},
  title = {Processes and Application Lifecycle},
  organization = {{Android Developers}},
  author = {{Android Developers}},
  year = {2020},
  note = {Last retrieved 31 July 2020},
}

@misc{android_store_listing_guide,
  title = {Improve your app’s quality and discoverability},
  url = {https://developer.android.com/distribute/best-practices/launch/store-listing},
  year = {2020},
  organization = {{Android Developers}},
  author = {{Android Developers}},
  note = {Last retrieved 13 July 2021},   
  extracts = {
    The Google Play Store is committed to connecting users with a diverse catalog of high quality apps. Our recommendations are composed of a mix of human curation and algorithmic calculations, of which, the two largest components considered are relevance and quality. The best practices below explain how we evaluate quality for your app, independent of which user might download it.
    
    Review the Android vitals dashboard to see how your app is performing on core vitals metrics including crash rate, ANR rate, excessive wakeups, and stuck partial wake locks in the background. Look at peer benchmarks to see how you measure up to others in your category.
  }
}

@misc{android_vitals_best_practices_key_metrics,
  url = {https://developer.android.com/distribute/best-practices/develop/android-vitals#key-metrics},
  title = {Google Play | Android Developers},
  organization = {{Android Developers}},
  author = {{Android Developers}},
  year = {2020},
  extracts = {
    Key metrics
      Stability | ANR rate: The percentage of users who experienced at least one application not responding (ANR) event during a daily session. ANRs are typically caused by deadlocks or slowness in UI thread and background processes (broadcast receivers).
      Stability | Crash rate: The percentage of users who experienced at least one crash event during a daily session. Crashes are often caused by unhandled exceptions, resource exhaustion, failed assertions, or other unexpected states.
  }
}

@online{appbrain,
  title = {AppBrain - Everything you need for a successful Android app},
  url = {https://www.appbrain.com/},
  year = {2021},
  author = {{AppBrain}},
  organization = {{AppBrain}},
}

@misc{appbrain_appsee,
  title = {{Appsee - Android SDK Statistics | AppBrain}},
  url = {https://www.appbrain.com/stats/libraries/details/appsee/appsee},
  organization = {AppBrain},
  author = {Appbrain},
  year = {2020},
  note = {Last retrieved 31 July 2020},
  abstract = {
    Appsee enables mobile app publishers and developers to track, understand and improve the user experience in their apps. It provides features such as User Recordings, Touch Heatmaps, Realtime App Analytics, and Conversion Funnels.

    Number of apps	Over 790
    Total number of downloads	Over 375 Million
    Tags	Analytics
    Website	https://www.appsee.com/
  }
}

@misc{androidauthority2021_huawei_app_gallery,
  title = {Huawei’s Play Store alternative has gotten better, but it’s the apps that count},
  url = {https://www.androidauthority.com/huawei-app-gallery-review-1101306/},
  author = {Robert Triggs},
  year = {2021},
  organization = {{Android Authority}},
}

@misc{androidauthority2021_the_huawei_ban,
  title = {The Huawei ban explained: A complete timeline and everything you need to know},
  url = {https://www.androidauthority.com/huawei-google-android-ban-988382/},
  author = {C. Scott Brown},
  note = {Most recent update at the time of writing was the \nth{12} May 2021.},
  year = {2021},
  organization = {{Android Authority}},
}

@misc{apkcombo_website_about_us,
  title = {Aboust us - APKCombo.com},
  url = {https://apkcombo.com/about},
  year = {2021},
  organization = {APKCombo},
  author = {APKCombo},
  abstract = {
    APKCombo is the largest APK store with 8 million Android games and apps. APKCombo was founded by Harry Phi and Johnny Nguyen in May 2018. We love Android and Technology!

    Every month, over 3 million people trust us to help them find and download file from our client. You can trust our technology to help you download any APK / OBB files for your needs (without country/regional restrictions). APKCombo pulls APK and OBB file directly from Google Play Store. There are no modded APKs on APKCombo. You can check APKs are safe and virus-free before installation with VirusTotal Analyzer (MD5/SHA1/SHA256, Developer Certificate, Android Permissions, Android Activities, Android Services).

    We believe in simplicity. We're excited to simplify idea for everyone through our technology solutions and community. Our slogan is "Simplicity is the key to brilliance".
  },
  note = {Last visited: \nth{01} July 2021},
}

@misc{apteligent2016_data_report_network_crash_edition,
  url = {https://web.archive.org/web/20171119170931/https://data.apteligent.com/research/network-crashes},
  title = {Data Report: Network Crash Edition},
  year = {2016},
  author = {{Apteligent}},
  organization = {{Apteligent}},
  publisher = {Internet Archive: Wayback Machine},
  abstract = {
  In this report we analyze mobile app crashes during interactions with cloud services. We compare the failure rates on iOS and Android, dive into which App Store categories are the most affected by networking issues, and most importantly analyze why these issues occur at such an alarming rate in the first place!

  Network Crash /ˈnɛtwərk kræʃ /
  A crash in a mobile app caused by a network call. For example, an app communicating with a cloud service may return bad data, result in an error, take too long for the request to complete, or simply fail to respond at all.
  }
}

@misc{bitbar2019_smartbear_acquired_bitbar,
  title = {SmartBear Acquires Bitbar to Enhance Mobile Testing Automation Offering},
  url = {https://bitbar.com/blog/smartbear-acquires-bitbar/},
  author = {Delfin Vassallo},
  year = {2019},
  organization = {{Bitbar Technologies}},
  publisher = {{SmartBear Inc.}},
  note = {Accessed \nth{16} July 2021},
}

@misc{bolt2019_how_to_programmatically_capture_screen_on_android,
  title = {How to programmatically capture screen on Android: a comprehensive guide},
  url = {https://medium.com/bolt-labs/how-to-programmatically-capture-screen-on-android-a-comprehensive-guide-f500c95e455a},
  author = {Yaroslav Shevchuk},
  year = {2019},
  publisher = {{Medium Inc.}},
  abstract = {
    At Bolt, we encourage employees to use the products we’re building, to provide feedback that helps us continuously improve. To make this process as simple and efficient as possible, we’ve added a special button to the version of the Android app used by employees internally. Upon tap, it gathers device logs, captures screen image, prompts for a description and sends everything directly to special channel with support representatives. Building a robust screen capturing mechanisms has its pitfalls, and we want to share results and the knowledge we gained with the community.
  }
}

@misc{books_in_print_donald_knuth,
  title = {Books in Print by Donald E. Knuth},
  url = {https://cs.stanford.edu/~knuth/books.html},
  year = {1999},
  author = {Donald E. Knuth},
  quote = {Click web links for current news about each book of interest. Lists of errors and amendments can be downloaded as plain TeX files or read from DVI files or PostScript files cited on the relevant web pages. You are entitled to a reward of at least 0x$1.00 ($2.56) if you are the first person to report a bona-fide error not on those lists. Each page tells you how to report an error for the book in question.},
}

@misc{countly_which_operating_systems_are_supported,
  title = {Countly Help Center - Which operating systems are supported?},
  url = {https://support.count.ly/hc/en-us/articles/360037754031-Android\#which-operating-systems-are-supported},
  year = {2021},
  organization = {Countly},
  author = {Countly},
  note = {Their Android SDK help page was last modified at: June 02, 2021 13:14}
}

@misc{crashscope_project_homepage,
  title = {CrashScope: A Practical Automated Android Testing Tool},
  url = {https://www.android-dev-tools.com/crashscope-home},
  author = {{SEMERU Research Group}},
  organization = {College of William \& Mary --- SEMERU},
  year = {2018},
  note = {Last visited: \nth{01} July 2021},
}

@misc{fdroidwebsite,
  title = {F-Droid - Free and Open Source Android App Repository},
  url = {https://www.f-droid.org/},
  year = {2021},
  organization = {{F-Droid Limited and Contributors}},
  author = {{F-Droid Limited and Contributors}},
  abstract = {
    F-Droid is an installable catalogue of FOSS (Free and Open Source Software) applications for the Android platform. The client makes it easy to browse, install, and keep track of updates on your device.
  },
}

@misc{fileinfo_obb_format,
  title = {.OBB File Extension},
  url = {https://fileinfo.com/extension/obb},
  year = {2021},
  organization = {Sharpened Productions},
  author = {Unknown},
  note = {Last visited: \nth{01} July 2021},
}

@misc{firebaseblog2016_how_does_firebase_initialize_on_android,
  title = {The Firebase Blog: How does Firebase initialize on Android?},
  url = {https://firebase.googleblog.com/2016/12/how-does-firebase-initialize-on-android.html},
  author = {Doug Stevenson},
  year = {2016},
  organization = {Google},
}

@misc{firebaseblog2017_take_control_of_your_firebase_init_on_android,
  title = {The Firebase Blog: Take Control of Your Firebase Init on Android},
  url = {https://firebase.googleblog.com/2017/03/take-control-of-your-firebase-init-on.html},
  author = {Doug Stevenson},
  year = {2017},
  organization = {Google},
}

@misc{gartner_what_is_mobile_app_analytics_software,
  title = {What is Mobile App Analytics software?},
  url = {https://www.gartner.com/reviews/market/mobile-app-analytics/vendors},
  organization = {{Gartner}},
  author = {{Gartner}},
  year = {[2021?]},
  abstract = {
    Mobile app analytics tools collect and report on in-app data pertaining to the operation of the mobile app and the behavior of users within the app. These areas of app analytics are defined as follows: Operational analytics: Provides visibility into the availability and performance of mobile apps in relation to device, network, server and other technology factors. Operational analytics are essential to capture and fix unexpected app behavior (such as crashes, bugs, errors and latency) that can lead to user frustration and abandonment of the app. Such analytics should be applied at both the app testing phase and after release of the app into production. Behavioral analytics: Shows how app users interact with the app to gain actionable insights, drive app improvements and improve business outcomes. Behavioral data can be analyzed based on correlating clicks, swipes, views and other usage stats based on user profiles, segmentation/cohorts, retention, funnel/event tracking and A/B testing.},
}

@misc{googblogs_I_O_2017_everything_new_in_the_google_play_console,
  title = {{I/O 2017: Everything new in the Google Play Console}},
  url = {https://www.googblogs.com/io-2017-everything-new-in-the-google-play-console/},
  author = {Vineet Buch},
  organization = {Google Inc.},
  year = {2017},
}

@misc{harty_stareast2005_keynote,
  title = {The Imperative of Non-Functional System Testing},
  maintitle = {StarEast 2005 Conference},
  organization = {{Commercetest Ltd.}},
  publisher = {{SQE}},
  year = {2005},
  author = {Julian Harty},
  address = {Orlando, Florida, USA.},
}

@misc{harty_wama_dataset_examples,
  title = {Dataset: Examples of JSON data generated by Vitals-Scraper script},
  url = {https://dl.acm.org/do/10.1145/3345843/full/},
  year = {2019},
  author = {Julian Harty},
}

@misc{harty2021_logging_practices_arxiv,
      title={Logging Practices with Mobile Analytics: An Empirical Study on Firebase}, 
      author={Julian Harty and Haonan Zhang and Lili Wei and Luca Pascarella and Mauricio Aniche and Weiyi Shang},
      year={2021},
      eprint={2104.02513},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      abstract = {
        Software logs are of great value in both industrial and open-source projects. Mobile analytics logging enables developers to collect logs remotely from their apps running on end user devices at the cost of recording and transmitting logs across the Internet to a centralised infrastructure.

        This paper makes a first step in characterising logging practices with a widely adopted mobile analytics logging library, namely Firebase Analytics. We provide an empirical evaluation of the use of Firebase Analytics in 57 open-source Android applications by studying the evolution of code-bases to understand: a) the needs-in-common that push practitioners to adopt logging practices on mobile devices, and b) the differences in the ways developers use local and remote logging.

        Our results indicate mobile analytics logs are less pervasive and less maintained than traditional logging code. Based on our analysis, we believe logging using mobile analytics is more user centered compared to traditional logging, where the latter is mainly used to record information for debugging purposes.
    },
}

@misc{hbr_what_ai_app_stores_mean_for_radiology,
  title = {{What AI ``App Stores" Will Mean for Radiology}},
  author = {Woojin Kim, Karen Holzberger},
  url = {https://hbr.org/2019/06/what-ai-app-stores-will-mean-for-radiology},
  year = {2019},
  publisher = {HBR},
}

@misc{hp2015_email_Are_your_mobile_apps_summer_ready,
  title = {Are your mobile apps summer-ready?},
  year = {2015},
  author = {{The HP AppPulse Mobile Team}},
  organization = {{ Hewlett-Packard Development Company}},
  contents = {
  Summer's here and there's more to it this year than long days and warm nights. Mobile app users have little tolerance for dealing with slow performing, crashing, and battery hogging apps. The new Summer 2015 Release of HP AppPulse Mobile is now available and it’s packed full of exciting new features to help you address these issues and more, including:
  
  - Crash Analytics with User Crash Trail – Get full visibility into user experience prior to crashes and isolate the root cause of issues
  - UI Performance with HTTP Timeline – Drill down and investigate the HTTP traffic on a timeline to know which service is impacting the user experience
  - UI Performance with Threads Timeline – Conduct a deep dive analysis into those slow user actions with a complete breakdown into threads and methods
  - Automatic Error Correlation – Group errors by reported, failed HTTP requests, and error messages
  - HP Mobile Center Sharing – Apps in HP AppPulse Mobile can now be used in HP Mobile Center
  For an overview of all the new capabilities HP AppPulse Mobile has to offer, check out this short video webinar which demos of all the key features
  },
}

@misc{huawei_accessing_analytics_kit,
  title = {Analytics Kit - Accessing Analytics Kit},
  url = {https://developer.huawei.com/consumer/en/doc/development/HMSCore-Guides-V5/android-accessing-0000001050161888-V5},
  author = {HUAWEI Developers},
  year = {2021},
  organization = {{Huawei}},
  set_event_reporting_policies = {
    Four policies can be combined to determine when the Analytics SDK will attempt to send events. Screenshot of details saved in my dropbox.
  }  
}

@misc{huaweidevelopers_appgallery_review_guidelines,
  title = {AppGallery Review Guidelines},
  url = {https://developer.huawei.com/consumer/en/doc/30202},
  author = {{HUAWEI Developers}},
  year = {2021},
  organization = {{Huawei}},
  extract = {
    Please be aware of the following issues that may lead to a prolonged app review process or even rejection. Before submitting your app for review, please confirm that:

    1. Your app information and metadata are complete and accurate.
    2. Your app does not crash or experience bugs while running.
    3. Your contact information is authentic and valid, so that you can be reached when necessary.
    4. You can provide a valid test account and sign-in information, as well as any hardware or resources that may be required for app review. For example, the resources or information required for special configurations or special test environments need to be clarified.
    5. Your app is available for use during the review period.
    6. Your app complies with the relevant rules detailed in the following documents:
  }
}

@misc{huawei_ag_connect_crash,
  title = {AppGallery Connect - AGConnectCrash},
  url = {https://developer.huawei.com/consumer/en/doc/development/AppGallery-connect-References/agconnectcrash-android-0000001055420438},
author = {HUAWEI Developers},
  year = {2021},
  organization = {{Huawei}},
  abstract = {
    public class AGConnectCrash
    Creates an AGConnectCrash instance and a crash to facilitate debugging.}
}

@misc{huawei_analyticskit,
  title = {Analytics Kit - APP Intelligent Analysis Service - HUAWEI Developer},
  url = {https://developer.huawei.com/consumer/en/hms/huawei-analyticskit},
  author = {HUAWEI Developers},
  year = {2021},
  organization = {{Huawei}},
  abstract = {Provides free data analysis for a wide range of devices and platforms, so you can make informed decisions on product optimization and marketing based on your users' behavior.},
}

@misc{huawei_analyticskit_dataexport_codelab,
  title = {Analytics Kit (Data Export)},
  url = {https://developer.huawei.com/consumer/en/codelabsPortal/carddetails/HMSAnalyticsKit-Data-Export},
  author = {HUAWEI Developers},
  year = {2021},
  organization = {{Huawei}},  
}

@misc{huawei_analyticskit_pre_release_check,
  title = {Analytics Kit - Pre-release Check},
  url = {https://developer.huawei.com/consumer/en/doc/development/HMSCore-Guides-V5/android-pre-release-check-0000001050420841-V5#EN-US_TOPIC_0000001055224472__sd34f1bf539604a8bbea6cbfc6f7ba4b4},
  author = {HUAWEI Developers},
  year = {2021},
  organization = {{Huawei}}, 
  notice = {
    Since HUAWEI Analytics Kit 4.0.3.300, the HMS Core Analytics SDK for Android has been significantly improved in terms of stability, security, and reliability. If the SDK you integrated is earlier than 4.0.3.300, please upgrade it to 4.0.3.300 or later before April 30, 2021. From May 1, 2021, Analytics Kit will not receive data reported by the Analytics SDK earlier than 4.0.3.300 for Android. If you have integrated Remote Configuration, App Linking, or Crash, you also need to upgrade their SDKs for Android before April 30, 2021. Otherwise, functions that depend on Analytics Kit will become unavailable.
  }
}

@misc{huawei_appconnect_terms_of_service,
  title = {Statement About AppGallery and Privacy},
  url = {https://consumer.huawei.com/minisite/cloudservice/hiapp/privacy-statement.htm?code=SG&branchid=2&language=en_US#},
  year = {2021},
  data_collection_includes = {
  The following categories of information will be collected and used in order for you to use the service:
    •Account information, such as name, email address, phone number, date of birth, and third-party account login data.
    •Network information, such as IP address.
    •Service usage information, such as operation logs.
    •Browser information, such as cookies.
  The data controller is Aspiegel SE, a subsidiary of Huawei in Ireland. Your data will also be used for analytical and service improvement purposes. For more information about how we process your data and about your rights, including the right to object, please click here.
  }
}

@misc{huawei_appgallery_cloud_testing,
  title = {AppGallery Connect Help Center - Cloud Testing},
  url = {https://developer.huawei.com/consumer/en/doc/distribution/app/agc-help-cloud-test-0000001156844797},
  author = {HUAWEI Developers},
  year = {2021},
  organization = {{Huawei}},   
}


@misc{huawei_appgallery_connect_service_whitepaper,
  title = {AppGallery Connect Service White Paper},
  url = {https://developer.huawei.com/consumer/en/doc/distribution/app/agc-help-service-white-paper-0000001156658451},
  year = {2021},
  organization = {{Huawei}},
  author = {{HUAWEI Developers}},
}

@misc{huawei_appgallery_integration_check,
  title = {AppGallery Connect Help Center - Integration Check},
  url = {https://developer.huawei.com/consumer/en/doc/distribution/app/agc-help-self-check-0000001100158786},
  author = {HUAWEI Developers},
  year = {2021},
  organization = {{Huawei}},   
}

@misc{huawei_android_crashservice_sdk_version_change_history,
  title = {Android Crash SDK Version Change History},
  url = {https://developer.huawei.com/consumer/en/doc/development/AppGallery-connect-Guides/agc-crash-sdkchangenotes-0000001054941952},
  author = {HUAWEI Developers},
  year = {2021},
  organization = {{Huawei}}, 
}

@misc{huawei_crashservice_codelab,
  title = {AppGallery Connect Crash Service Development},
  url = {https://developer.huawei.com/consumer/en/codelabsPortal/carddetails/CrashService},
  alternate_url = {https://developer.huawei.com/consumer/en/codelab/CrashService},
  author = {Huawei Developers},
  year = {2021},
  organization = {{Huawei}},
}

@misc{huawei_agc_success_stories,
  title = {Crash - Success Stories},
  url = {https://developer.huawei.com/consumer/en/doc/development/AppGallery-connect-Guides/agc-crash-stories-0000001058075936},
  author = {Huawei Developers},
  year = {2021},
  organization = {{Huawei}},
}

@online{huawei_introduction_to_appgallery_connect_crash_service,
  title = {Introduction to AppGallery Connect Crash Service},
  url = {https://forums.developer.huawei.com/forumPortal/en/topic/0204405648104650146},
  author = {Ritesh Chanchal},
  organization = {HUAWEI},
  year = {2020},
  extracts = {
  Crash Service notification
  Crash service monitors app in real time and notifies developer if any crash occurs. Developers will be reminded if a crash meets the following condition in last one hour:
    1. The crash rate of a problem is greater than the threshold 1\%
    2. Number of app launches is greater than 500.
    3. No crash notification has been triggered for this problem before. That is, the system does not repeatedly send notifications for the same problem.
    
  Tips and Tricks
    1.   Huawei Crash services work on non-Huawei device.
    2.   AGConnectCrash.getInstance().testIt(mContext) triggers app crash. Make sure to comment or remove it before releasing your app.
    3.   Crash Service takes around 1 to 3 minutes to post the crash logs on App Gallery connect dashboard/console.
    4.   Crash SDK collects App and system data.
      System data :
      AAID, Android ID (obtained when AAID is empty), system type, system version, ROM version, device brand, system language, device model, whether the device is rooted, screen orientation, screen height, screen width, available memory space, available disk space, and network connection status.
      App data: 
      APK name, app version, crashed stack, and thread stack.
    5.   The Crash SDK collects data locally and reports data to the collection server through HTTPS after encrypting the data.
    
    The Crash SDK collects data locally and reports data to the collection server through HTTPS after encrypting the data.
  }
}

@misc{yet_to_cite_anchor_android_crash_dataset_on_github,
  title = {GitHub - Anchor-Locator/anchor: Locator for Android app Crashes},
  url = {https://github.com/Anchor-Locator/anchor},
  year = {2020},
  author = {Various developers},
  contact = {anchor-locator@gmail.com},
}

@misc{huawei_crashservice_github_examples,
  title = {huaweicodelabs/CrashService},
  url = {https://github.com/huaweicodelabs/CrashService},
  author = {Various developers},
  year = {2021},
  organization = {{Huawei}},
}

@misc{huawei2020_appgallery_connect_crash_service_article_on_medium,
  title = {Huawei AppGallery Connect Crash Service},
  url = {https://medium.com/huawei-developers/huawei-appgallery-crash-service-1861bd920143},
  author = {Serkan Mutlu},
  year = {2020},
  organization  = {{Huawei}},
  publisher = {{Medium Inc.}},  
}

@misc{huawei2020_press_release_on_hms_ecosystem,
  title = {Huawei Accelerates the Development of the HMS Ecosystem in Anticipation of the 5G Era},
  url = {https://www.prnewswire.com/news-releases/huawei-accelerates-the-development-of-the-hms-ecosystem-in-anticipation-of-the-5g-era-301010305.html},
  author = {Huawei Consumer Business Group},
  organization = {Cision US Inc.},
  year = {2020},
}
@misc{ibm_mobile_foundation_7_1_app_crash_analytics,
  title = {Foundation 7.1 Accessibility and Application Crash Analytics Support is now available},
  author = {Chevy Hungerford},
  year = {2016},
  url = {https://mobilefirstplatform.ibmcloud.com/blog/2015/12/31/foundation-7.1-accessibility-and-application-crash-analytics-support-is-now-available/},
  abstract = {
    This month IBM MobileFirst™ Platform Foundation released two new features in an iFix for Operational Analytics, accessibility, and application crash analytics.

    Crash Analytics: Previously you have had the ability to capture crashes, now with Operational Analytics you can analyze your captured application crashes, thus allowing you to better monitor and troubleshoot your applications. We now offer new charts for monitoring application crashes and charts for application troubleshooting on the dashboard of the Operational Analytics Console.

    Crash Analytics has an easy client-side implementation which makes capturing crashes a much simpler task. Rogue crashes are common on apps released to the public. Crash Analytics captures and analyzes rogue crashes and informs the Operational Analytics users. Knowing what causes these rogue crashes gives the developer the opportunity to debug and fix without having to spend the time reproducing the problem. With that being said, Crash Analytics helps your developers save time and betters your app user experience.

    Accessibility: Accessibility features enable people with disabilities, such as restricted mobility and limited vision, to work successfully with IBM MobileFirst™ Platform Foundation.

    The IBM MobileFirst Operational Analytics Console now provides accessibility features.
  },
}

@misc{inapptics2017_mobile_heatmap_visualise_user_behaviour,
  title = {Mobile Heatmap: One of the Best Tools to Visualize User Behavior},
  url = {https://uxplanet.org/mobile-heatmap-one-of-the-best-tools-to-visualize-user-behavior-f0c056d4c196},
  year = {2017},
  author = {{inapptics}},
  publisher = {{Medium Inc.}},
  abstract = {While traditional mobile app analytics provides key metrics and information on demographics, a mobile heatmap is an essential tool that visually displays user behavior in an app. Thus the value of heatmaps in tracking users’ app usage patterns is invaluable.}
}

@misc{khan2019_medium_filtering_adb_logcat_efficiently,
  title = {Filtering Android adb logcat efficiently in bash command line},
  url = {https://medium.com/@hissain.khan/filtering-android-adb-logcat-efficiently-in-bash-command-line-4992fb1acd61},
  author = {Sazzad Hissain Khan},
  year = {2019},
  publisher = {{Medium Inc.}},
  organization = {Samsung},
}

@misc{kiwixandroid_issue_1223_bitbar_should_run_tests_on_multiple_devices,
  title = {Bitbar should run tests on multiple devices},
  url = {https://github.com/kiwix/kiwix-android/issues/1223},
  author = {Seán Mac Gillicuddy},
  year = {2019},
  organization = {{Kiwix-Android Project}},  
  abstract = {
  Is your feature request related to a problem? Please describe. Continuation of #110. After adding the devices listed in this ticket the build became nigh unpassable
  Describe the solution you'd like: Get rid of bitbar, use Firebase Test Lab.
  Describe alternatives you've considered: Find through trial and error find a combination on bitbar that works}
}

@misc{kiwixandroid_issue_1228_bibbar_not_working_with_split_apks,
  title = {Bitbar not working with split APKs},
  url = {https://github.com/kiwix/kiwix-android/issues/1228},
  author = {Seán Mac Gillicuddy},
  year = {2019},
  organization = {{Kiwix-Android Project}},
  abstract = {Uploads to bitbar on travis are not functional on develop or any derived branches. There is a possibility that with the advent of #1219 and the generation of a universal apk that bitbar will select it from the group of generated apks.},
}

@misc{knuth_the_bank_of_san_serriffe,
  title = {Knuth: The Bank of San Serriffe},
  url = {https://cs.stanford.edu/~knuth/boss.html},
  year = {2020},
  author = {Donald E. Knuth},
  publisher = {by author},
  organization = {Stanford University},
}

@misc{levy2016_crash_and_churn_report,
  url = {https://web.archive.org/web/20170317232255/https://data.apteligent.com/research/crash-and-churn},
  title = {Data Report: Crash \& Churn Edition},
  year = {2016},
  author = {{Apteligent}},
  organization = {{Apteligent}},
  publisher = {Internet Archive Wayback Machine},
  abstract = {In an industry first, Apteligent has quantified the correlation between mobile app crashes and increased churn rates. In this report, we not only deduce that a correlation exists — we leverage our data to show exactly how crashes drive an increase in churn. For those new to the space, churn can be thought of as the inverse of retention.},
}

@misc{levy2017_the_crash_and_burn_report_findings,
  title = {The "Crash and Burn" Report Findings},
  url = {https://www.apmdigest.com/the-crash-and-burn-report-findings},
  author = {Andrew Levy},
  year = {2017},
  organization = {Apteligent},
  publisher = {APM digest},
}

@misc{li2017_mining_device_Specific_app_usage_patterns,
      title={Mining Device-Specific Apps Usage Patterns from Large-Scale Android Users}, 
      author={Huoran Li and Xuan Lu},
      year={2017},
      eprint={1707.09252},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@misc{lightstephq2021_observability_will_never_replace_monitoring,
  title = {Observability will never replace Monitoring (because it shouldn’t)},
  url = {https://medium.com/lightstephq/observability-will-never-replace-monitoring-because-it-shouldnt-eeea92c4c5c9},
  author = {Ben Sigelman},
  year = {2021},
  publisher = {{Medium Inc.}},
  extracts = {
    ...the anatomy of observability. There are three layers:
      I. (Open)Telemetry: acquire high-quality data with minimal effort
      II. Storage: “Stats over time” and “Transactions over time”
      III. Benefits: *solve actual problems*
  }
}

@misc{lotan2015_apple_apps_and_algorithmic_glitches,
  title = {Apple, Apps and Algorithmic Glitches - A data analysis of iTunes’ top chart algorithm},
  url = {https://medium.com/message/apple-apps-and-algorithmic-glitches-f7bc8dd2cda6},
  author = {Gilad Lotan},
  year = {2015},
  publisher = {{Medium Inc.}},
  extract = {
    On October 29th and December 18th, 2014, something very strange happened to the iTunes top apps chart. Like an earthquake shaking up the region, all app positions in the chart were massively rearranged, some booted off completely. These two extremely volatile days displayed rank changes that are orders of magnitude higher than the norm — lots of apps moving around, lots of uncertainly.
    
    If you build apps for iOS devices, you know that the success of your app is contingent on chart placement. If you use apps on iPhones and iPads, you should realize just how difficult it is for app developers to get you to download their app. Apple deploys an algorithm that identifies the Top Apps across various categories within its iTunes app store. This is effectively a black box. We don’t know exactly how it works, yet many have come to the conclusion that the dominant factor affecting chart placement is the number of downloads within a short period of time.
  }
}

@misc{lotan2015_apples_app_charts,
  title = {Apple’s App Charts: 2015 Data and Trends …or how much harder it is to get into the top charts},
  url = {https://medium.com/i-data/apple-s-app-charts-2015-data-and-trends-abb95300df57},
  author = {Gilad Lotan},
  year = {2015},
  publisher = {{Medium Inc.}},
  extract = {
    I did not observe glitches in this year’s data. The algorithmic system governing the charts appears to have reached some level of stability. On the one hand, a stable system is great — it is predictable, there’s less uncertainty, which makes planning easier. But on the other had, there’s a stronger “rich get richer effect” — once an app makes it to an advantageous position, it’s heightened visibility reinforces it’s continued ranking.
  }
}

@misc{mcclintok_mixpanel_update_on_autotrack_data_collection,
  title = {Update on Autotrack data collection},
  url = {https://mixpanel.com/blog/update-autotrack-data-collection/},
  author = {Jon McClintok},
  year = {2018},
  organization = {Mixpanel},
  topics = {
    They screwed up and collected lots of sensitive data using their autotrack data collection.
    They decided to add the following changes to their practices:
      Incorporating additional privacy reviews as part of our design and development processes,
      In-depth security/privacy audits of key existing product areas,
      Operationalizing our response tooling,
      Data filtering and detection,
  }
}

@misc{moodspace2021_privacy_policy,
  title = {Privacy Policy | Moodspace},
  url = {https://moodspace.org/privacy-policy},
  year = {2021},
  author = {{Moodspace}},
}

@misc{mukherjee_implicit_versus_explicit_event_tracking_hits_and_misses,
  title = {Implicit Versus Explicit Event Tracking: Hits and Misses},
  url = {https://iterative.ly/blog/implicit-vs-explicit-event-tracking-hits-and-misses/},
  author = {Debdut Mukherjee},
  organization = {{Iteratively}},
  year = {2020},
  topics = {
  Hits & misses of implicit or codeless event tracking
  Hits & misses of explicit or code-based event tracking
  So what should you choose?
  },
}
@misc{meyer2018_towards_empirical_answers_to_important_engineering_questions,
  title = {Towards empirical answers to important software engineering questions},
  url = {https://bertrandmeyer.com/2018/01/26/towards-empirical-answers-important-software-engineering-questions/},
  year = {2018},
  author = {Bertrand Meyer},
  publisher = {Bertrand Meyer},
  source = {(Adapted from a two-part article on the Communications of the ACM blog.)},
  extracts = {
    The first has to do with the distinction introduced above between the two kinds of possible targets for empirical assessment: products (artifacts) versus processes.
    Both aspects are important, but one is much easier to investigate than the other. For software products, the material of study is available in the form of repositories mentioned above, with their wealth of information about lines of code, control and data structures, commits, editing changes, bug reports and bug fixes. Processes are harder to grasp. You gain some information on processes from the repositories (for example, patterns and delays of bug fixing), but processes deserve studies of their own. For example, agile teams practice iterations (sprints) of widely different durations, from a few days to a few weeks; what is the ideal length? A good empirical answer would help many practitioners. But this example illustrates how difficult empirical studies of processes can be: you would need to try many variations with teams of professional programmers (not students) in different projects, different application areas, different companies; for the results to be believable the projects should be real ones with business results at stake, there should be enough samples in each category to ensure statistical significance, and the companies should agree to publication of some form, possibly anonymized, of the outcomes. The difficulties are formidable.
    
    Indeed, this is what we are entitled to expect from empirical studies: guidance. The slogan of empirical software engineering is that software is worthy of study just like geological strata, photons, and lilies-of-the-valley; OK, sure, but we are talking about human artifacts rather than wonders of the natural world, and the idea should be to help us produce better software and produce software better.
  }
}

@misc{nikgapps,
  title = {NikGApps - Custom Google Apps Package!},
  url = {https://nikgapps.com/},
  author = {{NikGApps team}},
  year = {2021},
  notes = {Last visited~\nth{9} June 2021},
}

@misc{nist_pii,
  title = {PII Glossary | CSRC},
  url = {https://csrc.nist.gov/glossary/term/PII},
  organization = {{National Institute of Standards and Technology}},
  author = {{NIST}},
  year = {2021},
  notes = {Last visited~\nth{31} May 2021},
}

@misc{norwied2012_download_android_install_files,
  title = {Download Android install files *.apk from play.google.com},
  url = {https://norwied.wordpress.com/2012/08/10/download-android-install-files-apk-from-play-google-com/},
  author = {Norbert Wiedermann},
  year = {2012},
  notes = {Last visited~\nth{27} Jul 2021},
}

@misc{nytimes20191221_total_surveillance_is_not_what_america_signed_up_for,
  title = {Total Surveillance Is Not What America Signed Up For},
  url = {https://www.nytimes.com/interactive/2019/12/21/opinion/location-data-privacy-rights.html},
  author = {{The Editorial Board}},
  year = {2019},
  month = {21 Dec},
  publisher = {{NY Times}},
}

@misc{nytimes20210111_who_should_make_the_online_rules,
  title = {Who Should Make the Online Rules? - The New York Times},
  url = {https://www.nytimes.com/2021/01/11/technology/twitter-facebook-parler-rules.html},
  abstract = {A handful of unelected tech executives have tremendous influence on public discourse. Is that right?},
  year = {2021},
  month = {11 Jan},
  author = {Shira Ovide},
  publisher = {{NY Times}},
}

@misc{nytimes20210721_the_nightmare_of_our_snooping_phones,
  title = {The Nightmare of Our Snooping Phones - The New York Times},
  url = {https://www.nytimes.com/2021/07/21/technology/phones-location-data.html},
  unlocked_url = {https://www.nytimes.com/2021/07/21/technology/phones-location-data.html?unlocked_article_code=AAAAAAAAAAAAAAAACEIPuonUktbfqohkT1UZAibJUNMnqBqCgvfeh6Q8gXnzN22RTj1L0-USBc2M8lvEI6p_Yt95lxKqeOh8Cp59Dvpj0r0YeEV3VwijppbDlJpffCht99n2Djho0teQBetntDa3MTX8JbtyyefhtBnaYDG9S7WfhSN6XHttoZZhc16p2XMN1_2FRrYzgo8iqK9nUpNqRj4AZD2Iue3oC3h9PtaBaBLa6momSr0TGGGTzZPHteV2IEgFAknGTXh8_W829NpaXdsSN6r6JBMgE9HshbL7qcq8MesyI6IGg2pHKg&smid=url-share},
  year = {2021},
  month = {21 July},
  author = {Shira Ovide},
  publisher = {{NY Times}},
  abstract = {A Catholic official’s resignation shows the real-world consequences of practices by America’s data-harvesting industries.},
}

@misc{objectbox2020_moodspace_interview,
  title = {MoodSpace Mobile App Use Case},
  year = {2020},
  author = {Alyssa Coke and Ian Alexander},
  url = {https://objectbox.io/moodspace-mobile-app-use-case/},
  abstract = {
    We speak with Ian Alexander, founder and lead developer at MoodSpace, a beautiful app making mental health exercises accessible to everyone. MoodSpace was released in 2019, and has over 150k+ downloads. The COVID-crises highlights the importance of digital support for wellbeing and saw MoodSpace surge. After trying several databases, Ian settled on ObjectBox because of its high performance and ease of use.
  }
}

@misc{opengapps,
  title = {The Open GApps Project},
  url = {https://opengapps.org/},
  organization = {The Open GApps Team},
  author = {{The Open GApps Team}},
  year = {2021},
  label = {Open GApps},
  notes = {Last visited~\nth{9} June 2021},  
}

@misc{openstf_website,
  title = {STF | Smartphone Test Farm},
  url = {https://openstf.io/},
  year = {2018},
  organization = {{CyberAgent, Inc.}},
  author = {{CyberAgent, Inc.}},
}

@misc{overops2021_what_causes_97pct_of_1billion_java_logged_errors,
  title = {We Crunched 1 Billion Java Logged Errors – Here’s What Causes 97\% of Them},
  url = {https://www.overops.com/blog/we-crunched-1-billion-java-logged-errors-heres-what-causes-97-of-them-2/},
  author = {Nick Andrews},
  year = {2021},
  abstract = {97\% of Logged Errors are Caused by 10 Unique Errors},
  organization = {{OverOps}},
}

@misc{read2018_digital_takeover_avionics,
  title = {Digital Takeover},
  author = {Bill Read},
  url = {https://www.aerosociety.com/news/digital-takeover/},
  year = {2018},
  publisher = {{Royal Aeronautical Society}},
}

@misc{coillet2016-wikimedia-kiwix-ten-years,
  title = {No internet? No problem! Kiwix celebrates ten years of offline Wikipedia reading},
  url = {https://diff.wikimedia.org/2016/10/11/kiwix-ten-years/},
  author = {Stéphane Coillet-Matillon},
  year = {2016},
  organization = {{Wikimedia Foundation}},
}

@misc{gaudin2017_wikimedia_kiwix_android,
  title = {Carry the entirety of Wikipedia in your pocket with Kiwix for Android},
  url = {https://diff.wikimedia.org/2013/04/17/carry-the-entirety-of-wikipedia-in-your-pocket-with-kiwix-for-android/},
  author = {Renaud Gaudin},
  year = {2017},
  organization = {{Wikimedia Foundation}},  
}

@misc{gomez2017_wikimedia_kiwix_article,
  title = {The future of offline access to Wikipedia: The Kiwix example},
  url = {https://diff.wikimedia.org/2017/10/02/offline-access-wikipedia-kiwix/},
  author = {Anne Gomez},
  year = {2017},
  organization = {{Wikimedia Foundation}},
}

@misc{yet_to_cite_alibabcloud2020_what_to_do_if_your_app_crashes_during_coronavirus_outbreak,
  title = {What Would You Do If Your App Crashes during Coronavirus Outbreak?},
  url = {https://www.alibabacloud.com/blog/what-would-you-do-if-your-app-crashes-during-coronavirus-outbreak_596036},
  author = {Ding Jie},
  key = {Alibaba},
  organization = {{Alibaba Cloud}},
  year = {2020},
  comments = {A server-/cloud-centric perspective. They encourage chaos engineering and offer fault injection services. From a production stability perspective they offer sentinel - a lightweight throttling framework and other practices to help the service to degrade gracefully.}
}

@misc{scmp2021_chinas_covid_19_tracking_app_crashes,
  title = {China’s Covid-19 tracking app crashes as traffic surges amid fresh coronavirus outbreak},
  url = {https://www.scmp.com/tech/policy/article/3143487/chinas-covid-19-tracking-app-crashes-traffic-surges-amid-fresh},
  year = {2021},
  author = {Xinmei Shen},
  organization = {{South China Morning Post}},
  abstract = {
    The widely used app’s crash on Monday morning caused chaos for commuters in many places across the country
    The China Academy of Information and Communications Technology said the service resumed nationwide on Monday afternoon
    A widely used travel history tracking app jointly developed by the Chinese government and the country’s three mobile network operators crashed on Monday morning, as the country grapples with its most widespread Covid-19 outbreak in months.
    Users of the “telecommunications big data travel history” app could not load the program on Monday morning, resulting in chaos for commuters in many places across the country.
  }
}

@misc{scrumdictionary_chore,
  title = {"Chore."},
  author = {{ScrumDictionary.com}}, 
  note = {Accessed, \nth{08} July 2021},
  year = {2021},
  url = {https://scrumdictionary.com/term/chore/},
}

@misc{segmentio_analytics_android_issue_770_fake_implementation_for_testing,
  title = {Fake Implementation for Segment Analytics \#770},
  url = {https://github.com/segmentio/analytics-android/issues/770},
  year = {2021},
  month = {Jul},
  author = {Saad Farooq},
  organization = {Segment},
}

@misc{sentry_features_breadcrumbs,
  title = {Reproduce errors without user feedback},
  url = {https://sentry.io/features/breadcrumbs/},
  author = {{Sentry}},
  year = {2021},
  note = {Accessed, \nth{15} Sep 2021},
  quote = {Breadcrumbs show you events that lead to errors.},
}

@misc{sigg2016_sovereignty_of_apps_there_s_more_to_relevance_than_downloads,
      title={Sovereignty of the Apps: There's more to Relevance than Downloads}, 
      author={Stephan Sigg and Eemil Lagerspetz and Ella Peltonen and Petteri Nurmi and Sasu Tarkoma},
      year={2016},
      eprint={1611.10161},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
}

@misc{sutherland2014_wikimedia_on_kelson,
  title = {Emmanuel Engelhart, Inventor of Kiwix: the Offline Wikipedia Browser},
  url = {https://diff.wikimedia.org/2014/09/12/emmanuel-engelhart-inventor-of-kiwix/},
  author = {Joe Sutherland},
  year = {2014},
  organization = {{Wikimedia Foundation}},
}

@misc{yet_to_cite_sunderland2019_the_one_star_android_review,
  title = {The 1-Star Android App Review},
  url = {https://medium.com/swlh/the-1-star-android-app-review-b2892756925f},
  author = {Thomas Sunderland},
  year = {2019},
  publisher = {{Medium Inc.}},
  abstract = {
    Deconstructing, Responding, and Avoiding: 3 Real-World Examples
  },
  relevance = {
    A really useful, practical explanation of how this developer addressed three distinct types of 1 star review. The example where they used the meta-data of the end user's device is very practical and a similar technique applies for the failure analysis when using mobile analytics.
  }
}

@misc{uxcam_hackermoon_2020_heatmapping,
  title = {Mobile App Heatmaps: A Powerful Weapon (And How to Use Them)},
  url = {https://hackernoon.com/mobile-heatmaps-a-powerful-secret-weapon-for-app-companies-311p36v9},
  author = {{UXCam}},
  year = {2020},
  publisher = {HackerMoon},
}

@misc{vodafone2021_huawei_appgallery,
  title = {Vodafone | What is the Huawei AppGallery?},
  url = {https://www.vodafone.co.uk/mobile/brands/huawei/appgallery},
  organization = {{Vodafone UK}},
  author = {Vodafone},
  year = {2021},
  abstract = {
    AppGallery is the official Huawei app platform, like the Google Play Store or the App Store. It’s safe to use, and has a four-layer detection mechanism to ensure app security. You can search for, download, manage, and share apps in the AppGallery. 

    Simply browse and download the apps you want and discover more exclusive offers, innovative experiences and fun, in-app activities.

    It has over one million applications worldwide, and has had over 180 billion cumulative application downloads in the past year. Currently, there are more than 45,000 apps available.
  }
}

@misc{wikipedia_ecological_validity,
  author = "{Wikipedia contributors}",
  title = "Ecological validity --- {Wikipedia}{,} The Free Encyclopedia",
  year = "2020",
  howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Ecological_validity&oldid=975507963}",
  note = "[Online; accessed 29-August-2020]"
  }

@misc{wikipedia__knuth_reward_checks_2020, 
  title={Knuth reward check}, 
  url={https://en.wikipedia.org/wiki/Knuth_reward_check}, 
  journal={Wikipedia}, 
  publisher={{Wikimedia Foundation}},
  author = "{Wikipedia contributors}",
  year={2020}, 
  month={Feb}
}

@misc{wikipedia__digital_twin,
  title = {Digital twin},
  url = {https://en.wikipedia.org/wiki/Digital_twin},
  journal={Wikipedia}, 
  publisher={{Wikimedia Foundation}},
  author = "{Wikipedia contributors}",
  year={2020},
  note={Retrieved 16 July 2020},
}

@misc{wikipedia_maslows_hierarchy_of_needs,
  author = "{Wikipedia contributors}",
  title = "Maslow's hierarchy of needs --- {Wikipedia}{,} The Free Encyclopedia",
  year = "2020",
  url = "https://en.wikipedia.org/w/index.php?title=Maslow\%27s_hierarchy_of_needs&oldid=980201185",
  note = "[Online; accessed 29-September-2020]"
}

  @misc{wikipedia_post_hoc_fallacy,
    author = "{Wikipedia contributors}",
    title = "Post hoc ergo propter hoc --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2021",
    howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Post_hoc_ergo_propter_hoc&oldid=1020538525}",
    note = "[Online; accessed 2-July-2021]"
  }

@misc{wikipedia_streetlight_effect,
  author = "{Wikipedia contributors}",
  title = "Streetlight effect --- {Wikipedia}{,} The Free Encyclopedia",
  year = "2020",
  url = "https://en.wikipedia.org/w/index.php?title=Streetlight_effect&oldid=994601415",
  note = "[Online; accessed 17-May-2021]"
  }

@misc{wikipedia_survivorship_bias,
  author = "{Wikipedia contributors}",
  title = "Survivorship bias --- {Wikipedia}{,} The Free Encyclopedia",
  year = "2020",
  howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Survivorship_bias&oldid=977467926}",
  note = "[Online; accessed 22-September-2020]"
}

@misc{wikipedia_rubicon,
    author = "{Wikipedia contributors}",
    title = "Rubicon --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2020",
    howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Rubicon&oldid=989791012}",
    note = "[Online; accessed 30-November-2020]"
  }

@online{7_basic_quality_tools_with_R,
  title = {7 Basic Quality Tools with {R}},
  url = {https://towardsdatascience.com/7-basic-tools-of-quality-using-r-49fef5481e07},
  year = {2019},
  author = {Roberto Salazar},
  organization = {Medium},
  journal = {Medium}
}

@online{adil2020_sending_logs_from_flutter_apps,
  title = {Sending logs from Flutter apps in real-time using ELK stack \& MQTT},
  url = {https://itnext.io/sending-logs-from-flutter-apps-in-real-time-using-elk-stack-mqtt-c24fa0cb9802},
  author = {Umair Adil},
  organization = {ITNEXT},
  year = {2020},
}

@online{altindag2020_unit_testing_log_messages_made_easy,
  title = {Unit Testing Log Messages Made Easy},
  author = {Hakan Altındağ},
  url = {https://dzone.com/articles/unit-testing-log-messages-made-easy},
  year = {2020},
  abstract = {Unit testing presents specific challenges around logging. A developer and DZone Core members discusses an open source project he created to help.},
  publisher = {{DZone}},
  organization = {{DZone}},
}

@online{amplitude_are_you_data_driven,
  title = {Are You Data-driven, Data-informed or Data-inspired?},
  url = {https://amplitude.com/blog/data-driven-data-informed-data-inspired},
  author = {Shayna Stewart},
  year = {2019},
  organization = {{Y Media Labs}},
}

% Funny how Google included in docs for their Chinese domain https://developer.android.google.cn/topic/performance/vitals
@online{android_vitals_overview_2019,
    title = {Android vitals},
    url = {https://developer.android.com/topic/performance/vitals},
    abstract = {Android vitals is an initiative by Google to improve the stability and performance of Android devices. When an opted-in user runs your app, their Android device logs various metrics, including data about app stability, app startup time, battery usage, render time, and permission denials. The Google Play Console aggregates this data and displays it in the Android vitals dashboard},
    author = {Google Android},
    organization = {{Google Inc.}},
    urldate = {2019-06-17},
    date = {2019},
    year = {2019}
}

@online{androiddevelopersblog2012_android_application_error_reports,
  title = {Android Developers Blog: Android Application Error Reports},
  url = {https://android-developers.googleblog.com/2010/05/google-feedback-for-android.html},
  author = {Jacek Surazski},
  year = {2010},
      organization = {{Google Inc.}},
    urldate = {2010-05-21},
  abstract = {
    The upcoming release of Android [Froyo 2.2]will include a new bug reporting feature for Market apps. Developers will receive crash and freeze reports from their users. The reports will be available when they log into their Android Market publisher account. No more blind debugging!
    
    When an app freezes or stops responding, the user can send a bug report to the developer with a click of a button, right from their phone. The new button appears in the application error dialog; if the user chooses to click it, the Google Feedback client running on the device will analyze the offending app and compose a report with information needed to diagnose it. The system is set up with user privacy in mind — the app developer will not receive information which could identify the user in any way. The user can also preview all information that will be sent.
    
    If users choose to do so, they may also send additional system information like device logs. Because there is a chance these may contain private information, they will not be passed on to the developer; they will be used by Google to track down bugs in the Android system itself.
    
    On the receiving end, developers will get tools to diagnose, triage and fix bugs in their apps. A popular app can generate hundreds of thousands of reports. Google Feedback aggregates them into "bugs" - individual programming errors. Bugs are displayed to developers sorted by severity, measured as the rate at which reports for the bug are flowing in.
    
    Clicking on a bug will display information such as stack traces, statistics about which type of hardware the bug occurred on and what versions of the app the user was running. In case of freezes, stack traces for all threads in the app will be displayed. This data should give developers a good idea how well their apps are faring in the wild.
    
    Google is constantly working on improving and extending the feedback feature to provide developers with tools to improve the quality of their apps. The benefits should be felt by both developers and their users.
  }
}

@online{androiddevelopersblog2019_io2019_whats_new_in_play,
  title = {Android Developers Blog: I/O 2019: New features to help you develop, release, and grow your business on Google Play},
  url = {https://android-developers.googleblog.com/2019/05/whats-new-in-play.html},
  year = {2019},
  author = {Kobi Glick},
  organization = {{Google Inc.}},
}

@online{androiddevelopers2020_permission_denials,
  title = {Permission Denials - Android Developers},
  url = {https://developer.android.com/topic/performance/vitals/permissions},
    author = {{Google Android Documentation}},
    organization = {{Android Developers}},
    urldate = {2020-11-24},
    year = {2020},
}

@misc{appannie2021,
  title = {App Annie - The App Analytics and App Data Industry Standard},
  url = {https://www.appannie.com/en/},
  organization = {{App Annie}},
  author = {{App Annie}},
  year = {2021},
  products = {
    App Annie Intelligence: With data on over 8 million apps and thousands of websites, get the complete picture of the mobile landscape you need to acquire and retain customers, prioritize your roadmap, enter new markets, and optimize ROI.
    App Annie Ascend: Manage, enrich and identify hidden performance opportunities across your own advertising and monetization data. Access intuitive dashboards, advanced normalization tools, and data from 400+ partner connections — all in one place.
    App Annie Connect: Track your own apps' most critical data, including downloads, revenue, usage, and advertising – all in one place.
  }
}

@online{apple2020_how_to_review_your_apps_crash_logs,
  title = {How to review your app’s crash logs},
  url = {https://developer.apple.com/news/?id=nra79npr},
  year = {2020},
  urldate = {2020-06-09},
  author = {{Apple Inc.}},
  organization = {{Apple}},
}

@online{appleappstore2021_app_completeness,
  title = {App Store Review Guidelines - Apple Store: 2. Performance - App Completeness},
  url = {https://developer.apple.com/app-store/review/guidelines/#app-completeness},
  author = {{Apple Inc.}},
  organization = {Apple Inc.},
  year = {2021},
  note = {Visited \nth{12} July 2021},
  quote = {Please don’t treat App Review as a software testing service. We will reject incomplete app bundles and binaries that crash or exhibit obvious technical problems.},
  see_also = {After You Submit -> Bug Fix Submissions: For apps that are already on the App Store, bug fixes will no longer be delayed over guideline violations except for those related to legal or safety issues. If your app has been rejected, and qualifies for this process, please use the Resolution Center to communicate directly with the App Review team indicating that you would like to take advantage of this process and plan to address the issue in your next submission.}
}

@online{appleappstore2021_review_avoiding_common_app_rejections,
  title = {App Review - App Store - Apple Developer: Avoiding common app rejections},
  url = {https://developer.apple.com/app-store/review/},
  author = {{Apple Inc.}},
  organization = {Apple Inc.},  
  year = {2021},
  quote = {On average, over 40\% of app rejections are for Guideline 2.1 – Performance: App Completeness.},
  extract = {
    Avoiding common app rejections
      We’ve highlighted some of the most common issues that cause apps to get rejected to help you better prepare your apps before submitting them for review.
    Crashes and bugs
      You should submit your app for review only when it is complete and ready to be published. Make sure to thoroughly test your app on devices running the latest software and fix all bugs before submitting. For apps already on the App Store that may have minor guideline issues, bug fixes can be approved as long as there are no legal concerns.
  },
}

@online{appledeveloper2020_bug_reporting_feedback_assistant_for_developers,
  url = {https://developer.apple.com/bug-reporting/},
  title = {Bug Reporting - Feedback Assistant for Developers},
  year = {2020},
  urldate = {2020-08-28},
  author = {{Apple Developers}},
  organization = {{Apple}},
}

@online{calleosoftware_AppPulseMobile,
  title = {AppPulse Mobile},
  url = {https://www.calleosoftware.co.uk/products/application-monitoring/apppulse-mobile},
  year = {[2015?]},
  author = {{Calleo Software}},
  organization = {{Calleo Consultants Ltd.}},
}

@online{ft2020_apple_risks_losing_an_epic_challenge,
  title = {Apple risks losing an epic challenge},
  url = {https://www.ft.com/content/a01807f8-606c-4444-8a27-398984e3bf3d},
  year = {2020},
  author = {{The Editorial Board}},
  organization = {{The Financial Times Limited}},
  quotes = {
	This week, Yvonne Gonzalez Rogers, a US district judge, ruled that while the two companies were locked in litigation Apple could continue to ban Fortnite from its App Store for violating its guidelines. But she also ruled that Apple could not revoke Epic’s right to access its developer ecosystem, harming innocent bystanders. 

	Existing laws do not adequately cover all the complex dynamics and inherent conflicts of interest in corporate-run digital markets, such as Apple’s App Store, Google’s Play Store or Amazon’s Marketplace. As it is, Apple itself plays in its own market as an app developer while operating as judge, jury, executioner and court of last appeal for all others. If Apple does not itself update its App Store to distinguish between those roles and become more flexible and transparent, then it can hardly complain if legislators eventually deploy far more blunt instruments to enforce those changes.
  },
}

@online{ft2020_apple_tracks_iphone_users_without_consent,
  title = {Apple tracks iPhone users without consent, claims activist Max Schrems},
  url = {https://www.ft.com/content/aa43188a-0624-48b2-bc18-96b1e78df836},
  author = {Javier Espinoza and Siddharth Venkataramakrishnan},
  year = {2020},
  publisher = {Financial Times},
  organization = {{The Financial Times Limited}},
  quote_1 = {According to noyb, the unique tracking code generated by each iPhone lets Apple and all iPhone app developers see how users behave without their knowledge or agreement},
}

@online{ft2021_building_trust_in_ai_systems_is_essential,
  title = {Building trust in AI systems is essential},
  url = {https://www.ft.com/content/85b0882e-3e93-42e7-8411-54f4e24c7f87},
  year = {2021},
  author = {{The Editorial Board}},
  organization = {{The Financial Times Limited}},
  quotes = {
	"Translating such high principles into everyday practice is hard, especially when so much money is at stake. But three rules should always apply. First, teams that develop AI systems must be as diverse as possible to reduce the risk of bias. Second, complex AI systems should never be deployed in any field unless they offer a demonstrable improvement on what already exists. Third, algorithms that companies and governments deploy in sensitive areas such as healthcare, education, policing, justice and workplace monitoring should be subject to audit and comprehension by outside experts."

  }
}

@online{flutter_dev_site,
  title = {Flutter - Beautiful native apps in record time},
  url = {https://flutter.dev/},
  organization = {{Google LLC}},
  author = {{Google}},
  year = {2020},
  urldate = {2020-09-09},
  abstract = {Flutter is Google’s UI toolkit for building beautiful, natively compiled applications for mobile, web, and desktop from a single codebase.},
}

@online{gdpr_article_17_right_to_erasure,
  title = {Art. 17 GDPRRight to erasure (`right to be forgotten’)},
  url = {https://gdpr-info.eu/art-17-gdpr/},
  author = {{intersoft consulting}},
  organization = {{intersoft consulting services AG}},
  year = {2020},
  notes = {Retrieved 08 October 2020},
  privacy = {https://gdpr-info.eu/imprint-privacy-policy/},
}

@online{google_code_in_archive,
  title = {Google Code-in Archive},
  url = {https://codein.withgoogle.com/archive/},
  year = {2020},
  organization = {{Google Inc.}},
  author = {{Google}},
  abstract = {Google Code-in was a contest that introduced pre-university students (ages 13-17) to open source software development. The contest was held for 10 years starting in November 2010 and wrapping up the final contest in January 2020.},
}

@online{google_play_how_to_use_the_play_console,
  title = {How to use the Play Console},
  url = {https://support.google.com/googleplay/android-developer/answer/6112435?hl=en-GB},
  organization = {{Google}},
  author = {{Google}},
  year = {2020},
  summary = {Describes the 4 steps to register for a Google Play Developer Account: 1. Sign up for a Google Play Developer account, 2. Accept the Developer Distribution Agreement, 3. Pay the registration fee, of \$25 USD, 4 Complete your account details.}
}

@misc{google_play_launch_checklist,
  title = {Launch checklist},
  url = {https://developer.android.com/distribute/best-practices/launch/launch-checklist},
  year = {2020},
  last_updated = {Last updated 2020-04-16.},
  author = {{Google Developers}},
  organization = {{Google}},
  quote = {[Checklist step 2] `Prepare your developer account. Sign up for a developer account and check your developer account details are accurate. If you're going to sell products, set up your merchant account.'}
}

@online{google_play_policy_center_broken_functionality,
  author = {{Google Inc.}},
  organization = {{Google Inc.}},
  title = {[Broken Functionality] - Spam and Minimum Functionality - Developer Policy Center},
  url = {https://play.google.com/about/spam-min-functionality/min-functionality/},
  year = {2019}
}

@online{google_summer_of_code,
  title = {Google Summer of Code},
  url = {https://summerofcode.withgoogle.com/archive/},
  year = {2020},
  organization = {{Google Inc.}},
  author = {{Google}},
  abstract = {Google Summer of Code is a global program focused on bringing more student developers into open source software development. Students work on a three month programming project with an open source organization during their break from university.

  Since its inception in 2005, the program has brought together over 17,000 student participants and over 35,000 mentors from 124 countries worldwide. Google Summer of Code has produced over 38 million lines of code for 715 open source organizations.},
}

@online{hall2015_HP_courts_developers_with_tools_for_monitoring_mobile_apps,
  title = {HP Courts Developers with Tools for Monitoring Mobile Apps},
  url = {https://thenewstack.io/hp-courts-developers-with-tools-for-monitoring-mobile-apps/},
  author = {Susan Hall},
  year = {2015},
  publisher = {{The New Stack}},
  organization = {{The New Stack}},
}

@online{helloworld2017,
  title = {Say `Hello World' in 28 Different Programming Languages},
  url = {https://excelwithbusiness.com/blog/say-hello-world-in-28-different-programming-languages/},
  year = {2017},
  author = {Amanda Fielding},
  organization = {Excel with Business},
  _details_obtained_from = {https://excelwithbusiness.com/blog/tag/languages/ the first result},
  abstract = {Computers are dumb. They only do what they’re told. How do you tell a computer what to do? You use a programming language. The very first thing you’ll do when learning a new programming language is how to make the computer display “Hello, World”.}
}

@online{mark_dodson_medium_story,
  title = {Google completely terminated our new business via our Google Play Developer Account},
  url = {https://blog.usejournal.com/google-wrongly-terminated-our-new-business-via-our-google-play-developer-account-5f5b7b742542},
  author = {Mark Dodson},
  organization = {HoopApp},
  abstract = {A plea to all android app developers and small start-up tech business owners to come together and force Google to change their automatic termination policies. And see https://www.hoopapp.co.uk/},
  year = {2019},
  month = {February},
  day = {7},
  note = {Last checked on 2020-01-26}
}

@misc{google_use_pre_launch_reports,
  title = {Use pre-launch reports to identify issues},
  url = {https://support.google.com/googleplay/android-developer/answer/7002270?hl=en},
  year = {2020},
  author = {{Google}},
}

@misc{play_console_help_android_vitals_2019,
    title = {Monitor your app's technical performance with Android vitals},
    url = {https://support.google.com/googleplay/android-developer/answer/7385505},
    abstract = {},
    author = {{Google}},
    urldate = {2019-07-23},
    year = {2019}
}

@misc{googlepatent_hyman2016_collecting_application_usage_analytics,
  title={Method and system for collecting and providing application usage analytics},
  author={Hyman, Jonathan and Magnuson, William},
  year={2016},
  month=jan # "~19",
  publisher={Google Patents},
  note={US Patent 9,239,771}
}

@online{appbrain_download_statistics_june_2019,
    title = {Android app download statistics on Google Play},
    url = {https://www.appbrain.com/stats/android-app-downloads},
    author = {AppBrain},
    organization = {{AppBrain}},
    urldate = {2019-06-20},
    date = {2019-06-19},
    year = {2019}
}


@online{appbrain_android_analytics_libraries_23-oct_2019,
    title = {Android analytics libraries},
    url = {https://www.appbrain.com/stats/libraries/tag/analytics/android-analytics-libraries},
    author = {{AppBrain}},
    urldate = {2019-10-22},
    date = {2019-10-23},
    year = {2019},
    note = {Retrieved 2019-Oct-23}
}

@online{appbrain_android_crash_reporting_libraries_18_oct_2019,
    title = {Android crash reporting libraries},
    url = {https://www.appbrain.com/stats/libraries/tag/crash-reporting/android-crash-reporting-libraries},
    author = {{AppBrain}},
    urldate = {2019-08-09},
    date = {2019-08-08},
    year = {2019},
    month = {Oct},
    day = {18},
    note = {Retrieved 2018-Oct-18}
}

@online{___answersblog_2015_june_update,
  title = {Answers June update: behind the curtain},
  url = {https://web.archive.org/web/20160414070440/https://answers.io/blog/answers-june-update-behind-the-curtain},
  author = {Brian Swift},
  year = {2015},
  organization = {{Twitter Inc.}},
  abstract = {
    While we’ve been heads down working on some major upgrades for Answers, we thought it would be the perfect time to look back on how far our team has come. Answers is about to celebrate its first birthday, and in the past year, we’ve reached some incredible milestones and learned a lot about what you need in your analytics solution.

    In early 2014, the same team that built Crashlytics worked on a “Hackweek” project focused on what an analytics product would look like if we decided to build it. With the support of Twitter’s executive team, we formed a small team of six and built what was then called “Insights by Crashlytics.” But 48 hours before “Insights” was slated to launch, we decided to change the name to what you know today as Answers.

    The rest of the journey has been incredible.

    We went from processing 50 billion sessions a month to over five billion sessions every day. In May, Answers was ranked the #2 mobile analytics solution on iOS, and #3 on Android. But that just scratches the surface of where we’ve come from, and where we’re heading. We’re humbled by the support and community that has been built around Answers, and we’re thrilled for the future.
  },
}

@online{burke2014_wayne_chang_interview,
  title = {Wayne Chang, Crashlytics co-founder and Twitter developer lead},
  url = {https://www.siliconrepublic.com/play/the-interview-wayne-chang-crashlytics-co-founder-and-now-twitter-developer-lead},
  author = {Elaine Burke},
  year = {2014},
  publisher = {{Silicon Republic}},
  organization = {{Silicon Republic}},
  quotes = {
    The product was built for developers to let them know what caused their apps to crash, down to the line of code.
    Chang was heavily involved in the creation of Twitter Fabric and, for him, developers will always be at the heart of his concerns. One feature he highlighted was the way in which Twitter Fabric uses Crashlytics data in a way that saves developers from information overload or “analysis paralysis”.
  }
}

@online{catrobat_first_steps_into,
  title = {First steps into Catrobat},
  url = {https://developer.catrobat.org/first_steps},
  year = {2015},
  organization = {{Catrobat}},
  author = {{The Catrobat Project}},
  urldate = {2021-04-28},
}

@online{catrobat_project,
  title = {Home - Catrobat},
  url = {https://catrobat.org/},
  year = {2021},
  urldate = {2021-04-28},
  organization = {Catrobat},
  author = {Catrobat},
}

@online{cfdr_usenix,
  title = {THE COMPUTER FAILURE DATA REPOSITORY (CFDR) - USENIX},
  url = {https://www.usenix.org/cfdr},
  organization = {{USENIX}},
  author = {{USENIX}},
  year = {2021},
  details = {Public datasets from 1996 to 2009, seemingly active from 2006 to 2009},
}

@online{chang2015_how_six_people_built_crashlytics,
  title = {The inside story of Answers: How six people built the number one most popular mobile analytics tool in just a few months},
  url = {https://chang.com/how-six-people-built-the-2-mobile-analytics-tool-in-just-a-few-months-full-article/index.html},
  author = {Wayne Chang},
  organization = {{Crashlytics}},
  year = {2015},
  note = {Retrieved 2020-Dec-23},
}

@online{ebling2018_so_s9_specific_webview_device_crash_report,
  title = {S9/S9+ specific WebView device crash report},
  url = {https://stackoverflow.com/questions/49645746/s9-s9-specific-webview-device-crash-report},
  year = {2018},
  author = {Andrew Ebling},
  organization = {Stack Exchange},
  publisher = {Stack Overflow},  
}

@online{exodus_privacy_project,
   title = {exodus},
   key = {exodus privacy project},
   url = {https://reports.exodus-privacy.eu.org/en/},
   organization = {{Exodus Privacy}},
   urldate = {2020-09-09},
   year = {2020},
   abstract = {Exodus Privacy is a non-profit organization led by hacktivists. Its purpose is to help people get a better understanding of the Android applications tracking issues.},
   note = {Retrieved 2020-Sep-09}
}

@online{github_catroid,
  title = {Catrobat/Catroid: Writing programs on an Android device without prior knowledge.},
  url = {https://github.com/Catrobat/Catroid},
  year = {2021},
  author = {{Catrobat Team}},
  organization = {{Catrobat}},
  journal = {GitHub repository},
  urldate = {2021-04-28},
}

@online{iteratively_homepage,
  title = {Iteratively - Capture customer data you trust},
  url = {https://iterative.ly/},
  key = {Iteratively},
  year = {2020},
  urldate = {2020-09-11},
  organization = {{Iteratively, Inc}},
}

@online{kiwix_about_the_project,
  title = {About Kiwix},
  url = {https://www.kiwix.org/en/about/},
  year = {2021},
  organization = {{Kiwix Association}},
  author = {Kiwix},
  urldate = {2021-05-28},
}

@misc{github_kiwix_android,
  title = {Kiwix for Android},
  url = {https://github.com/kiwix/kiwix-android},
  year = {2021},
  author = {Various contributors},
  journal = {GitHub repository},
  organization = {{GitHub, Inc.}},
}

@online{kiwix_release_2_5_0,
    title = {Release 2.5.0},
    url = {https://github.com/kiwix/kiwix-android/releases/tag/2.5.0},
    abstract = {},
    author = {Mac Gillicuddy, Seán},
    organization = {{The Kiwix Project}},
    journal = {GitHub repository},
    urldate = {2019-06-17},
    year = {2019}
}

@online{kiwix_release_2_5_3,
    title = {Release 2.5.3},
    url = {https://github.com/kiwix/kiwix-android/releases/tag/2.5.3},
    abstract = {},
    author = {Mac Gillicuddy, Seán},
    organization = {{The Kiwix Project}},
    journal = {GitHub repository},
    urldate = {2019-08-20},
    year = {2019}
}

@online{knuth_trutex,
  title = {An Example of Donald Knuth's Reward Check},
  url = {http://www.truetex.com/knuthchk.htm},
  author_with_PhD = {Richard J. Kinch {Phd}},
  author = {Richard J. Kinch},
  year = {1999},
  organization = {{TrueTeX}},
  key = {Kinch},
}

@online{littledata2020_google_analytics_doesnt_match_shopify,
  title = {Why your Google Analytics data doesn't match your Shopify data},
  url = {https://drive.google.com/file/d/1VTxaih8TVZ9V9hLekKVEgmHP55JdeoB4/view},
  year = {2020},
  key = {Littledata},
  organization = {Littledata},
  sections = {
    1. Top 6 reasons for innacuracy[sic],
    2. How a data mismatch damages your bottom line,
    3. Comparing different tracking methods,
  },
  note = {The guide appears to be incomplete.},
  urldate = {2020-12-28},
}

@online{noyb2020_noyb_files_complaint_against_apples_tracking_code_idfa,
  title = {noyb files complaints against Apple's tracking code "IDFA"},
  url = {https://noyb.eu/en/noyb-files-complaints-against-apples-tracking-code-idfa},
  key = {nyob},
  organization = {{NOYB – European Center for Digital Rights}},
  year = {2020},
}

@online{r_date_conversion_article,
  title = {Easily Converting Strings to Times and Dates in R with flipTime},
  url = {https://www.displayr.com/r-date-conversion/},
  author = {Matthew McLean},
  organization = {{Displayr, Inc}},
  year = {2017},
}

https://github.com/Displayr/flipTime/graphs/contributors 
@online{r_date_conversion_github,
  title = {flipTime - Tools for manipulating and presenting time series data},
  url = {https://github.com/Displayr/flipTime},
  author = {Various},
  organization = {{Displayr, Inc}},
  journal = {GitHub repository},
  year = {2017},
}

@online{r_bloggers_date_formats_in_r,
  title = {Date Formats in R},
  url = {https://www.r-bloggers.com/date-formats-in-r/},
  year = {2013},
  author = {Mollie Taylor},
  organization = {R-bloggers},
}

@online{r_bloggers_using_colclasses,
  title = {Using {colClasses} to Load Data More Quickly in {R}},
  url = {https://www.r-bloggers.com/using-colclasses-to-load-data-more-quickly-in-r/},
  year = {2013},
  author = {Mollie Taylor},
  organization = {R-bloggers},
}

@online{firebasesupport2020_dependencies_of_firebase_sdks_on_google_play_services,
  title = {Dependencies of Firebase Android SDKs on Google Play services},
  url = {https://firebase.google.com/docs/android/android-play-services},
  year = {2020},
  author = {{Google Developers}},
  organization = {{Google Inc.}},
  abstract = {Some Firebase Android SDKs depend on Google Play services, which means they will only run on devices and emulators with Google Play services installed. These Firebase SDKs communicate with the Google Play services background service on the device to provide a secure, up-to-date, and lightweight API to your app. Certain Android devices, such as Amazon Kindle Fire devices or those sold in some regions, do not have Google Play services installed.},
  note = {Updated 2020-12-16 UTC},
}

@online{fowler_datensparsamkeit_2013,
	title = {Datensparsamkeit},
	url = {https://www.martinfowler.com/bliki/Datensparsamkeit.html},
	abstract = {Datensparsamkeit is a German word that's difficult to translate properly into English. It's an attitude to how we capture and store data, saying that we should only handle data that we really need.},
	author = {Fowler, Martin},
	urldate = {2019-05-03},
	date = {2013-12-12},
	year = {2013},
	file = {Datensparsamkeit:/Users/julianharty/Zotero/storage/NZZBCYCZ/Datensparsamkeit.html:text/html}
}

@online{google_account_help_android_share_data_2019,
    title = {Share usage \& diagnostics information with Google},
    url = {https://support.google.com/accounts/answer/6078260},
    abstract = {To help us improve Android, you can let your device send us information about how you use it and how it’s working.},
    author = {Google},
    urldate = {2019-06-14},
    date = {2019},
    year = {2019}
}

@misc{harty_aymer_playbook_website,
	title = {The {Mobile} {Analytics} {Playbook} ({Website})},
	url = {http://www.themobileanalyticsplaybook.com/},
	urldate = {2017-10-03},
	author = {Harty, Julian and Aymer, Antoine},
	file = {The Mobile Analytics Playbook:/Users/julianharty/Library/Application Support/Zotero/Profiles/4slbw694.default/zotero/storage/3T9BHTRQ/www.themobileanalyticsplaybook.com.html:text/html},
}

@online{harty_beaufort_scale_2018,
    title = {Beaufort Scale of Testing Software},
    url = {http://blog.bettersoftwaretesting.com/2018/04/beaufort-scale-of-testing-software/},
    abstract ={},
    author = {Harty, Julian},
    urldate = {2019-06-19},
    date = {2018-04-13},
    year = {2018}
}

@online{hurd2016_answer_to_how_do_i_develop_android_apps_on_kindle_fire,
  title = {How do I develop Android apps on Kindle Fire?},
  author = {Blake Hurd},
  organization = {Amazon Inc.}},
  publisher = {{Qu{ora}},
  year = {2016},
  developing_for_kindle_fire = {
    Amazon’s Fire OS is Amazon’s own version of Android OS. It pretty closely tracks Google’s releases of the OS, so you will find that most Android apps work just fine on it. The main difference is that the Fire OS does not include Google Services app which many of Googles’ service APIs rely on. Instead it includes a variety of Amazon’s libraries and APIs which you get access to. If your app doesn’t depend on Google Service APIs, then you can just submit it to the Amazon App Store; it should work fine on Fire devices. Or you can install it onto your own personal device in the same way you would any Android device.
  },
}

@online{itil_ishikawa_example,
  title = {Annex 6C: Ishikawa Diagrams},
  url = {http://gurri-itil.tripod.com/Service\%20Support/cd/content/ss06_6c.htm},
  organization = {{SAS Institute}},
  author = {Unspecified},
  year = {2000},
  copyright = {Crown Copyright 2000},
}

@online{izzy_android_without_google_microg_2015,
    title = {Android without Google: microG},
    url = {https://android.izzysoft.de/articles/named/android-without-google-5a},
    author = {Rehberg, Andreas Itzchak}, 
    urldate = {2019-06-14},
    date = {2015-10-27},
    year = {2015}
}
% izzy's name found in https://android.izzysoft.de/text/books/images/inoffizielles_android_1.png

@online{krysmanski2012_so_redirect-stdout-to-logcat-in-android-NDK,
  title = {Redirect stdout to logcat in Android NDK},
  url = {https://stackoverflow.com/questions/10531050/redirect-stdout-to-logcat-in-android-ndk},
  author = {Sebastian Krysmanski},
  year = {2012},
  organization = {Stack Exchange},
  publisher = {Stack Overflow},
}

@online{learner2011_so_how_to_access_anrs_and_tombstones,
  title = {how to access android files /data/anr/traces.txt and /data/tombstones/tombstones},
  url = {https://stackoverflow.com/questions/5467972/how-to-access-android-files-data-anr-traces-txt-and-data-tombstones-tombstones},
  year = {2011},
  author = {learner},
  organization = {Stack Exchange},
  publisher = {Stack Overflow},  
}

@online{mopinion2017_top11_mobile_in_app_feedback_tools,
  title = {Top 11 Best Mobile In-App Feedback Tools: An Overview},
  url = {https://mopinion.com/top-11-best-mobile-in-app-feedback-tools-an-overview/},
  year = {2017},
  author = {Erin Gilliam Haije},
  organization = {mopinion},
  publisher = {mopinion},
}

@online{NHS_organ_donation_in_england,
    title = {Organ donation law in England - NHS Organ Donation},
    url = {https://www.organdonation.nhs.uk/uk-laws/organ-donation-law-in-england/},
    abstract = {From spring 2020, all adults in England will be considered to have agreed to be an organ donor when they die unless they have recorded a decision not to donate or are in one of the excluded groups. This is commonly referred to as an ‘opt out’ system. You may also hear it referred to as 'Max and Keira's Law'.},
    author = {NHS Blood and Transplant},
    organization = {{NHS, UK}},
    urldate = {2019-06-20},
    date = {2019-06-20},
    year = {2019}
}

@online{mcquate_I_saw_you_were_online,
    title={‘I saw you were online’: How online status indicators shape our behavior},
    url={https://www.washington.edu/news/2020/04/13/how-online-status-indicators-shape-our-behavior/},
    author={Sarah McQuate},
    organization = {University of Washington},
    year={2020},
    summary={A summary of User Experiences with Online Status Indicators' written for a general audience.},
    quote_1={Then the researchers asked the participants to time themselves while they located the settings to turn off “appearing online” in each app they used regularly. For the apps that have settings, participants gave up before they found the settings 28\% of the time. For apps that don’t have these settings, such as WhatsApp, participants mistakenly thought they had turned the settings off 23\% of the time.},
    quote_2={“When you put some of these pieces together, you’re seeing that more than a third of the time, people think they’re not broadcasting information that they actually are,” Cobb said. “And then even when they’re told: ‘Please go try and turn this off,’ they’re still not able to find it more than a quarter of the time. Just broadly we’re seeing that people don’t have a lot of control over whether they share this information with their network.”},
    quote_3={},
    quote_4={},
}

@online{play_console_help_view_crashes_2019,
    title = {View crashes \& application not responding (ANR) errors},
    url = {https://support.google.com/googleplay/android-developer/answer/6083203},
    abstract = {Using the Play Console, you can view data for crashes and application not responding (ANR) errors for your apps. Data comes from Android devices whose users have opted in to automatically share their usage and diagnostics data.
    
    Also, if you use pre-launch reports to identify issues with your apps, crashes found during testing are listed with your app’s crashes and ANRs. However, because crashes found while generating a pre-launch report come from test devices, they don’t affect your crash statistics.},
    author = {Google},
    urldate = {2019-06-14},
    date = {2019},
    year = {2019}
}

@online{popper_crokage_2019,
    title = {CROKAGE: A New Way to Search Stack Overflow},
    url = {https://stackoverflow.blog/2019/08/14/crokage-a-new-way-to-search-stack-overflow/?cb=1},
    abstract = {},
    author = {Popper Ben},
    urldate = {2019-08-29},
    date = {2019-08-14}
}

@online{salomonbrys_github_anr_watchdog,
  title = {SalomonBrys/ANR-WatchDog- A simple watchdog that detects Android ANRs (Application Not Responding).}, 
  url = {https://github.com/SalomonBrys/ANR-WatchDog},
  author = {Salomon Brys},
  year = {2013},
  urldate = {2021-07-21},
  organization = {GitHub},
}

@online{schreckengost2019_extending_elastic_stack_android_adb,
  title = {Extending the Elastic Stack to Fit Our Needs},
  url = {https://crossbrowsertesting.com/blog/development/extending-the-elastic-stack-to-fit-our-needs/},
  author = {Harold Schreckengost},
  urldate = {2020-12-23},
  organization = {{SmartBear CrossBrowserTesting}},
  year = {2019},
  extract = {
    we built at CrossBrowserTesting using libbeat was a custom shipper for Android mobile device logs. We have hundreds of real Android devices and trying to keep track of logging across that many devices poses a unique challenge that the majority of businesses will never have.
    
    With Androidbeat, our in-house data shipper for Android mobile devices, we had several requirements:
    
    It needed to use the standard Android Debug Bridge (adb) functionality built in to Andro
    It needed to handle temporary unavailability of a device gracefully
    It needed to be able to add in metadata about the devices attached, such as our unique identifiers, the severity of the logs, and the time the message was sent.
  }
}

@misc{se2012_story_points_for_bug_fixing_tasks_in_scrum,
  title = {Story points for bug fixing tasks: Is it suitable for Scrum?},
  url = {https://softwareengineering.stackexchange.com/questions/162145/story-points-for-bug-fixing-tasks-is-it-suitable-for-scrum},
  author = {palacsint},
  year = {2012},
  organization = {Stack Exchange},
  publisher = {Software Engineering}, 
  question = {
    I'm just wondering if we should assign story points to bug fixing tasks or not. JIRA, our issues-tracking software, does not have story point field for Bug type issues (it's only for Storys and Epics).
    Should we add the Bug issue type to the applicable issue types of the Story Points field? What are the pros and cons? Would it be suitable for Scrum?
  },
}

@misc{so2021_shankar_strange_crash_in_cronetDynamite.apk,
  title = {Strange Crash in CronetDynamite.apk (offset 0x1000) Android},
  author = {Hari Shankar S},
  year = {2021},
  organization = {Stack Exchange},
  publisher = {Stack Overflow},  
}

@online{github2017_k9mail_issue_2705,
  title = {Deleted mail from inbox is doubled in trash directory},
  url = {https://github.com/k9mail/k-9/issues/2705},
  author = {gandogar},
  year = {2017},
  key = {K-9 Mail GitHub},
  organization = {{K-9 Mail}},
  journal = {GitHub repository},
  urldate = {2020-12-23},
  comment = {The issue was opened on 25 Aug 2017 and took until 24 Apr 2019. Multiple people contributed examples.},
}

@online{github2020_k9mail_logging_errors,
  title = {Logging Errors - K-9 Mail},
  url = {https://github.com/k9mail/k-9/wiki/LoggingErrors},
  abstract = {K-9 Mail has controllable debug logging. Users can activate logging to help diagnosing problems and errors.},
  year = {2020},
  authorx = {Various},
  key = {K-9 Mail GitHub},
  organization = {{K-9 Mail}},
  journal = {GitHub repository},
  urldate = {2020-12-23},
}

@online{github2020_sematext_logsene_android,
  title = {Sematext Logs is ELK as a Service.},
  url = {https://github.com/sematext/sematext-logsene-android},
  authorx = {Various},
  key = {Sematext GitHub},
  year = {2020},
  urldate = {2020-12-23},
  organization = {{Sematext}},
  journal = {GitHub repository},
  abstract = {
    Sematext Logs is ELK as a Service. This library lets you collect mobile analytics and log data from your Android applications using Sematext. There is an equivalent library for shipping logs from iOS available... Use the Mobile Application Logs Integration to get out-of-the-box reports with the most important information about your mobile applications.}
}

@misc{rcdailey2018_ndk_redirect_to_logcats,
  title = {How to redirect stdout and stderr to android logcats?},
  url = {https://github.com/android/ndk/issues/671},
  journal = {GitHub repository},
  author = {Robert Dailey},
  year = {2018},
}

@online{safedk2015_in_app_protection_on_youtube,
  title = {SafeDK - In App Protection},
  url = {https://youtu.be/tGwAXEouy80},
  organization = {{SafeDK}},
  year = {2015},
  urldate = {2020-05-26},
  author = {{SafeDK}},
}

@online{techcrunch2015_quettra_mobile_analytics_acquired,
  title = {SimilarWeb Buys Quettra To Move Deeper Into Mobile Analytics And Big Data},
  url = {https://techcrunch.com/2015/12/10/similarweb-buys-quettra-to-move-deeper-into-mobile-analytics-and-big-data/},
  year = {2015},
  author = {Ingrid Lunden},
  organization = {TechCrunch},
  quotes = {
    "Yet more consolidation is happening in the world of analytics. Today Israeli web analytics and traffic measurement startup SimilarWeb announced that it has acquired Quettra, a provider of mobile analytics and measurement tools founded by Ankit Jain, the former head of search and discovery in the Google Play store.",
    
  },
}

@online{timber_io_homepage_2020,
  title = {Log Better. Solve Problems Faster.},
  url = {https://timber.io/},
  urldate = {2020-08-18},
  author = {{Timber Technologies, Inc.}},
  abstract = {Timber is a new kind of cloud-based logging system designed for applications and developers. Spend less time debugging and more time shipping.},
  quote_0 = {Structured logging meets readability. Developers read their logs. Timber's thoughtful presentation gives you the power of structured logs without sacrificing readability.},
  quote_1 = {Do more with your logs. Structured data, events, context, timings, metrics? We've got you covered.},
  quote_2 = {User Context. Always know which user generated which log with automatic user context.},
  quote_3 = {Tail a user},
}

@misc{techyourchance2021_contentprovider_in_android_libraries_considered_harmful,
  title = {ContentProvider in Android Libraries Considered Harmful},
  url = {https://www.techyourchance.com/contentprovider-in-android-libraries-considered-harmful/},
  year = {2021},
  author = {Vasiliy Zukanov},
  organization = {{TechYourChance}},
}

@misc{tsiombikas2014_native_NDK_stdio_to_android_log,
  title = {How to use standard output streams for logging in android apps},
  url = {https://codelab.wordpress.com/2014/11/03/how-to-use-standard-output-streams-for-logging-in-android-apps/},
  author = {John Tsiombikas},
  year = {2014},
}

@online{twitter2015_edsolovey_handling_5B_sessions_a_day_in_real_time,
	title = {Handling five billion sessions a day – in real time},
	url = {https://blog.twitter.com/engineering/en_us/a/2015/handling-five-billion-sessions-a-day-in-real-time.html},
	urldate = {2017-10-16},
	author = {@edsolovey},
	year = {2015},
	organization = {{Twitter}},
	file = {Handling five billion sessions a day – in real time:/Users/julianharty/Library/Application Support/Zotero/Profiles/4slbw694.default/zotero/storage/DXUH8T5P/handling-five-billion-sessions-a-day-in-real-time.html:text/html}
}

@online{twitterdev2015_a_deep_dive_into_the_answers_backend,
  title = {Twitter Flight 2015 - A Deep Dive into the Answers Backend by Ed Solovey},
  url = {https://youtu.be/G1LrMXR3Zko},
  organization = {{Twitter}},
  year = {2015},
  urldate = {2020-12-27},
  author = {{Twitter Developers}},
  abstract = {Answers is a real-time, opinionated mobile analytics product. In the 15 months since its public launch, Answers has grown to be one of the most widely adopted mobile analytics SDK's. Today it links over a million events per second, and every day processes more than 6 billion mobile app sessions across tens of thousands of mobile apps. This talk will briefly introduce the product and go over some of the back-end architecture design that helps us scale it to these volumes: stream/batch processing, probabilistic data structures, intelligent event grouping during stream processing, tailored key-value schemas, and sampling.},
}

@online{zipternet_github,
  title = {Zipternet GitHub project},
  url = {https://github.com/ISNIT0/AndroidCrashDummy/blob/9c6d85b633568fcf64a78fc55e810ae9f5e864b4/app/src/main/java/com/example/user/androidtestapp/MainActivity.java},
  journal = {GitHub repository},
  year = {2019},
  note = {Last checked on 2019-10-12}
}

@misc{bbc_iplayer_app_april_2021_webview_information,
  title = {BBC iPlayer - Apps on Google Play [cached 05 Apr 2021]},
  url = {https://play.google.com/store/apps/details?id=bbc.iplayer.android&start=100&hl=en_IE&gl=US},
  year = {2021},
  author = {{BBC}},
  extract = {BBC iPlayer
    Media Applications Technologies for the BBC Entertainment
    TeenTeen
    126,994
    Add to wishlist
    Screenshot Image Screenshot Image Screenshot Image Screenshot Image Screenshot Image Screenshot Image Screenshot Image Screenshot Image Screenshot Image Screenshot Image Screenshot Image Screenshot Image Screenshot Image Screenshot Image Screenshot Image Screenshot Image Screenshot Image Screenshot Image
    If you are experiencing a problem when opening or using the BBC iPlayer app, updating the Android System WebView via Google Play should now resolve the issue.
    1. Navigate to Play Store app
    2. Search for Android System WebView
    (https://play.google.com/store/apps/details?id=com.google.android.webview)
    3. Select the "Update" option

    BBC iPlayer brings you the latest and greatest TV series and box sets from the BBC. Watch live, on-demand or download to take away with you - all in one app!
  },
}

@misc{bbcnews2021_google_fixes_crashing_android_app_issues,
  title = {Google fixes crashing Android app issues},
  url = {https://www.bbc.co.uk/news/technology-56496783},
  author = {{BBC News}},
  year = {2021},
  article = {
    Google has fixed a problem that meant Android phone apps were crashing for many users.
    The issues began on Monday and affected apps such as Gmail, Facebook and Amazon.
    It appears that an update to the Android System WebView, which allows Android apps to display web content, was to blame.

    Google told users to update their Android System WebView and Google's Chrome browser.

    "We have resolved the issue with WebView that caused some apps on Android to crash for some users. Updating Android System WebView and Google Chrome via Google Play should now resolve the issue," a Google spokesman told the BBC.

    Social media was filled with reports of apps crashing, and DownDetector, a website that measures outages, showed a surge in problems for Gmail and Amazon. Android WebView is a system component that is pre-installed on all Android devices.

    To update it, users need to: navigate to the Play Store app, search for Android System WebView, select the "Update" option, repeat these steps for Google Chrome
  }
}

@misc{bbcnews2020_nhs_covid_19_app_bluescreen_glitch,
  title = {NHS Covid-19 app suffers 'blue screen' glitch},
  url = {https://www.bbc.com/news/technology-54958785},
  author = {{BBC News}},
  year = {2020},
  article = {
    The NHS Covid-19 app has stopped working for many iPhone owners, who are unable to get it to launch. Users report being stuck at a blue loading screen with the contact-tracing app's logo - but nothing else happens.
    
    The NHS has published a workaround for the problem in its help files, but has not said what caused the problem or when it will be fixed. Apple does not believe the problem is at its end, since it has not seen the issue arise in other countries' apps.
    
    Many different nations use the same underlying technology, which is designed by Apple and Google, to notify users if they were recently near to someone who subsequently tested positive for the virus.

    Some users have deleted and reinstalled the app to fix the fault, but that deletes useful information - this includes a log of venues the user has checked into via QR barcode scans.

    The NHS's workaround instead asks users to reset their iPhone's location and privacy settings. It also recommends users have the most up-to-date version of Apple's iOS operating system downloaded and installed.

    But carrying out the reset prevents all apps on the handset from using the device's location until they are granted permission again.

    NHS Covid app updated to 'fix' phantom messages
    Contact-tracing app not sharing data with police

    Some users have said they fixed the problem by force-quitting the app - which can be done by flicking the frozen screen up and off the display - and then re-launching it.

    The problem first emerged last week, but complaints became more frequent over the weekend and into Monday. The cause, however, remains unclear. In a statement, the Department of Health and Social Care said it was aware of the issue. "The app is still scanning, even if the screen appears blue," it said. "There are simple steps iPhone users can take to resolve this issue, which are set out on the app's website, and work is underway to identify the cause. "Users experiencing this issue should make sure their Apple iOS is updated to the latest version of the software." 
  }
}

@misc{bischoff2020_firebase_missconfiguration,
  title = {Estimated 24,000 Android apps expose user data through Firebase blunders},
  author = {Bischoff, Paul},
  url = {https://www.comparitech.com/blog/information-security/firebase-misconfiguration-report/},
  year = {2020},
  publisher = {{Comparitech}},
}

@misc{crittercism2015_homepage,
  title = {Mobile Application Intelligence - Crittercism},
  url = {https://web.archive.org/web/20150908160428/https://www.crittercism.com/},
  year = {2015},
  author = {{Crittercism, Inc.}},
  claims = {
    Let Crittercism give you insight to success. We monitor billions of digital customer experiences each day. Deliver great functionality so users stay engaged with your app.


    Protect your mobile revenue. Monitor how and when your app stops working so you can resolve problems before they cost you users.
    Keep your developer’s life simple. With just one line of code, your team can organize and prioritize errors for a stronger plan of attack.
    Diagnose critical issues, see which devices and operating systems cause trouble, and prioritize bug fixes for the biggest business impact.
    Crittercism takes less than five minutes to install, runs in the background, and comes with world-class support (just in case).
    
    Don’t let small issues become big problems. Using a complete set of tools, get real-time data that shows where and how errors occur so you can resolve issues quickly and keep your app running seamlessly.

    Transaction Monitoring. Monitor critical mobile workflows such as login, checkout, and barcode scan.Track revenue at risk, success rates, and slow transactions that could lead to abandonment.
    
    Crash & Error Monitoring. See all mobile app crashes (and gracefully handled exceptions) in real time from the end users perspective so you’re in the know, not the last to know.
    
    3rd Party API Monitoring. Poorly performing APIs take a toll on your business revenue. Get latencies, error rates, data bandwidth and the number of API requests so you can troubleshoot.
  }
}

@misc{dave2020_reuters_firebase_squeeze,
  title = {Google critics see its {Firebase} tools as another squeeze play},
  author = {Dave, Pavesh},
  publisher = {Reuters},
  year = {2020},
  url = {https://uk.reuters.com/article/us-google-antitrust-focus/google-critics-see-its-firebase-tools-as-another-squeeze-play-idUKKBN2161GA},
}

@misc{google_play_developer_policy_center,
  title = {Print view: Google Play Developer Policy Center},
  url = {https://play.google.com/about/developer-content-policy-print/},
  year = {2019},
  author = {{Google}},
}

@misc{graham_measuring_2009,
	title = {Measuring the effectiveness of testing using {DDP}},
	pages = {22},
	journaltitle = {case studies},
	author = {Graham, Dorothy},
	date = {2009},
	year = {2009},
	langid = {english},
	file = {Graham - 2009 - Measuring the effectiveness of testing using DDP.pdf:/Users/julianharty/Zotero/storage/3J7ZYZ9B/Graham - 2009 - Measuring the effectiveness of testing using DDP.pdf:application/pdf}
}

@misc{Martinez_2019, 
  title={Google just terminated our start-up Google Play Publisher Account on Christmas day}, url={https://android.jlelse.eu/google-just-terminated-our-start-up-google-play-publisher-account-on-christmas-day-5cb69a454da0}, 
  abstractNote={An open letter from an Android developer to the Android Community and specially to Purnima Kochikar, director of Google Play, Apps & Games…}, 
  journal={Medium}, 
  author={Martínez, Pablo A.}, 
  year={2019}, 
  month={Jul}
}

@misc{moneycontrolnews2021_cowin_apps_crash,
  title = {CoWIN, Aarogya Setu apps crash as COVID vaccine registration for 18-44 age group opens},
  url = {https://www.moneycontrol.com/news/coronavirus/cowin-aarogya-setu-apps-crash-as-covid-vaccine-registration-for-18-44-age-group-opens-6826441.html},
  author = {{MoneyControl News}},
  year = {2021},
  article = {
    Several users are reporting issues with the app.
    
    The CoWIN and Aarogya Setu apps crashed shortly after the registration for COVID -19 vaccination of the age groups 18 to 44 opened at 4 pm today. Several users reported they were unable to access these apps to register for vaccination.

    The government recently announced that citizens between the ages of 18 and 44 would be able to register for the COVID-19 vaccination from 04:00 pm (IST) on April 28.
    The issues range from server issues, “Cowin Server is facing issues, please try later”, to timeout errors, “504 Gateway Time-out”. We were also able to confirm the error. Several Twitter users have been critical of the government’s lack of preparedness for the influx of traffic on the app.

    Cowin server has been crashed
    Aarogya setu app working like IRCTC app.
    No reservations on time
    Retweet #CowinApp#CovidIndia#CoWin#cowinregistration#Maharashtra#MaharashtraFightsCorona#mumbai#MumbaiFightsCorona#pune#PuneFightsCorona#UddhavThackeray#cowinregistrationpic.twitter.com/Qupq2cAljd

    — Shashank shukla (@ishukla_sk) April 28, 2021


    #CoWin@narendramodi u cannot run one app, how would run the country? just asking? #CowinApp#cowinregistration#MautKaSaudagarpic.twitter.com/ZOvBQXaS4z

    — Logical_Bhartiya (@LogicalBhartiy2) April 28, 2021


    #cowinregistration#CoWin#NarendraModi
    Expected.. pic.twitter.com/Fulb1q393Q

    — Suresh Raju (@IamSureshNV) April 28, 2021''


    Waiting for OTP#VaccineRegistration pic.twitter.com/01p9EHWBrt

    — Rohan (@rohanreplies) April 28, 2021

    It is worth noting that even if you are able to register for the vaccine, it might take a bit of time before you can actually get it as many states are saying that the Serum Institute of India (SII) has promised them supplies only after May 15. 
  }
}

@misc{winder2020_forbes_on_the_class_action_firebase_analytics,
  author = {Winder, Davey},
  title = {Google Still Tracks App Users When They've Opted Out, Privacy Lawsuit Alleges},
  url = {https://www.forbes.com/sites/daveywinder/2020/07/15/google-still-tracks-app-users-when-theyve-opted-out-privacy-lawsuit-alleges-class-action-firebase-data-tracking/#75b7d5c53864},
  year = {2020},
  note = {Retrieved 16 July 2020},
}


@misc{zeller2012_udacity_software_debugging_course,
  title = {Software Debugging - Automating the Boring Tasks},
  url = {https://www.udacity.com/course/software-debugging--cs259},
  author = {Andreas Zeller and Gundega Dekena},
  year = {2012},
  note = {Visited \nth{14} July 2021},
  publisher = {{Udacity Inc.}},
  source_of_year = {https://www.whyprogramsfail.com/},
}

@misc{zeller2021_tweet_the_devils_guide_to_incremental_research_15_18,
  title = {The Devil's Guide to Incremental Research 15/18},
  author = {Andreas Zeller},
  url = {https://twitter.com/AndreasZeller/status/1382702371474653192?s=20},
  year = {2021},
  abstract = {
    Things to avoid: User studies. Real code. Talking to practitioners. All you'll learn is that your assumptions were all wrong, your approach won't work, and the problem is much hairier than expected. Don't let reality come between you and your publication. (15/18)
  }
  
}


@online{18f_dot_voting,
  title = {Dot voting - 18F methods},
  url = {https://methods.18f.gov/discover/dot-voting/},
  organization = {United States government},
  author = {18F},
  year = {2019},
  source = {https://github.com/18F/methods/blob/c83cf54e90db3541ed85d1334eb2b87e9926ce5b/_methods/dot-voting.md},
  note = {Retrieved 11 November 2020},
}


@online{android_crash_dummy,
  title = {AndroidCrashDummy GitHub project},
  url = {https://github.com/ISNIT0/AndroidCrashDummy},
  year = {2017},
  author = {Joe Reeve and Julian Harty},
  journal = {GitHub repository},
  note = {Last checked on 2021-01-03}    
}

@online{android_dashboard,
  title={Distribution dashboard},
  url={https://developer.android.com/about/dashboards/},
  author={{Android Project}},
  organization ={{Android Developers}},
  date={May 7, 2019},
  year={2019},
  month={May},
  day={7},
  urldate={11-Oct-2019},
  note = {Last checked on 2019-10-12}
}

@online{apk_expansion_files,
  title = {APK Expansion Files | Android Developers},
  url = {https://developer.android.com/google/play/expansion-files},
  author = {{Android Project}},
  organization ={{Android Developers}},
  year = {2021},
  urldate = {05-Jun-2021},
  note = {Last checked on 2021-06-05},
}

@online{android_log_assert,
  title = {AndroidLogAssert},
  url = {https://github.com/ISNIT0/AndroidLogAssert},
  year = {2017},
  organization = {{Commercetest Limited}},
  author = {Joe Reeve and Julian Harty},
  journal = {GitHub repository},
  note = {Last checked on 2021-01-03},
}

@online{android-stability-analysis,
  title = {Android Vitals Analysis GitHub project},
  url = {https://github.com/commercetest/android-stability-analysis},
  year = {2019},
  author= {{Commercetest Limited}},
  organization = {{Commercetest Limited}},
  journal = {GitHub repository},
  note = {Last checked on 2019-10-14}, 
}

@online{android_vitals_best_practices,
  title = {Use Android vitals to improve your app's performance, stability, and size},
  url = {https://developer.android.com/distribute/best-practices/develop/android-vitals.html},
  author={{Android Project}},
  organization ={{Android Developers}},
  note = {Last checked on 2020-11-02},
  quote1 = {Apps whose metrics are higher have greater promotability, which raises their ranking in Google Play Store searches. They also are more likely to be eligible for the New & Updated and Editor's Choice collections on Google Play, and to be nominated in the Google Play Awards.},
  year = {2019},
}

@misc{android2010_froyo_highlights_new_developer_services,
  title = {Android 2.2 Platform Highlights - New Developer Services},
  url = {https://web.archive.org/web/20100722094037/https://developer.android.com/sdk/android-2.2-highlights.html#DeveloperServices},
  year = {2010},
  author={{Android Project}},
  organization ={{Android Developers}},
  note = {Source: Wayback Machine: Last checked on 2021-06-17},
}

@misc{android_cronet_library,
  title = {Perform network operations using Cronet},
  url = {https://developer.android.com/guide/topics/connectivity/cronet},
  year = {2021},
  organization = {{Google Inc.}},
  author={{Android Project}},
  abstract = {
    Cronet is the Chromium network stack made available to Android apps as a library. Cronet takes advantage of multiple technologies that reduce the latency and increase the throughput of the network requests that your app needs to work.
    
    The Cronet Library handles the requests of apps used by millions of people on a daily basis, such as YouTube, Google App, Google Photos, and Maps - Navigation & Transit.
  }
  
}

@misc{android_platform_system_crash_reporter,
  title = {[Android] platform/system/crash\_reporter - Git at Google},
  url = {https://android.googlesource.com/platform/system/crash_reporter/},
  author={{Android Project}},
  year = {2016},
  organization = {{Google Inc.}},
  abstract = {
    crash_reporter is a deamon running on the device that saves the call stack of crashing programs. It makes use of the Breakpad library.

    During a build, Breakpad symbol files are generated for all binaries. They are packaged into a zip file when running m dist, so that a developer can upload them to the crash server.

    On a device, if the user has opted in to metrics and crash reporting, a Breakpad minidump is generated when an executable crashes, which is then uploaded to the crash server.

    On the crash server, it compares the minidump's signature to the symbol files that the developer has uploaded, and extracts and symbolizes the stack trace from the minidump.
  }
}

@misc{android_webview_privacy,
  title = {User privacy in WebView reporting | Android Developers},
  url = {https://developer.android.com/guide/webapps/webview-privacy},
  year = {2020},
  author={{Android Project}},
  organization ={{Android Developers}},
  note = {Last checked on 2021-06-17},  
}

@online{apple_app_review_overview,
  title = {App Review},
  url = {https://developer.apple.com/app-store/review/},
  author = {Apple},
  organization = {Apple},
  year = {2020},
  note = {Last checked on 2020-11-02},
  abstract = {We review all apps and app updates submitted to the App Store in an effort to determine whether they are reliable, perform as expected, respect user privacy, and are free of objectionable content. As you plan and build your app, use these guidelines and resources to help your app approval go as smoothly as possible.},
  quote_1 = {Review times may vary by app. On average, 50\% of apps are reviewed in 24 hours and over 90\% are reviewed in 48 hours.}
}

@online{chooseanopensourcelicense2020,
  title = {Choose an open source license},
  url = {https://choosealicense.com/},
  author = {Various},
  organization = {{GitHub, Inc.}},
  year = {2020},
  note = {Accessed:~\nth{16} November 2020},
}

@online{codehouse2020_cohort_analysis,
  title = {Cohort Analysis and how to use it in Google Analytics},
  url = {https://www.codehousegroup.com/insight-and-inspiration/digital-strategy/cohort-analysis-and-how-to-use-it-in-google-analytics},
  author = {Amith Singh},
  year = {2020},
  organization = {{Codehouse}},
  urldate = {2020-07-31},
}

@online{datethics2020_workshop,
  title = {{DatEthics 2020} workshop},
  url = {https://mobilehci-2020.datacentricdesign.org},
  author = {Jacky Bourgeois, Aaron Ding, Jered Vroon and Ella Peltonen},
  organization = {ACM},
  address = {Virtual Venue},
  year = {2020},
  urldate = {2020-10-05},
  abstract = {The Internet of Things makes human activity data – what people do, how they move, how they socialise – an abundant resource. However, this rich and intimate perspective on people, which uniquely shape and characterise their behaviours, can have tremendous ethical implication if data is handled irresponsibly. Being personal, contextual and accessible, mobile devices are key facilitators of (ir)responsible collection and use of data. In this workshop, we will use the Future Workshop approach to develop a research agenda towards an ethical data-centric design of intelligent behaviours. As part of this approach, we will (1) criticise the current mechanisms and infrastructure to frame ethical challenges, (2) fantasise on futures which support user and designer values, and (3) implement a research agenda for the MobileHCI community to emphasise the barriers to tackle. The outcomes of this workshop will foster ethical research and inspire the MobileHCI community.}
}

@online{dotmocracy,
  title = {How to Use Dot Voting Effectively},
  url = {https://dotmocracy.org/},
  organization = {Feedback Frames},
  author = {Jason Diceman},
  year = {2020},
  note = {Retrieved 11 November 2020},  
}

@online{firebasecrashlytics2020_customize_crash_reports,
  title = {Customize your Firebase Crashlytics crash reports},
  url = {https://firebase.google.com/docs/crashlytics/customize-crash-reports?platform=android},
  author = {{Google Developers}},
  year = {2020},
  organization = {{Google Inc.}},
}

@misc{gergelyorosz2021_twitter_mobile_app_poll,
  title = {Twitter Poll - how many mobile devs?},
  url = {https://twitter.com/GergelyOrosz/status/1345288831029956610},
  author = {Gergely Orosz},
  abstract = {
    (Native) mobile engineers: how many iOS / Android devs work on the same app at your company? Meaning contributing to the same app's codebase/dependencies. If it's more than 20 engineers, would love to hear the company name in a comment.},
    year = {2021},
    note = {Twitter poll with 818 votes, Jan 2, 2021},
}

@online{googleanalytics2021_the_cohort_analysis_report,
  title = {The Cohort Analysis report},
  url = {https://support.google.com/analytics/answer/6074676?hl=en},
  year = {2021},
  organization = {{Google Inc.}},
  author = {{Google Inc.}},
}

@online{log-captor-github-project,
  title = {LogCaptor},
  author = {Hakky54 and fossabot},
  url = {https://github.com/Hakky54/log-captor},
  journal = {GitHub repository},
  organization = {{GitHub, Inc.}},
  note = {Last checked 2021-09-16},
  year = {2021},
}

@online{mitlicense2020_ongithub,
  title = {MIT License},
  url = {https://choosealicense.com/licenses/mit/},
  author = {{Massachusetts Institute of Technology}},
  organization = {{GitHub, Inc.}},
  year = {2020},
  note = {Accessed:~\nth{16} November 2020},
}

@online{nngroup_dot_voting,
  title = {Dot Voting: A Simple Decision-Making and Prioritizing Technique in UX},
  url = {https://www.nngroup.com/articles/dot-voting/},
  year = {2019},
  summary = {By placing colored dots, participants in UX workshops, activities, or collaborative sessions individually vote on the importance of design ideas, features, usability findings, and anything else that requires prioritization.},
  organization = {Nielsen Norman Group},
  author = {Sarah Gibbons},
}

@online{segment_analytics_for_android_docs,
  title = {Analytics for Android},
  url = {https://segment.com/docs/connections/sources/catalog/libraries/mobile/android/#analytics-for-android},
  year = {2020},
  organization = {{Segment.io inc}},
  author = {{Segment.io}},
  quote_0 = {Analytics-Android saves up to 1000 calls on disk, and these never expire.},
}

@online{segmentio_supporting_6_months_offline,
  title = {Supporting 6 months offline \#701},
  url = {https://github.com/segmentio/analytics-android/issues/701},
  author = {John Hatton and Prayansh Srivastava},
  organization = {{SIL International and Segment.io}},
  journal = {GitHub repository},
  year = {2020},
  details = {Our non-profit reading app is used to support literacy education in developing countries, and the analytics are used to see which books are being read, and how well kids are doing on comprehension questions. We use Segments warehouse service to populate our postgresql DB, then offer public dashboards showing how and where books are being read. So we love that this sdk saves 1000 events while waiting to send them when online. However, that is no longer enough.

  In an upcoming aid project, 1400 tablets containing our app will go out to kids in a super rural part of the Pacific. Every six months, the tablets will be brought together for the purpose of getting new books and collecting analytics. The project needs our app to store up to six months of reading activity; at a minimum of 3 events per tiny book, some kids may blow past the 1000 events. Our event payloads are tiny.

  #1 We are thinking of doing a fork that increases this to 10 * 10,000. Do you see any red flags on that? If you'd rather we submit a PR that makes it configurable, we could do that instead.

  #2 Even during the every-six-months gathering of the tablets, the project would still like to not depend on a rural connection to the internet. They would like to automatically pull the QueueFile off of every machine and store it for later processing. Does that sound feasible? In other words, are there any known barriers to us writing some utility that takes 1400 queue files, then sends each even to Segment, one at a time?}
}

@online{vitals_scraper_github_package,
    title = {Vitals Scraper source code on GitHub},
    url = {https://github.com/commercetest/vitals-scraper},
    author = {{Commercetest Limited}},
    journal = {GitHub repository},
    year = {2019},
    note = {Last checked on 2019-10-12}
}

@comment{Thanks to https://tex.stackexchange.com/questions/500642/urldate-not-being-considered/500665#500665 for the note field}

@online{vitals_scraper_npm_package,
    title = {Vitals Scraper npm package},
    author = {{Commercetest Limited.}},
    year = {2019},
    url = {https://www.npmjs.com/package/vitals-scraper},
    note = {Last checked on 2019-10-12}
}


@online{google_play_developer_distribution_agreement,
    title = {Google Play Developer Distribution Agreement},
    url = {https://play.google.com/intl/ALL_at/about/developer-distribution-agreement.html},
    author = {{Google Inc.}},
    organization ={{Google Inc.}},
    urldate = {11-Oct-2019},
    date = {15-Apr-2019},
    year = {2019},
}

@online{google_play_download_and_export_monthly_reports,
  title = {Download \& export monthly reports},
  url = {https://support.google.com/googleplay/android-developer/answer/6135870},
  author = {{Google Inc.}},
  organization = {{Google}},
  year = {2020},
}

@online{google_play_view_crashes_and_ANR_errors,
  title = {View crashes \& application not responding (ANR) errors},
  url = {https://support.google.com/googleplay/android-developer/answer/6083203#crashes_previous},
  author = {{Google Inc.}},
  organization = {{Google}},
  year = {2020},
}

@online{google_play_share_usage_and_diagnostics_info_with_google,
  title = {Share usage \& diagnostics information with Google},
  url = {https://support.google.com/accounts/answer/6078260},
  author = {{Google Inc.}},
  organization = {{Google}},
  year = {2020},
  quote_1 = {Battery level, How often you use your apps, Quality and length of your network connections (like mobile, Wi-Fi, and Bluetooth)},
  quote_2 = {Turn usage & diagnostics on or off. Important: If you turn off usage and diagnostics, your device can still get essential services, like a new version of Android. Turning off usage and diagnostics won’t affect info that apps might collect.
  
  To choose whether to send usage and diagnostics info to Google:
  1. Open your device's Settings app.
  2. Tap Google And then More More And then Usage & diagnostics.
  3. Turn Usage & diagnostics on or off.
  Tip: If you use a shared device, other user profiles may change this setting.},
  quote_3 = {For example, Google can use usage and diagnostics info to improve: 
  - Battery life: Google can use info about what's using the most battery on your device to help make common features use less battery.
  - Crashing or freezing on devices: Google can use info about when apps crash and freeze on your device to help make the Android operating system more reliable.
  Some aggregated info can help partners, like Android developers, make their apps and products better, too.},
}

@online{nii_shonan_workshop_152,
   title = {Release Engineering for Mobile Applications},
   maintitle = {NII Shonan Meeting 152},
   address = {Shonan Village Center, Japan},
   year = {2019},
   month = {December},
   author = {Shane McIntosh and Yasutaka Kamei and Meiyappan Nagappan},
   url = {https://shonan.nii.ac.jp/seminars/152/},
   organization = {{NII Shonan}},
}

@online{nii_shonan_152_workshop_report,
   title = {Release Engineering for Mobile Applications},
   maintitle = {NII Shonan Meeting Report 152},
   address = {Shonan Village Center, Japan},
   year = {2019},
   month = {December},
   author = {Shane McIntosh and Yasutaka Kamei and Meiyappan Nagappan},
   url = {https://shonan.nii.ac.jp/docs/No.152.pdf},
   organization = {{NII Shonan}},
}

@online{guardiannewspaper_right_to_be_forgotten_articles,
  title = {The Guardian newspaper - right to be forgotten articles},
  url = {https://www.theguardian.com/technology/right-to-be-forgotten},
  author = {{Guardian Newspaper: Various authors}},
  year = {2020},
  publisher = {{Guardian News \& Media Limited}},
  organization = {{The Guardian Newspaper}},
  quote = {About 129 results for Right to be forgotten},
}

@online{using_puppeteer_to_automate_your_google_analytics_testing,
  title = {Using Puppeteer to automate your {Google Analytics} testing},
  url = {https://www.dumkydewilde.nl/2020/04/using-puppeteer-to-automate-your-google-analytics-testing/},
  author = {Dumky de Wilde},
  organization = {Beyond measure},
  year = {2020},
  month = {April},
  urldate = {2020-08-18}
}

@online{using_the_itly_cli_verify_the_instrumentation,
  title = {{Using the Itly CLI}},
  organization = {Iteratively},
  key = {iteratively},
  url = {https://iterative.ly/docs/using-the-itly-cli#step-5-verify-the-instrumentation},
  urldate = {2020-08-19},
  year = {2020},
  abstract = {Itly is Iteratively’s command line app. It works hand-in-hand with the Iteratively web app and enables developers to quickly and correctly instrument tracking code in their apps.},
  quote_1 = {Step 5: Verify the instrumentation#
  To make sure you’re tracking all the right events, and that you’re tracking those events correctly, Itly can lint your source code and warn you about any errors. For example, Itly can tell if you’ve forgotten to track any required events, or if you’re not passing along all required properties.
  The verify command will scan your source code for tracking calls and compare the results to what's expected per your team's tracking plan. Include --update to update your company's tracking plan online and share the latest analytics implementation status with your team. If the command reports all green, you're all good!

  You can configure your CI pipeline to automatically run the verify command at check-in so you never miss another analytics bug again.},
}

@online{using_the_itly_cli_itly_verify,
  title = {itly verify | {Using the itly CLI}},
  organization = {Iteratively},
  key = {iteratively},
  url = {https://iterative.ly/docs/using-the-itly-cli#itly-verify},
  urldate = {2020-08-19},
  year = {2020},
  quote_1 = {itly verify
    Verify (lint) your source code for analytics.
    
    Run this command in the root folder of your project. The command will scan your source files, locate all calls to the Itly tracking library, and let you know which events are being tracked, and which have yet to be instrumented.

    Include --update to update your company's tracking plan online and share the latest analytics implementation status with your team. Your teammates will be able to tell when events were first implemented, the last time they've been detected in the source code, and where exactly in the source code they are tracked.
    }
}

@mastersthesis{nilsson2016_a_recipe_for_responsiveness_for_improving_android_apps_spotify_masters,
  title={A Recipe for Responsiveness: Strategies for Improving Performance in Android Applications},
  author={Nilsson, Elin},
  year={2016},
  school = {Umeå University},
  abstract = {Mobile applications are expected to be fast and responsive to user interaction, despite challenges mobile platforms and devices face in terms of limited computational power, battery, memory, etc. Ensuring that applications are performant is however not trivial, as performance bugs are difficult to detect, fix, and verify. In order for mobile applications and devices to appear perfectly responsive to the user, they need to meet a 60 frames per second frame rate, and keep load times preferably between 0-300 ms. Meeting these expectations means that there is no room for performance bugs, so there is a need for improving and developing better testing tools and strategies in order to help mobile developers improve performance in their applications.

  This thesis investigates strategies for testing and improving performance in Android applications by conducting a literary study, and a case study with the Spotify Android application. Some of the key findings of this thesis include promising results from tools that visualise sources of performance bugs in the user interface of applications, as well as proposed strategies and tools aimed to help developers profile and improve performance in their Android applications.},
  url = {http://urn.kb.se/resolve?urn=urn:nbn:se:umu:diva-125772},
}

@phdthesis{adam2009balancing,
  title={Balancing privacy needs with location sharing in mobile computing},
  author={Adam, Karim Anthony},
  year={2009},
  school={The Open University},
  doi = {10.21954/ou.ro.00004b35},
  oro = {http://oro.open.ac.uk/19253/},
}

@phdthesis{awwad2017_automated_bidi_testing,
  title={Automated Bidirectional Languages Localization Testing for Android Apps Development},
  author={Aiman Mamdouh Ahmad Ayyal Awwad},
  school={{TU Graz}},
  year={2017},
}

@phdthesis{al2019software_engineering_in_the_age_of_app_stores,
  title={Software Engineering in the Age of App Stores: Feature-Based Analyses to Guide Mobile Software Engineers},
  author={Al-Subaihin, Afnan A},
  year={2019},
  school={UCL (University College London)}
}

@phdthesis{didar2018data_analytics_phd_thesis,
  title={Data analytics for decision support in software release management},
  author={Didar Al Alam, SM},
  year={2018},
  publisher={Graduate Studies},
  type = {{Ph.D.} dissertation},
  school={University of Calgary},
  doi={10.11575/PRISM/31856},
  relevance_to_phd={Plan Monitor Improve Decision Making Framework. BTW: supervised by Guenther Ruhe.},
  journal = {" "},
}

@phdthesis{kong2021_taming_android_app_crashes,
  title={Taming Android App Crashes},
  author={Kong, Pingfan},
  year={2021},
  school={University of Luxembourg,​ Luxembourg City, Luxembourg},
  type = {{Ph.D.} dissertation},
  url = {http://hdl.handle.net/10993/46741},
  download = {https://orbilu.uni.lu/bitstream/10993/46741/1/Dissertation_Pingfan_KONG_Uni_Luxembourg.pdf},
  abstract = {
  App crashes constitute an important deterrence for app adoption in the android ecosystem. Yet, Android app developers are challenged by the limitation of test automation tools to ensure that released apps are free from crashes. In recent years, researchers have proposed various automation approaches in the literature. Unfortunately, the practical value of these approaches have not yet been confirmed by practitioner adoption. Furthermore, existing approaches target a variety of test needs which are relevant to different sets of problems, without being specific to app crashes.
  
  Resolving app crashes implies a chain of actions starting with their reproduction, followed by the associated fault localization, before any repair can be attempted. Each action however, is challenged by the specificity of Android. In particular, some specific mechanisms (e.g., callback methods, multiple entry points, etc.) of Android apps require Android-tailored crash-inducing bug locators. Therefore, to tame Android app crashes, practitioners are in need of automation tools that are adapted to the challenges that they pose. In this respect, a number of building blocks must be designed to deliver a comprehensive toolbox.
  
  First, the community lacks well-defined, large-scale datasets of real-world app crashes that are reproducible to enable the inference of valuable insights, and facilitate experimental validations of literature approaches. Second, although bug localization from crash information is relatively mature in the realm of Java, state-of-the-art techniques are generally ineffective for Android apps due to the specificity of the Android system. Third, given the recurrence of crashes and the substantial burden that they incur for practitioners to resolve them, there is a need for methods and techniques to accelerate fixing, for example, towards implementing Automated Program Repair (APR).
  
  Finally, the above chain of actions is for curative purposes. Indeed, this "reproduction, localization, and repair" chain aims at correcting bugs in released apps. Preventive approaches, i.e., approaches that help developers to reduce the likelihood of releasing crashing apps, are still absent. In the Android ecosystem, developers are challenged by the lack of detailed documentation about the complex Android framework API they use to develop their apps. For example, developers need support for precisely identifying which exceptions may be triggered by APIs. Such support can further alleviate the challenge related to the fact that the condition under which APIs are triggered are often not documented.
  
  In this context, the present dissertation aims to tame Android crashes by contributing to the following four building blocks:
  
  Systematic Literature Review on automated app testing approaches:
  We aim at providing a clear overview of the state-of-the-art works around the topic of Android app testing, in an attempt to highlight the main trends, pinpoint the main methodologies applied and enumerate the challenges faced by the Android testing approaches as well as the directions where the community effort is still needed. To this end, we conduct a Systematic Literature Review (SLR) during which we eventually identified 103 relevant research papers published in leading conferences and journals until 2016. Our thorough examination of the relevant literature has led to several findings and highlighted the challenges that Android testing researchers should strive to address in the future. After that, we further propose a few concrete research directions where testing approaches are needed to solve recurrent issues in app updates, continuous increases of app sizes, as well as the Android ecosystem fragmentation.
  
  Locating Android app crash-inducing bugs:
  We perform an empirical study on 500 framework-specific crashes from an open benchmark. This study reveals that 37 percent of the crash types are related to bugs that are outside the crash stack traces. Moreover, Android programs are a mixture of code and extra-code artifacts such as the Manifest file. The fact that any artifact can lead to failures in the app execution creates the need to position the localization target beyond the code realm. We propose ANCHOR, a two-phase suspicious bug location suggestion tool. ANCHOR specializes in finding crash-inducing bugs outside the stack trace. ANCHOR is lightweight and source code independent since it only requires the crash message and the apk file to locate the fault. Experimental results, collected via cross-validation and in-the-wild dataset evaluation, show that ANCHOR is effective in locating Android framework-specific crashing faults.
  
  Mining Android app crash fix templates:
  We propose a scalable approach, CraftDroid, to mine crash fixes by leveraging a set of 28 thousand carefully reconstructed app lineages from app markets, without the need for the app source code or issue reports. We develop a replicative testing approach that locates fixes among app versions which output different runtime logs with the exact same test inputs. Overall, we have mined 104 relevant crash fixes, further abstracted 17 fine-grained fix templates that are demonstrated to be effective for patching crashed apks. Finally, we release ReCBench, a benchmark consisting of 200 crashed apks and the crash replication scripts, which the community can explore for evaluating generated crash-inducing bug patches.
  
  Documenting framework APIs' unchecked exceptions:
  We propose Afuera, an automated tool that profiles Android framework APIs and provides information on when they can potentially trigger unchecked exceptions. Afuera relies on a static-analysis approach and a dedicated algorithm to examine the entire Android framework. With Afuera, we confirmed that 26739 unique unchecked exception instances may be triggered by invoking 5467 (24\%) Android framework APIs. Afuera further analyzes the Android framework to inform about which parameter(s) of an API method can potentially be the cause of the triggering of an unchecked exception. To that end, Afuera relies on fully automated instrumentation and taint analysis techniques. Afuera is run to analyze 50 randomly sampled APIs to demonstrate its effectiveness.Evaluation results suggest that Afuera has perfect true positive rate. However, Afuera is affected by false negatives due to the limitation of state-of-the-art taint analysis techniques.
  }
}


@phdthesis{oliver2018_first_steps_in_retrofitting_a_versatile_sw_testing_architecture,
  title={First Steps in Retrofitting a Versatile Software Testing Infrastructure to Android},
  author={Oliver, Carol Anne},
  year={2018},
  school = {Florida Institute of Technology},
  address = {Melbourne, Florida, USA},
}


@phdthesis{shuba2019mobile,
  title={Mobile Data Transparency and Control},
  author={Shuba, Anastasia},
  year={2019},
  school={UC Irvine},
  relevance_to_phd={The research studies information mobile apps collect and send over the network, often for advertising, also for analytics. The author developed various tools (AntMonitor, AntShield, NoMoAds, and AutoLabel) to help users see what was happening and to block unwanted communications. The tools can block analytics data being sent, which would affect our approach by reducing the number of users the data is collected from. They discuss related works that use static analysis to identify third-party data collection libraries.}
}

@phdthesis{stanik2020requirements_intelligence_on_the_analysis_of_user_feedback,
  title={Requirements Intelligence: On the Analysis of User Feedback},
  author={Stanik, Christoph},
  year={2020},
  school = {Universität Hamburg},
  relevance_to_phd = {continuous sources for requirements-related information; comparison between explicit and implicit user feedback (like app usage data)},
  abstract = {
    Traditionally, software requirements engineering involved users through workshops, interviews, and observations in the early software development phases. Although beneﬁcial to software teams, these approaches are challenging to carry out continuously and can involve only a limited number of users. In recent years, requirements stakeholders started analyzing explicit user feedback, such as written app reviews, and implicit user feedback like app usage data as continuous sources for requirements-related information. Yet, research highlights that stakeholders rarely use explicit and implicit user feedback in their decision-making process because they receive it in large and unﬁltered amounts, making a manual analysis unfeasible. As user satisfaction is crucial for the success of an app, stakeholders need automated approaches for analyzing user feedback to understand user needs and to guide their decision-making. In an interview study, we found that stakeholders need to know how their apps perform, to eﬃciently identify innovative features, and to understand reported issues and bugs.

    This dissertation introduces requirements intelligence, a framework that continuously collects, preprocesses, ﬁlters, as well as transforms and matches explicit and implicit user feedback to requirements. The framework aims to generate insights for stakeholders in an integrated interactive visualization. The core enablers for requirements intelligence include two main analysis activities on explicit and implicit feedback: Feedback ﬁltering and feedback to requirements analysis. Feedback ﬁltering is the activity that identiﬁes requirements-relevant feedback, such as problem reports, inquiries, and feature requests. Feedback to requirements extracts the software features users discuss and matches them with the features as documented on, e.g., app pages. We developed and empirically evaluated supervised machine learning approaches for both feedback types and activities. Our approaches rely on crowdsourcing studies for training machine learning models and on benchmarking experiments for identifying the optimal machine learning models.

    Based on our requirements intelligence framework, we iteratively developed the prototype feed.ai. We evaluated feed.ai with a total of 15 stakeholders from a major telecommunication company for 12 months. We found that the stakeholders agreed with 92\% of the automated ﬁltering results indicating high accuracy. The stakeholders found requirements intelligence beneﬁcial for departments working with user feedback like customer care, marketing, and technology innovation. In a ﬁnal survey, ten stakeholders anonymously rated feed.ai’s functionality, on average, with 4.1/5 and its usability with 4.3/5. They further reported that feed.ai helped them to reduce 70\% of their time spent on analyzing user feedback, indicating a high eﬀectiveness of our approach.},
}

% Thanks to https://tex.stackexchange.com/questions/203901/how-to-cite-iso-or-british-standards-in-latex-bibtex
% and to https://tex.stackexchange.com/questions/54441/bib-sorting-without-author
@techreport{BS_7925_1_1998,
  type = {Standard},
  key = {BS 7925-1:1998},
  author = {Various},
  year = {1998},
  title = {Software testing. Vocabulary},
  volume = {1998},
  address = {London, UK},
  institution = {British Standards Institution},
}

@techreport{RFC3164,
  author = {C. Lonvick},
  title = {The BSD Syslog Protocol},
  howpublished = {Internet Requests for Comments},
  type = {RFC},
  number = {3164},
  year = {2001},
  month = {August},
  issn = {2070-1721},
  publisher = {RFC Editor},
  institution = {RFC Editor},
  shorthand = {RFC3164},
}

@techreport{iso25010-2011-en,
  type = {standard},
  title = {ISO/IEC 25010:2011(en)
Systems and software engineering — Systems and software Quality Requirements and Evaluation (SQuaRE) — System and software quality models},
  author = {Technical Committee : ISO/IEC JTC 1/SC 7 Software and systems engineering},
  pages = {34},
  year = {2011},
  institution = {ISO},
}

@techreport{iso29119-1-2013,
  type = {standard},
  title={ISO/IEC/IEEE 29119-1:2013(E): Software and Systems Engineering Software Testing Part 1:Concepts and Definitions},
  author={ISO/IEC/IEEE 29119-1:2013(E)},
  institution={International Organization for Standardization and International Electrotechnical Commission and Institute of Electrical and Electronics Engineers and IEEE-SA Standards Board},
  isbn={9780738185972},
  series={IEEE Std},
  year={2013},
  address={New York, USA},
  publisher={IEEE}
}

@techreport{dimensionalresearch2015_mobile_app_use_and_abandonment,
  title = {Mobile app Use and Abandonment Global Survey of Mobile app Users},
  year = {2015},
  publisher = {{HP}},
  author = {{dimensional research}},
  institution = {{Dimensional Research}},
  executive_summary = {
    Users are reaching for mobile devices numerous times everyday to use mobile apps. These users shared that peer reviews and ratings significantly affect their app choices. App stability and performance heavily define the user experience and overall satisfaction. Yet this study finds that users are experiencing severe app performance issues regularly. Importantly, apps that exhibit these issues are quickly abandoned after just a few occurrences.
    For companies who create mobile apps, while good performance can lead to app downloads and satisfied users, poor performance will result in quick app abandonment and brand tarnishment.
  },
  participants = {A total of 3011 participants that use mobile apps completed the global survey including those in Belgium, Canada, France, Germany, Netherlands, United Kingdom, and United States.},
  methodology = {Mobile devices users in North America and Europe were invited to participate in a survey on the topic of mobile apps. The survey was administered electronically and participants were offered a token compensation for their participation.},
  research_goal = {The primary research goal was to capture hard data on the key factors that determine end user satisfaction with mobile apps. In addition, research sought to determine what users did when they were unsatisfied with a mobile app.},
}

@techreport{gartner2015_market_guide_for_mobile_app_analytics,
  title = {Market Guide for Mobile App Analytics},
  author = {Wong, J.},
  institution = {{Gartner Inc.}},
  year = {2015},
  url = {https://www.gartner.com/doc/3130520?ref=SiteSearch&sthkw=mobile\%20app\%20analytics&fnl=search&srcId=1-3478922254},
}