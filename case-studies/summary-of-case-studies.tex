\section{Summary of Case Studies}
MUST-DO revise and update this summary close to submitting the thesis - when the main canon of the work is closed; i.e. when there are no new case studies likely to be added.

The case studies includes a useful range of Android apps developed by independent teams using a variety of programming languages, mindsets, objectives, and constraints. In each team they learned to actively focus on stability metrics as reported in various technology-facing analytics tools, and the developers continue to see the merit of doing so on an ongoing basis.

Developers want to improve their software however their ongoing use of usage analytics ranges from infrequent to integrating it in their working practices. Moonpig and Kiwix Android are two projects where they actively engage with mobile analytics. They have chosen different approaches, Moonpig incorporated in-app mobile analytics into their app and use the platform level analytics as a secondary source of information. Kiwix Android relies on the platform level analytics. Of these Moonpig is able to address issues more quickly and target failures accurately through the additional data they receive from Firebase Analytics. 

Testability is a cross-cutting concern which applies throughout the development and operational aspects of mobile app development. Examples include the ease (or otherwise) of being able to reproduce reported issues, testability of the analytics libraries, tools, and services, and testability of changes to the app pre-release to try and ascertain whether the stability of the app will improve once it has been launched and deployed to a large portion of the userbase.

Integration and streamlining become increasingly important as the userbase and teams grow. The case studies alone are insufficient to provide quantitative trends, nonetheless they align with my experience across multiple large organisations in the USA, Europe, and beyond. The ad-hoc survey by Orosz provides examples of tens of organisations who have development teams of at least 20+, 50+, and even 100+ developers on the team for an app.


Many of these large teams will have apps that include crash and in-app analytics, and they are likely to have multiple people accessing and using analytics reports. They may include multiple people contributing code related to using the respective crash and in-app analytics APIs which may lead to gaps and inconsistencies in the application of these tools unless the entire team works hard to maintain consistency in their practices. Tools, such as those provided by Iterative.ly (a tool vendor introduced in one of the case studies in this thesis) aim to help provide coherence, clarity and consistence in the use of mobile analytics. Software development style guides, software quality utilities, and productive code reviews, may also help to establish, maintain and even improve the practices. In the opensource projects evaluated in one of the case studies \textbf{MUST-DO} write that up and extend this section... the developers were found to update their use of mobile analytics for logging less frequently than other aspects of their code.

% Also, COULD-DO For the 107 Android apps we reviewed post Shonan #152, are there any patterns between the use of Firebase Analytics and the active developer userbase? i.e. how many significant contributors does each of these projects have?