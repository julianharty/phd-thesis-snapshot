\section{Summary of Case Studies}
MUST-DO revise and update this summary close to submitting the thesis - when the main canon of the work is closed; i.e. when there are no new case studies likely to be added.

Summary of bugs
Bugs will reach production. They vary on their visibility, effects, and ease of reproduction. Some may be latent for long periods. Their effects may also vary, with some affecting a small number of users, others may have a more widespread effect across a population. Some will be destructive to the end user's experiences while others may be virtually unnoticeable. 





The case studies includes a useful range of Android apps developed by independent teams using a variety of programming languages, mindsets, objectives, and constraints. In each team they learned to actively focus on stability metrics as reported in various technology-facing analytics tools, and the developers continue to see the merit of doing so on an ongoing basis.

Developers want to improve their software however their ongoing use of usage analytics ranges from infrequent to integrating it in their working practices. Moonpig and Kiwix Android are two projects where they actively engage with mobile analytics. They have chosen different approaches, Moonpig incorporated in-app mobile analytics into their app and use the platform level analytics as a secondary source of information. Kiwix Android relies on the platform level analytics. Of these Moonpig is able to address issues more quickly and target failures accurately through the additional data they receive from Firebase Analytics. 

Ease-of-use and richness of the reports are key factors and led several of the teams to choose reports from App Center, Fabric, and Firebase over Google Play Console.

Being able to actively manage and track failures across and between systems is also an important factor for at least some of the teams. As examples, the Greentech team actively cross-reference crashes with their ticketing system, as do both the commercial teams, one going so far as to automatically create tickets for newly discovered crashes in their apps.


Testability is a cross-cutting concern which applies throughout the development and operational aspects of mobile app development. Examples include the ease (or otherwise) of being able to reproduce reported issues, testability of the analytics libraries, tools, and services, and testability of changes to the app pre-release to try and ascertain whether the stability of the app will improve once it has been launched and deployed to a large portion of the userbase.






Integration and streamlining become increasingly important as the userbase and teams grow. The case studies alone are insufficient to provide quantitative trends, nonetheless they align with my experience across multiple large organisations in the USA, Europe, and beyond. The ad-hoc survey by Orosz provides examples of tens of organisations who have development teams of at least 20+, 50+, and even 100+ developers on the team for an app.


Many of these large teams will have apps that include crash and in-app analytics, and they are likely to have multiple people accessing and using analytics reports. They may include multiple people contributing code related to using the respective crash and in-app analytics APIs which may lead to gaps and inconsistencies in the application of these tools unless the entire team works hard to maintain consistency in their practices. Tools, such as those provided by Iterative.ly (a tool vendor introduced in one of the case studies in this thesis) aim to help provide coherence, clarity and consistence in the use of mobile analytics. Software development style guides, software quality utilities, and productive code reviews, may also help to establish, maintain and even improve the practices. In the opensource projects evaluated in one of the case studies \textbf{MUST-DO} write that up and extend this section... the developers were found to update their use of mobile analytics for logging less frequently than other aspects of their code.


% Also, COULD-DO For the 107 Android apps we reviewed post Shonan #152, are there any patterns between the use of Firebase Analytics and the active developer userbase? i.e. how many significant contributors does each of these projects have?