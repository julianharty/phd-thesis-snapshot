\chapter{Applying analytics to development practices}
\label{chapter-applying-analytics-to-development-practices}

\akb{Helpful if chapter introduction sets context with respect to the research questions - which is what would have preceded this}
\def\yy#1{\color{blue}[YY: #1]\color{black}}

In the context of this thesis, analytics data is based on the software being used, so the software needs to be created and able to run before relevant data is generated. 
Much of the usage data comes from end users using mobile apps on their devices, however, it may also be available earlier in the software development lifecycle. In particular, various phases of testing of the app also uses the app and will therefore generate usage data unless this usage data is suppressed. 
%\yy{Do you need to justify that the thesis only considers Mobile Apps?}
% @Yijun I've done so in the introduction chapter. 
%\akb{Explicitly remind readers how analytics data can come from earlier in the development cycle}
% @Arosha, I've added a sentence that provides an explicit example. I've also got at least one sketch intended to illustrate when mobile analytics data is generated in the software development and usage phases. %SHOULD-DO incorporate the figure in this chapter if not earlier in the thesis.

\section{The rest of this chapter}
This chapter provides a holistic model pertaining to applying analytics to development practices, including \href{motivation-section}{\textit{motivation}}, \textit{selection}, and \textit{incorporation} of analytics tools; their \textit{use}; and the useful life of the materials provided by the analytics tools. 
\yy{The word "holistic" sets the expectation high: what are the parts of the whole? The word "model" is used but not clear what it is, do you mean "design" instead? The phrase "useful life of the materials" is also quite vague, maybe you mean "longitudinal analysis" instead?}

% There is some repetition in the chapter as none of the aspects are truly independent and they overlap; for instance, selection may include aspects of incorporation of analytics libraries in order to learn about them and evaluate them. 
% \yy{Remove this sentence: I don't think it is a good excuse to say there are overlaps in the chapter that meant to classify the difference kinds of analytics and applications. Try to avoid giving reader an impression this chapter is a brain dump. You need to give a well-thought description centered around the chapter content.}
% Thank you, I've done so by commenting this out. I'll remove it and our comments by the time I finish this round of editing, I'll keep it for now to try and make sure I address your points, and BTW the chapter started as a brain dump :)

There are also cross-cutting concerns including privacy aspects and evaluation criteria, these will be covered towards the end of this chapter.\yy{It is fine to introduce crosscutting concerns. I guess later you need to justify how they crosscut the functionalities, i.e., "design, uses, and analysis" of mobile analytics.}

\akb{By using 'including ..' before enumerating the four areas motivation, selection, incorporation and use, you are suggesting that there are other areas that are not covered here. Would be better to link these four areas to the mobile app software engineering context and convince the reader that these three areas cover everything that is relevant to understanding how analytics fit into the context.}

\akb{Also, is there a diagram that helps the reader understand how these four areas, engagement, selection, incorporation and use relate to each other? For example, intuitively I think there is a cyclic relationship where engagement influences selection, which in turn influences incorporation and use and that they in turn influence engagement.}

Related topics are covered in the discussion and future work chapters. Several examples will be included to root the research in the real world in addition to the theoretical underpinnings of the research. \yy{Is there a theoretical component in this chapter at all?}

\section{Motivation}~\label{motivation-section}
As mentioned in the introduction chapter, developers are motivated to understand how their software is being used, how it's behaving, and fix at least some of the flaws in their software. Their actual motivations and context will vary based on their context including their work priorities, their current and desired \href{subsection-levels-of-engagement}{\emph{levels of engagement}} in terms of incorporating and using analytics in their software engineering practices, privacy, data ownership and stewardship, costs, and so on.
\yy{Here "context" means everything? Also it does not make sense to say "context based on context". }

Some motivations may constrain, or limit, their options, for instance if they wish to be responsible the underlying data, or if they have budget constraints.

\subsection{Levels of engagement}~\label{subsection-levels-of-engagement}
Where development teams are aware of analytics they may choose their levels of engagement, or commitment, to using analytics in their software development practices. For those who are unaware, their level is equivalent to level zero (0) in the following enumerated list. 

Continuum of levels of engagement, or commitment, by developers:
\begin{enumerate}
    \setcounter{enumi}{-1} % unexpectedly this sets the first item in this list to zero.
    \item No analytics incorporated in the development process. If they exist, they're ignored.
    \item Passive analytics incorporated in the development practices, app does not contain any crash recording, remote logging, or other mobile analytics libraries.
    \item App incorporates one or more of the above mentioned libraries, initialises them where necessary but does not add any other additional calls to the libraries.
    \item App incorporates additional code to call one or standard methods using the APIs (\emph{etc.} if other mechanisms are available).
    \item App includes custom reporting where specific parameters are included in relevant API calls.
\end{enumerate}
\yy{By the 5 level system, you need to make sure they are in strictly total order, i.e., L4 > L3 > L2 > L1 > L0. I am not certain about L3 > L2.}

The development team have various choices available to them in terms of applying analytics to their software development practices. Their ~\href{subsection-levels-of-engagement}{\emph{levels of engagement}} range from not using analytics at all through to actively using an optimal mix of sources. 
Sources can include various forms of passive analytics to more hands-on techniques such as incorporating libraries and adding code to the app to report events, activities, and so on. 
Passive analytics can be combined with explicit analytics, one does not preclude the other.

% The next topic  incorporating analytics,
\subsection{Engagement practices}
An overview of applying analytics to development practices includes making various decisions. As many have observed, including the business guru Peter Drucker,~\emph{Not making a decision is a decision}, as summed up in the book by~\cite{gunther2013truth_about_better_decision_making}. % https://www.oreilly.com/library/view/the-truth-about/9780133445770/book1_ch31.html
Here are the key decisions:

\begin{itemize}
    \item Decide whether to use any existing, pre-provided analytics. This includes~\emph{passive analytics} (gathered without the developers needing to actively include analytics tools in their app). It may also include analytics provided as a side-effect of incorporating libraries into the app without the developers needing to add code to record additional information and/or explicit analytics already incorporated into the app for other purposes, for instance to gather marketing information. 
    \item Decide whether to incorporate additional analytics into the app, and if so what data to collect, which analytics library/libraries to incorporate and the many associated aspects we will cover in this chapter.
    \item Consider whether and how to test analytics and whether to filter [out] analytics during automated and internal testing.
    \item Decide to use and analyse the analytics data from the app and the platform (where available).
    \item The useful life of data and reports may exceed the default availability of the data in the analytics tool. Some data is transient, for instance a `daily' report, and unless preserved somehow, not available the following day.
    \item Triage and prioritise potential issues reported from external sources (pre-launch testing, new releases, active mainstream releases, etc.)
\end{itemize}
\yy{Are these decisions leading to various levels of engagement? Which one is the cause and which one is the effect? }

\akb{Can you elaborate on the relationship between these decisions? e.g., Do some outcomes of decision steps determine the next decision to be made? Would be helpful to think about a flow diagram for this decision process, highlighting the inputs needed for each decision point and the possible outcomes}

\textbf{TODO} wrap up this section and introduce the next batch.


\section{Selecting Mobile Analytics}
Selection includes choosing the analytics tools, the data to collect, the evaluation criteria for the analytics tools, and so on.
\begin{itemize}
    \item Establish the selection criteria \emph{e.g.} the intended goals and purposes of the data collection, compare with non-functional qualities, flexibility of the API, price, privacy, licensing, legal, and other selection criteria.
    \item Establish the acceptance criteria, including any design and implementation aspects.
    \item TBC...
\end{itemize}


\yy{A mapping table can be helpful to make it clear which decisions can be helped by which level of engagement. }
\begin{table}\begin{tabular}{|c|c|c|c|c|c|}\hline
Decision & L0 & L1 & L2 & L3 & L4 \\\hline
Using existing analytics &  &  &  &  & \\
Incorporating additional analytics & & & & & \\ 
Filtering analytics for testing & & & & & \\
Analysing analytics of app and platform & & & & & \\ 
Analysing life of data  & & & & & \\
Analysing External sources & & & & & \\\hline
Compare non-functional qualities & & & & & \\
Flexibility of API & & & & & \\
Price & & & & & \\
Privacy & & & & & \\
Licensing & & & & & \\
Legal & & & & & \\\hline
\end{tabular}\caption{MUST-DO: create Mapping table}\label{tab:my_label}
\end{table}

\section{Incorporation}
Incorporation of analytics code into an app (include and develop), incorporation of verification into the software construction, testing and release practices, incorporation of analytics results into the development and operational practices. c.f. Buse and Zimmermann's figure~\ref{fig:software_analytics_buse_and_zimmerman_2010}.
% \yy{You may need to include these diagrams here, otherwise reader may not be able to look up.}
% @Yijun, the Buse and Zimmerman figure is in the Related Works chapter, do you think it would help to reproduce it here too? (or to move it to here?). I've also moved the reference to Davenport et al.'s book to the related works chapter as that detail isn't relevant here.

Consider an aside on ways developers include a library in software, and activities libraries can perform actions independently of the app. 

Various practical aspects of incorporating mobile analytics are in an~\href{practical-aspects-appendix}{appendix} of this thesis.  


\subsection{Incorporating passive analytics to development practices}
Developers can integrate and incorporate the passive analytics %Google provides through the various reports 
in order to a) better understand how their app is doing, and b) change their app so it performs better as reported by these reports.

\akb{Need more explanation of the elements of 'incorporation' that you are going to cover - I need to understand how passive vs. active, designing analytics, testing analytics, etc fit together before the detail that follows.}

As mentioned earlier, passive analytics are those not actively under the control or influence of the development team, they are provided from other sources such as the operating system or the app store. Developers receive the reports and data they are given access to and therefore they need to their approach to suit the material that is provided if they wish to incorporate such analytics into their development practices.
\yy{According to your engagement types, "passive analytics" should be in "using existing analytics" instead of "incorporating additional analytics". Or maybe you are using the verbs "using" and "incorporating" to mean the same thing? Please make sure the classification has no overlaps, otherwise it is not a classification...}

In my research the passive analytics are all managed by an app store: Google Play Console. 
\yy{Why? Is it because of the definition, or because of convenience?}
This provides a rich mix of materials including passive analytics both pre and post launch. This section provides an overview of the reports as an example of well-established passive analytics, and later in this thesis, the section titled \href{google_play_console_section}{\emph{\nameref{google_play_console_section}}} provides examples of a variety of reports developers may receive on the performance of their Android app.

Here are the reports in the most likely chronological order of being generated if developers follow various recommendations made by Google, \emph{i.e.} to create and take advantage of test releases and use release management tools when rolling out a release of their Android app into production. Note: The reports are not guaranteed to be produced or be available, and the contents may expire after a period determined by the app store.

\begin{enumerate}
    \item \textbf{Pre-launch reports}: These include various static analysis checks and automated `robot' or `monkey' testing of apps. One of the key innovations is the automated matching of crashes reported in the field with those discovered during the automated testing, indicating the failure was found automatically and pre-launch. It could potentially have been addressed pre-release thereby restricting or even preventing the occurrence for end users.
    \item \textbf{Alpha and Beta channels}: optional mechanisms to support alpha and beta testing. Usage data is collected from the users' devices configured to do so, processed and reported on. 
    \item \textbf{Release Management}: Various analytics are collected for a period triggered by an app being released to end users. Releases may be constrained to a subset of users if developers wish. Comparisons with previous releases are available. Note this fits with existing research in release management by Shane Mcintosh and Guenther Ruhe, and others.
    \item (the app) \textbf{Dashboard}: A dashboard comprising various graphs is provided per released app.  
    \item \textbf{Android Vitals}: comprised various reports and filters intended to provide software quality related reports and data. 
    \item \textbf{User Feedback}: comprises various reports, filters, and classification of ratings and reviews. Reviews are translated into English automatically. 
\end{enumerate}

\subsection{Using passive analytics productively}

\textbf{TODO} add my notes based on the analysis of Google Play Console for Greentech apps.

\subsection{Adding and incorporating a crash-reporting library}
Crash-reporting libraries need to be incorporated into an application before they can be used, as mentioned in the section on~\href{section-packaging-mobile-apps}{\emph{packaging mobile apps}}. Generally~\footnote{A small minority of developers may follow other practices, nonetheless the principles mentioned here still apply}, the developer adds a few configuration lines to their application's build file (in \texttt{app/build.gradle} for Android apps) and also several lines of code to initialise the library when the application starts. These install the library as the global crash handler for the app, each time the app is started the library is initialised. 

When the library is initialised, it may perform various actions such as recording details of the operating system release, the model of device, \emph{etc.}. They may also perform house-keeping activities, for instance Crashlytics transmits crash reports from previous sessions.

Some crash-reporting libraries offer developers an API to add \emph{breadcrumbs} at run-time. If/when a crash occurs and is reported, the immediately preceding breadcrumb data may help developers piece together possible causes for a particular crash.

Some crash-reporting libraries offer developers a mechanism to report non-fatal crashes: caught exceptions. These would be handled by the application yet be considered noteworthy and worthy of analysis by the development team. A good example of a library that includes support for non-fatal crash reporting is the popular Crashlytics offering.  

\subsection{Testing crash-reporting}
\begin{itemize}
    \item \textbf{Sanity test}: as John Stuart Mill might do, a sanity test aims to establish a connection between a `fatal' crash occurring and the incident being reported in the respective analytics report, \emph{i.e.} applying the principles of concomitant variation (\cite{mill1884system}).
    \item \textbf{Latency}: there will be a delay between when the crash occurs and when it appears in an analytics report. There are several factors which affect the delay including delays in transmission, processing, and reporting. Testing the latency of the system helps to establish the latency of the overall end-to-end process and set the expectations of the development teams.
\end{itemize}

Some tools and/or reports may have thresholds which require sufficient volume of incidences (such as crashes) before they report them, again testing helps establish whether these thresholds exist in practice.

Testing a system intended to measure quality may adversely affect their rating of your apps and potentially even their willingness to accept you in their system. \emph{c.f.} credit checks may adversely affect your credit score score~\footnote{\url{https://www.experian.co.uk/consumer/guides/searches-and-credit-checks.html}}. The system may not distinguish between your testing of the measurement system and those experienced by end users of the software. Google is adamant they will not accept Android apps that crash: ~\emph{``\textbf{Broken Functionality} We don’t allow apps that crash, force close, freeze, or otherwise function abnormally."}~\cite{google_play_developer_policy_center}.


\subsection{Designing in-app analytics}
Designing in-app analytics has similarities to designing logging, for instance in determining when to send messages and what to include in those messages. For those who can customise~\footnote{Some developers work for organisations where they design and implement proprietary analytics tools, others may customise licensed source code, be it opensource or commercially licensed. Many commercial mobile analytics providers provide their client libraries as opensource projects, for example Segment~\url{https://github.com/segmentio/analytics-android}.} the client-side analytics library they may also be able to influence/design aspects of storing and transmitting the contents of the messages.

Design criteria include those that influence the reporting aspects, for instance the semantics of an operation. \textbf{MUST-DO} provide some examples from the joint research post Shonan.~\footnote{Note: aspects of this research have been presented and discussed with various people and organisations, in particular at a NII Shonan meeting in December 2019~\cite{nii_shonan_workshop_152} and recorded in~\cite{nii_shonan_152_workshop_report}. I am engaged in ongoing research into aspects of how mobile app developers use mobile analytics with a group of five researchers internationally. We are jointly writing at least one paper on this research.}
%
Semantic events e.g. `\texttt{itly.songUploaded(...)}' provides semantic information that `\texttt{mixpanel.track(...)}' lacks~\footnote{Example taken from~\url{https://iterative.ly/docs/migration-guide}}.

There is a wide range of scope in terms of the flexibility provided by various analytics tools. Some Analytics tools allow developers to create custom methods (which are named by the developers), others limit developers to using pre-defined methods where the contents of the payload can be customised, and others constrain both the methods and the payload.

\subsection{Testing in-app analytics}
Testing is one way to ascertain whether the analytics is working as intended. From personal experience in the industry testing may be haphazard or minimally done. The lack of testing in these cases has led to no end of downstream issues in terms of the trustworthiness and validation of the analytics. Here we consider several complementary approaches to testing in-app analytics with a focus on the verification aspects (aiming to answer: \emph{``does it work as intended?"}) questions.

Testing Steps:
\begin{enumerate}
    \item Build-time verification: concentrates on whether the analytics library and API calls have been integrated into the application correctly.
    \item Generation of the messages: focuses on whether the intended messages are generated in response to the intended triggers (events, and so on). Testing whether the necessary and appropriate content is in the relevant messages is also in scope. 
    \item Transmission of messages: given the assumption that data is transmitted from mobile devices to internet-based servers, the messages need to be transmitted in a timely manner. Testing the transmission is similar to a proof-of-posting test. 
    \item Arrival of the messages: this step focuses on the proof-of-delivery aspects - did the messages successfully arrive without problems (such as corruption, truncation, interception, spoofing, and so on) and did they arrive in order within the expected timescales, \emph{etc.}? 
    \item Processing of the messages: as analytics inherently involves analysis of the raw messages, testing of the processing aspects helps to determine whether the contents were interpreted correctly. 
    \item Testing the analysis and reporting: Analytics is little use without the resulting analysis and reports, therefore they also need to be tested they are correct. As relevantly they are also worth validating in terms of usefulness/utility. 
\end{enumerate}

\subsubsection{Build-time verification} 
In-app software analytics, in practice, involves incorporating a software component into the rest of the codebase for a mobile app. Typically the software component is a pre-built software library provided by same source as the analytics tool. The intended version of the library needs to be incorporated adequately and remain in the application binary created during the build process.

One of the companies who leading the industry in terms of build-time verification is \href{https://iterative.ly}{Iteratively} who currently provide tools, scripts, and services to check at build time whether a supported analytics library has been adequately incorporated in terms of calling each of the required API calls designed using their tools. They provide implementations for various platforms including Android and iOS~\footnote{e.g. Android Kotlin~\url{https://iterative.ly/docs/interacting-with-the-sdk\#android--kotlin} and iOS Swift~\url{https://iterative.ly/docs/interacting-with-the-sdk\#ios--swift}}. Their scripts can be run as part of a continuous build process and the build script configured to block code from being merged until all of the API calls have been called, they `\texttt{lint}' the source code of the app to perform the checks, ~\cite{using_the_itly_cli_verify_the_instrumentation, using_the_itly_cli_itly_verify}.

The approach, and the supporting tools and scripts as exemplified by Iteratively, demonstrate such verification is practical. To abstract away from the specifics of their offering, the approach is:
\begin{itemize}
    \item Establish and design the analytics messages.
    \item Generate a tracking library containing the appropriate API calls for one or more analytics providers (such as Firebase Analytics, Mixpanel, and so on).
    \item Modify the source code of the core application to call these API calls (connect, or bind, the app's code with the analytics code).
    \item Lint the resulting combined library and source code and determine if there are material issues with the implementation.
\end{itemize}

\subsubsection{Automated testing}
puppeteer~\cite{using_puppeteer_to_automate_your_google_analytics_testing}

\textbf{TODO} Cite research on looking at what analytics sends, discovered using network monitoring and instrumentation. 





\section{Use}
Peers, engineering process flows, DevOps, ...

Effective use of usage-derived analytics is intended to be mutually beneficial for both the development team and the quality of the app they produce. Data from operations (the use of the app) informs the development team on aspects of how the app is performing and provides feedback on the effects of the development and testing related to the releases of the app in current use. Failures can be used to identify flaws in the app, once these are addressed and the improved version of the app is available to users - the users may then receive an improved user experience. If developers choose to add instrumentation code to their app they may be able to glean additional data related to the use of the app that's germane to flaws and failures.

Examples of instrumentation code in the context of usage analytics include: breadcrumbs, and the recording and reporting of~\emph{`non-fatal exceptions'}. Non-fatal exceptions are exceptions that occur in the running software where the exception is handled within the app without the app crashing. Some crash reporting mechanisms, including passive analytics, rely on the crash emerging from the app when the app is terminated the non-fatal exceptions are not visible using passive analytics or global crash handlers (which 'handle' exceptions that have not been 'caught' or handled elsewhere in the app).



\subsection{Using failure clusters}~\label{section-select-aggregate-scope-analyse-triage-and-prioritise}
The proposed approach is intended to achieve practical results efficaciously and address real-world issues that \emph{`move the needle'} i.e. that will deliver positive improvements. 

A half-life of bugs worth addressing, where bugs that surface primarily in older releases of an app are less worthwhile to address as the users of these older releases will either upgrade to a newer release or remain on the older release even though improvements and fixes are available in newer releases. Also, at the time of writing, app stores do not support the equivalent of back-ports of fixes to older releases, they will only offer users the current release available for their device, etc. (Etc. as there may be regional and other restrictions which limit users to older releases. While developers could choose to create specific releases for particular groups of users (\emph{i.e.} the mechanisms exist in some app stores), there is little evidence that they do.)

Stages to consider when working with clusters of failures: 

\begin{itemize}
    \item \textbf{select}: where reports offer selection criteria - decide on a suitable selection to work with. Note there may be constraints in the analytics that limit what can be selected,
    \item \textbf{aggregate}: group together data with common factors, analytics tools do this already to varying degrees,
    \item \textbf{scope}: decide what is in and out of scope in terms of the selected and aggregated data, 
    \item \textbf{analyse}: for in-scope failures, analyse the patterns in the data aggregated together, there may also be correlations between distinct aggregations worth considering, 
    \item \textbf{triage}: categorise the clusters based on the analysis, those we want to prioritise, those unready to prioritise and those where the outcome will not vary sufficiently to address,
    \item \textbf{prioritise}: the first two of the triage categories: those ready to prioritise and those not yet ready - both can be worked on, the work differs. Select a useful subset which should be able to be addressed in the next development cycle (sprint, etc.).
\end{itemize}

The work in each stage should be sufficient and proportionate to the potential value of addressing that failure. These stages are expanded shortly, before I do, there are various timescales and latencies worth considering to help set the cadence and work-in-progress for applying this process and prioritising the work to address any of the failures.

Owing to the nature of mobile app stores and their user population's habits there are substantial and material latencies worth factoring into the decision making process. These latencies include:
\begin{itemize}
    \item \textbf{pre-release}: development teams and their software development practices provide a cadence and a timescale in terms of being able to create suitably evaluated release-candidates,
    \item \textbf{app store approvals}: where release-candidates require app store approvals, and where such approval is not instantaneous, there is a practical constraint on making releases available to users via the app store,
    \item \textbf{launch rollout}: release management tools, provided by an app store, may constrain the availability of a release to users via the app store,
    \item \textbf{user adoption of new releases}: users do not update apps instantaneously, indeed adoption may take a week or more to reach 50\% of the active user population (and much longer to reach 50\% of the inactive user population), 
    \item \textbf{users who remain on older releases}: some users stick with older releases of software - they will not receive the new release, yet their usage of the older releases may count adversely against the app.
\end{itemize}

\subsection{Select}
The analytics tools may provide selection criteria, for instance, the ability to select a range of dates is commonplace. App and operating system release selection criteria are also common, regional selections, hourly ranges, by manufacturer, by device models, etc. are less commonly available.

Note: the selection criteria may not be as flexible as desired and may be inconsistently available even within an analytics tool.

\subsection{Aggregate}
Dealing with large volumes of discrete failures is tedious. Grouping the failures by one or more common elements can help patterns and relevant characteristics to be detected. The nature of analytics tools is for them to aggregate individual data records, such as events, for reporting and comparisons. Aggregations performed by analytics tools may limit the scope for the development team to apply their own independent groupings, and similarly the analytics tools may not support some aggregations directly in the tool's reports. The development team may be able to aggregate data outside the analyics tool, for instance to group daily summaries into weekly summaries. 

\subsection{Scope}
\emph{Decide what is in and out of scope in terms of the selected and aggregated data.} Some of the failures may be out of scope in terms of analysis. For instance, those where a fix has been prepared for release, those in an external software component or on device models unavailable to the team, those that are in lower rankings, those we have the competence and where-with-all to investigate, and so on. These may be out-of-scope for the rest of the stages. c.f. 3 - 7 items can be held in short-term memory, limiting w-i-p - (\emph{Kanban} -, and other concepts intended to increase throughput.

c.f. a marketing funnel, focal depth (as in a camera), range, etc. 

In summary, disregard old bugs and cold bugs. Instead pick fertile current bugs those we have the energy and urgency to make a practical difference to in terms of the outcomes of our subsequent work.


\subsection{Analyse}
\textit{TODO expand this sub-section:}

Patterns and correlations across failure clusters... 

Trends, comparisons with previous periods, with peers, etc.

Historical analysis, transient nature of the data and reports, ...

\subsection{Triage}
Triaging the resulting analysis applies one of three categories to particular analysed failure clusters. These three categories are:
\begin{enumerate}
    \item Ready to action: these failure clusters have sufficient information and analysis to be ready to action when they have been prioritised sufficiently,
    \item Further work required: these failure clusters need, and are viable candidates for being actionable once additional work has been performed,
    \item Insufficient return-on-investment: these have marginal returns even if they were to be addressed, they may be impractical to address, and so on. Discount them.
\end{enumerate}

Of these categories, when further work is required sometimes the work may be viable within the current sprint, other instances may involve additional data collection in future.

\subsection{Prioritise}
The big, ongoing, and the ugly failures, together with a mix of easy-to-fix and satisfying bugs comprise the menu of what's likely to be addressed by the development team. 

\subsection{Next steps for prioritised ready to action failure clusters}

\begin{itemize}
    \item \textbf{source-code analysis}: for example, using stack traces from crash clusters,
    \item \textbf{bug investigation}: for example, to find out if the team can reproduce failures locally, predictably and consistently,
    \item \textbf{mitigation techniques}: where practical fixing the cause is generally the best option, other options may be available \textbf{MUST-DO} expand with materials from one of my previous papers.
    \item \textbf{evaluation of the mitigation techniques} pre- and post-release evaluation is important for various reasons including calibration, providing feedback to the teams, etc.
\end{itemize}




\section{The useful life of data and reports}
The are various cycles in app usage, including daily, weekly, and annual cycles. Some trends and patterns are more useful when compared against these cycles. 

As issues emerge it may be helpful to compare current and recent patterns with prior periods, to do so a similar richness of data for those periods facilitates the comparisons and analysis.

\subsection{Preserving material/evidence}
Here, material include what's provided by the analytics tools and evidence is where we've used, or applied, the material for a purpose such as reporting a bug based on reported failures.

For various reasons the analytics tools may limit access to prior material~\emph{e.g. Google removed access to crash reports in Google Play Console because of their policy changes.}, and glitches may mean the material is not available when needed even if it should be available. Therefore it may be useful and prudent for teams to preserve material and evidence independently of the analytics tool. 

\yy{Is this related to Business continuity and incident analysis?}

\section{Cross-cutting concerns}
\yy{First need to say what do they crosscut, and why crosscut?}
Privacy, peers, evaluation criteria (including testing, verification and validation).

\subsection{Privacy and Responsibilities for using passive analytics}
For passive analytics the developer does not actively choose what to collect or how it's collected, therefore they are constrained by whoever, or whatever if we discount the people involved in deciding what to collect, etc. and assume algorithms such as AI determine the data. Google, at least, is careful to only share non PII % MUST_DO expand and add PII to the Glossary.
data and with a few exceptions limits reports to populations that exceed thresholds determined by Google internally. %MUST_DO add reference to Google Help article(s).

Nonetheless, I recommend developers consider ethical and legal responsibilities if they discover that sensitive and other PII data is being collected through the passive analytics. This may include avoiding reports with such data in them and also reporting the concerns to upstream providers of analytics (and where appropriate internal and external legal authorities).

\subsection{Peers}
Who are the peers for my app?~\footnote{Inspired by:~\href{https://www.biblegateway.com/passage/?search=Matthew\%2012:46-50&version=NIV}{Matthew 12:46-50 New International Version}.} and how much can we glean of their app's performance?

Discuss options for peers, and in particular the facilities Google provides to enable developers to compare against their `peers'.



\section{Summary of applying analytics to development practices}
TODO TBC

\yy{I am very confused after reading the chapter. It doesn't tell me what development practices are, what analytics can do to the development practices (i.e., benefits and why considering analytics). There are applications, but I don't know whether these applications are holistic or not. Other unanswered question is, what is the theoretical underpinning to the chapter?}

\yy{Thinking about your readers, or beneficiaries, are they meant to be software developers? If so, I guess the first question they will have is, what can they learn from your applications of analytics, and how they can do as well (which includes how to incorporate them and how to analyse the reports.} 