\chapter{Applying analytics to development practices}~\label{chapter-applying-analytics-to-development-practices}
In the context of this thesis, analytics data is based on the software being used, so the software needs to be created and able to run before relevant data is generated.

Before going into details of applying analytics to development practices, I will introduce the basis for my theoretical perspective in terms of the ontology and epistemology.

\section{Ontology and Epistemology}
\begin{itemize}
    \item Ontology \( \rightarrow \) a theory of `being' (existence).
    \item Epistemology \( \rightarrow \) what we can know about the microcosm/the world and how we can know it (a theory of knowledge).
\end{itemize}

Both these terms are taken from~\cite{marsh2002skin}. While the article is aimed at social science research, it introduces both topics and their relationships clearly and practically~\footnote{Note: newer versions of the introductory material is published in a book: \href{https://www.macmillanihe.com/page/detail/Theory-and-Methods-in-Political-Science/?K=9781137603517}{\emph{``Theory and Methods in Political Science (\nth{4} Edition)".}}}.

\subsection{Ontology}
Analytics exist and are used across and throughout software development practices. Software is not perfect, it's formed through numerous human endeavours using flawed tools and techniques, no practical software is bug-free, however those involved can improve software through their choices and practices.

How to measure anything concepts...~\cite{hubbard}

\subsection{Epistemology}
What we can know about mobile analytics and how we can know it... Then more specifically, how we know about the identification and measurement of some flaws in behaviour of software that are considered measures of quality of the software in use. 

We can know through end-to-end testing, through asking the designers, constructors, operators, and users of a system (such as an analytics tool). However, we're also limited by who we can ask, what they are willing/able to communicate, and whether that communication is sufficiently open and transparent to be useful and reliable.


Various data can be potentially collected implicitly and explicitly. What can be collected depends on the observation mechanisms. Observation may be within an app or external to it, for instance by the operating system as Google Android does~\footnote{There are other custom versions of Android, for instance used in Amazon Kindle Fire devices.}. Within an app the observation may focus at a single layer, for instance the visual user interface, or several. The choices of observation mechanisms within an app are made by developers or their stakeholders. The choices external to an app can be made by various people including the platform provider, users, or indirectly using other software including third-party apps, spyware, accessibility software, and so on.


Analytics, such as user-journeys, can help to answer questions about the usage of the software. They help establish \emph{what-is}. As we understand more about what-is we can then consider \emph{what-would-be-better} and do gap analysis between what-is and what-would-be-better.

Reporting/generating and Observing... What happens within the app stays within the app unless someone looks inside the app or the app reports what's occurring. Observation without action limits the utility of whatever is learned. 


Toolbox of methods, how can we know about mobile analytics and the effects of using them?




\subsection{The rest of this chapter}
This chapter provides a holistic model pertaining to applying analytics to development practices, including \href{motivation-section}{\textit{motivation}}, \textit{selection}, and \textit{incorporation} of analytics tools; their \textit{use}; and the useful life of the materials provided by the analytics tools. There is some repetition in the chapter as none of the aspects are truly independent and they overlap; for instance, selection may will include aspects of incorporation of analytics libraries in order to learn about them and evaluate them. There are also cross-cutting concerns including privacy aspects and evaluation criteria, these will be covered towards the end of this chapter.

Related topics are covered in the discussion and future work chapters. Several examples will be included to root the research in the real world in addition to the theoretical underpinnings of the research.


\section{Motivation}~\label{motivation-section}
As mentioned in the introduction chapter, developers are motivated to understand how their software is being used, how it's behaving, and fix at least some of the flaws in their software. Their actual motivations and context will vary based on their context including their work priorities, their current and desired levels of engagement in terms of incorporating and using analytics in their software engineering practices, privacy, data ownership and stewardship, costs, and so on.

Some motivations may constrain, or limit, their options, for instance if they wish to be responsible the underlying data, or if they have budget constraints.

\subsection{Levels of engagement}~\label{subsection-levels-of-engagement}
Where development teams are aware of analytics they may choose their levels of engagement, or commitment, to using analytics in their software development practices. For those who are unaware, their level is equivalent to level zero (0) in the following enumerated list. 

Continuum of levels of engagement, or commitment, by developers:
\begin{enumerate}
    \setcounter{enumi}{-1} % unexpectedly this sets the first item in this list to zero.
    \item No analytics incorporated in the development process. If they exist, they're ignored.
    \item Passive analytics incorporated in the development practices, app does not contain any crash recording, remote logging, or other mobile analytics libraries.
    \item App incorporates one or more of the above mentioned libraries, initialises them where necessary but does not add any other additional calls to the libraries.
    \item App incorporates additional code to call one or standard methods using the APIs (\emph{etc.} if other mechanisms are available).
    \item App includes custom reporting where specific parameters are included in relevant API calls.
\end{enumerate}

The development team have various choices available to them in terms of applying analytics to their software development practices. Their ~\href{subsection-levels-of-engagement}{\emph{levels of engagement}} range from not using analytics at all through to actively using an optimal mix of sources. 
Sources can include various forms of passive analytics to more hands-on techniques such as incorporating libraries and adding code to the app to report events, activities, and so on. 
Passive analytics can be combined with explicit analytics, one does not preclude the other.

% The next topic  incorporating analytics,
\subsection{Engagement practices}
An overview of applying analytics to development practices includes making various decisions. As many have observed, including the business guru Peter Drucker,~\emph{Not making a decision is a decision}, as summed up in the book by~\cite{gunther2013truth_about_better_decision_making}. % https://www.oreilly.com/library/view/the-truth-about/9780133445770/book1_ch31.html
Here are the key decisions:

\begin{itemize}
    \item Decide whether to use any existing, pre-provided analytics. This includes~\emph{passive analytics} (gathered without the developers needing to actively include analytics tools in their app). It may also include analytics provided as a side-effect of incorporating libraries into the app without the developers needing to add code to record additional information and/or explicit analytics already incorporated into the app for other purposes, for instance to gather marketing information. 
    \item Decide whether to incorporate additional analytics into the app, and if so what data to collect, which analytics library/libraries to incorporate and the many associated aspects we will cover in this chapter.
    \item Consider whether and how to test analytics and whether to filter [out] analytics during automated and internal testing.
    \item Decide to use and analyse the analytics data from the app and the platform (where available).
    \item The useful life of data and reports may exceed the default availability of the data in the analytics tool. Some data is transient, for instance a `daily' report, and unless preserved somehow, not available the following day.
    \item Triage and prioritise potential issues reported from external sources (pre-launch testing, new releases, active mainstream releases, etc.)
\end{itemize}

\textbf{TODO} wrap up this section and introduce the next batch.


\section{Selecting Mobile Analytics}
Selection includes choosing the analytics tools, the data to collect, the evaluation criteria for the analytics tools, and so on.
\begin{itemize}
    \item Establish the selection criteria \emph{e.g.} the intended goals and purposes of the data collection, compare with non-functional qualities, flexibility of the API, price, privacy, licensing, legal, and other selection criteria.
    \item Establish the acceptance criteria, including any design and implementation aspects.
    \item TBC...
\end{itemize}




\section{Incorporation}
Incorporation of analytics code into an app, incorporation of verification into the software construction, testing and release practices, incorporation of analytics results into the development and operational practices. c.f. Buse and Zimmermann's figure~\ref{fig:software_analytics_buse_and_zimmerman_2010} based on Davenport and Harris's \emph{Key questions addressed by analytics} figure 1.1 (page 7 ~\cite{davenport2010analytics_at_work}). 

Where to log and what to log... Where to log has been researched by various authors. \cite{li2020_where_shall_we_log} identify six categories of logging locations in several mature opensource codebases, used in domains outside mobile apps. Research into where logging statements are added in large-scale industrial codebases are covered in various papers including:~\cite{zhu2015_learning_to_log} where a \emph{`Log Advisor'} made recommendations of where to log to developers, they excluded the contents of the log messages from their research and their log advisor as too difficult to address in the scope of their work at the time.



\subsection{Incorporating passive analytics to development practices}
As mentioned earlier, passive analytics are those not actively under the control or influence of the development team, they are provided from other sources such as the operating system or the app store. In my research the passive analytics are all managed by the app store, Google Play Console.

Later in this thesis, the section titled \href{google_play_console_section}{\emph{\nameref{google_play_console_section}}} provides examples of a variety of reports developers may receive on the performance of their Android app. Developers can integrate and incorporate the passive analytics Google provides through the various reports in order to a) better understand how their app is doing b) change their app so it performs better as reported by these reports.

Here are the reports in the most likely chronological order of being generated if developers follow various recommendations made by Google, \emph{i.e.} to create and take advantage of test releases and use release management tools when rolling out a release of their Android app into production. They are not guaranteed to be produced or be available, and the contents may expire after a period determined by the app store.

\begin{enumerate}
    \item \textbf{Pre-launch reports}: 
    \item \textbf{Alpha and Beta channels}:
    \item \textbf{Release Management}: Note this fits with existing research in release management by Shane Mcintosh and Guenther Ruhe, and others.
    
    \item (the app) \textbf{Dashboard, including User Feedback}:
    \item \textbf{Android Vitals}:
\end{enumerate}

\subsection{Using passive analytics productively}

\textbf{TODO} add my notes based on the analysis of Google Play Console for Greentech apps.

\subsection{Adding and incorporating a crash-reporting library}
Crash-reporting libraries need to be incorporated into an application before they can be used, as mentioned in the section on~\href{section-packaging-mobile-apps}{\emph{packaging mobile apps}}. Generally~\footnote{A small minority of developers may follow other practices, nonetheless the principles mentioned here still apply}, the developer adds a few configuration lines to their application's build file (in \texttt{app/build.gradle} for Android apps) and also several lines of code to initialise the library when the application starts. These install the library as the global crash handler for the app, each time the app is started the library is initialised. 

When the library is initialised, it may perform various actions such recording details of the operating system release, the model of device, \emph{etc.}. They may also perform house-keeping activities, for instance Crashlytics transmits crash reports from previous sessions.

Some crash-reporting libraries offer developers an API to add \emph{breadcrumbs} at run-time. If/when a crash occurs and is reported, the immediately preceding breadcrumb data may help developers piece together possible causes for a particular crash.

Some crash-reporting libraries offer developers a mechanism to report non-fatal crashes: caught exceptions. These would be handled by the application yet be considered noteworthy and worthy of analysis by the development team. A good example of a library that includes support for non-fatal crash reporting is the popular Crashlytics offering.  

\subsection{Testing crash-reporting}
\begin{itemize}
    \item Sanity test
    \item Latency
\end{itemize}

Testing a system intended to measure quality may adversely affect their rating of your apps and potentially even their willingness to accept you in their system. \emph{c.f.} credit checks may adversely affect your credit score score~\footnote{\url{https://www.experian.co.uk/consumer/guides/searches-and-credit-checks.html}}. The system may not distinguish between your testing of the measurement system and those experienced by end users of the software. Google is adamant they will not accept Android apps that crash: ~\emph{``\textbf{Broken Functionality} We donâ€™t allow apps that crash, force close, freeze, or otherwise function abnormally."}~\cite{google_play_developer_policy_center}.


\subsection{Designing the content/messages} 
% I'm not sure whether content or messages, or a mix of both words, best encompasses the topic I wish to discuss here. Messages can have content, however sometimes a message is a message by its existence, even with no payload. (2 rings on the home phone when you arrive, told the family you'd arrived without needing to pay for the telephone call. Heartbeat messages in systems, etc.). Also the design may include non-content aspects, content transformations, etc. Anyway, let's get writing. 
This section applies to messages that an app could emit regardless of the conduit (\emph{i.e.} it applies to logging and using mobile analytics). At the risk of some ambiguity, the term log will be used to reflect both logging and mobile analytics in this section to improve overall readability.

\emph{Related concepts}: The uneven U, computer protocols (layers, formatting, and contents), structured messages, what to log. %MUST_DO expand this section.

There are many choices that can be considered in terms of designing the content/messages. For various reasons developers may pay little strategic attention to logging in their daily work. For those who do choose to consider logging strategically there are various considerations, including:
What to log, how to log, where to log, data transformations, delivery mechanisms and characteristics.

Developers have control over what to log, how, and when to generate the log messages. They may be constrained in various ways by APIs, message lengths, encoding, and formats, access to messages, and when messages will be transmitted, \emph{etc.} 

It is possible to test the constraints, for instance by writing custom automated tests and/or apps that generate a variety of messages where the outputs are checked somehow. The checking may be partly or completely performed programmatically (we did some unpublished research in this area in 2018).

The purpose of the message may differ in the type and level of information it is intended to convey. Some messages may contain low-level, or detailed, error messages intended to help improve the technical aspects of the software to make the software more robust. Other messages may aim to communicate intent, completion of a task, activity or user-journey in the software. For example, IBM published a paper about software called CX Mobile that aims to record and visualise user journeys for iOS and Android apps~\cite{hu_tealeaf_cxmobile}.

\subsubsection{Designing logging}
Unstructured logging can serve immediate needs, for instance to trace code execution or display the value of a variable at run-time. The resulting entries into a log file have limited value in terms of longer term analysis and they may also be harder to identify, filter, and lack relevant content for such analysis.

In the domain of logging both business and research consider logging design important and valuable. 

Implementation choices: 

\subsubsection{Testing logging}



\subsubsection{Designing in-app analytics}

Semantic events e.g. `\texttt{itly.songUploaded(...)}' provides semantic information that `\texttt{mixpanel.track(...)}' lacks~\footnote{Example taken from~\url{https://iterative.ly/docs/migration-guide}}.

\subsubsection{Testing in-app analytics}
Testing is one way to ascertain whether the analytics is working as intended. From personal experience in the industry testing may be haphazard or minimally done. The lack of testing in these cases has led to no end of downstream issues in terms of the trustworthiness and validation of the analytics. Here we consider several complementary approaches to testing in-app analytics with a focus on the verification aspects (aiming to answer: \emph{``does it work as intended?"}) questions.

Testing Steps:
\begin{enumerate}
    \item Build-time verification: concentrates on whether the analytics library and API calls have been integrated into the application correctly.
    \item Generation of the messages: focuses on whether the intended messages are generated in response to the intended triggers (events, and so on). Testing whether the necessary and appropriate content is in the relevant messages is also in scope. 
    \item Transmission of messages: given the assumption that data is transmitted from mobile devices to internet-based servers, the messages need to be transmitted in a timely manner. Testing the transmission is similar to a proof-of-posting test. 
    \item Arrival of the messages: this step focuses on the proof-of-delivery aspects - did the messages successfully arrive without problems (such as corruption, truncation, interception, spoofing, and so on) and did they arrive in order within the expected timescales, \emph{etc.}? 
    \item Processing of the messages: as analytics inherently involves analysis of the raw messages, testing of the processing aspects helps to determine whether the contents were interpreted correctly. 
    \item Testing the analysis and reporting: Analytics is little use without the resulting analysis and reports, therefore they also need to be tested they are correct. As relevantly they are also worth validating in terms of usefulness/utility. 
\end{enumerate}

\subsubsection{Build-time verification} 
In-app software analytics, in practice, involves incorporating a software component into the rest of the codebase for a mobile app. Typically the software component is a pre-built software library provided by same source as the analytics tool. The intended version of the library needs to be incorporated adequately and remain in the application binary created during the build process.

One of the companies who leading the industry in terms of build-time verification is \href{https://iterative.ly}{Iteratively} who currently provide tools, scripts, and services to check at build time whether a supported analytics library has been adequately incorporated in terms of calling each of the required API calls designed using their tools. They provide implementations for various platforms including Android and iOS~\footnote{e.g. Android Kotlin~\url{https://iterative.ly/docs/interacting-with-the-sdk\#android--kotlin} and iOS Swift~\url{https://iterative.ly/docs/interacting-with-the-sdk\#ios--swift}}. Their scripts can be run as part of a continuous build process and the build script configured to block code from being merged until all of the API calls have been called, they `\texttt{lint}' the source code of the app to perform the checks, ~\cite{using_the_itly_cli_verify_the_instrumentation, using_the_itly_cli_itly_verify}.

The approach, and the supporting tools and scripts as exemplified by Iteratively, demonstrate such verification is practical. To abstract away from the specifics of their offering, the approach is:
\begin{itemize}
    \item Establish and design the analytics messages.
    \item Generate a tracking library containing the appropriate API calls for one or more analytics providers (such as Firebase Analytics, Mixpanel, and so on).
    \item Modify the source code of the core application to call these API calls (connect, or bind, the app's code with the analytics code).
    \item Lint the resulting combined library and source code and determine if there are material issues with the implementation.
\end{itemize}

\subsubsection{Automated testing}
puppeteer~\cite{using_puppeteer_to_automate_your_google_analytics_testing}

\textbf{TODO} Cite research on looking at what analytics sends, discovered using network monitoring and instrumentation. 





\section{Use}
Peers, engineering process flows, DevOps, ...

Effective use of usage-derived analytics is intended to be mutually beneficial for both the development team and the quality of the app they produce. Data from operations (the use of the app) informs the development team on aspects of how the app is performing and provides feedback on the effects of the development and testing related to the releases of the app in current use. Failures can be used to identify flaws in the app, once these are addressed and the improved version of the app is available to users - the users may then receive an improved user experience. If developers choose to add instrumentation code to their app they may be able to glean additional data related to the use of the app that's germane to flaws and failures.

Examples of instrumentation code in the context of usage analytics include: breadcrumbs, and the recording and reporting of~\emph{`non-fatal exceptions'}. Non-fatal exceptions are exceptions that occur in the running software where the exception is handled within the app without the app crashing. Some crash reporting mechanisms, including passive analytics, rely on the crash emerging from the app when the app is terminated the non-fatal exceptions are not visible using passive analytics or global crash handlers (which 'handle' exceptions that have not been 'caught' or handled elsewhere in the app).



\subsection{Using failure clusters}~\label{section-select-aggregate-scope-analyse-triage-and-prioritise}
The proposed approach is intended to achieve practical results efficaciously and address real-world issues that \emph{`move the needle'} i.e. that will deliver positive improvements. 

A half-life of bugs worth addressing, where bugs that surface primarily in older releases of an app are less worthwhile to address as the users of these older releases will either upgrade to a newer release or remain on the older release even though improvements and fixes are available in newer releases. Also, at the time of writing, app stores do not support the equivalent of back-ports of fixes to older releases, they will only offer users the current release available for their device, etc. (Etc. as there may be regional and other restrictions which limit users to older releases. While developers could choose to create specific releases for particular groups of users (\emph{i.e.} the mechanisms exist in some app stores), there is little evidence that they do.)

Stages to consider when working with clusters of failures: 

\begin{itemize}
    \item \textbf{select}: where reports offer selection criteria - decide on a suitable selection to work with. Note there may be constraints in the analytics that limit what can be selected,
    \item \textbf{aggregate}: group together data with common factors, analytics tools do this already to varying degrees,
    \item \textbf{scope}: decide what is in and out of scope in terms of the selected and aggregated data, 
    \item \textbf{analyse}: for in-scope failures, analyse the patterns in the data aggregated together, there may also be correlations between distinct aggregations worth considering, 
    \item \textbf{triage}: categorise the clusters based on the analysis, those we want to prioritise, those unready to prioritise and those where the outcome will not vary sufficiently to address,
    \item \textbf{prioritise}: the first two of the triage categories: those ready to prioritise and those not yet ready - both can be worked on, the work differs. Select a useful subset which should be able to be addressed in the next development cycle (sprint, etc.).
\end{itemize}

The work in each stage should be sufficient and proportionate to the potential value of addressing that failure. These stages are expanded shortly, before I do, there are various timescales and latencies worth considering to help set the cadence and work-in-progress for applying this process and prioritising the work to address any of the failures.

Owing to the nature of mobile app stores and their user population's habits there are substantial and material latencies worth factoring into the decision making process. These latencies include:
\begin{itemize}
    \item \textbf{pre-release}: development teams and their software development practices provide a cadence and a timescale in terms of being able to create suitably evaluated release-candidates,
    \item \textbf{app store approvals}: where release-candidates require app store approvals, and where such approval is not instantaneous, there is a practical constraint on making releases available to users via the app store,
    \item \textbf{launch rollout}: release management tools, provided by an app store, may constrain the availability of a release to users via the app store,
    \item \textbf{user adoption of new releases}: users do not update apps instantaneously, indeed adoption may take a week or more to reach 50\% of the active user population (and much longer to reach 50\% of the inactive user population), 
    \item \textbf{users who remain on older releases}: some users stick with older releases of software - they will not receive the new release, yet their usage of the older releases may count adversely against the app.
\end{itemize}

\subsection{Select}
The analytics tools may provide selection criteria, for instance, the ability to select a range of dates is commonplace. App and operating system release selection criteria are also common, regional selections, hourly ranges, by manufacturer, by device models, etc. are less commonly available.

Note: the selection criteria may not be as flexible as desired and may be inconsistently available even within an analytics tool.

\subsection{Aggregate}
Dealing with large volumes of discrete failures is tedious. Grouping the failures by one or more common elements can help patterns and relevant characteristics to be detected. The nature of analytics tools is for them to aggregate individual data records, such as events, for reporting and comparisons. Aggregations performed by analytics tools may limit the scope for the development team to apply their own independent groupings, and similarly the analytics tools may not support some aggregations directly in the tool's reports. The development team may be able to aggregate data outside the analyics tool, for instance to group daily summaries into weekly summaries. 

\subsection{Scope}
\emph{Decide what is in and out of scope in terms of the selected and aggregated data.} Some of the failures may be out of scope in terms of analysis. For instance, those where a fix has been prepared for release, those in an external software component or on device models unavailable to the team, those that are in lower rankings, those we have the competence and where-with-all to investigate, and so on. These may be out-of-scope for the rest of the stages. c.f. 3 - 7 items can be held in short-term memory, limiting w-i-p - (\emph{Kanban} -, and other concepts intended to increase throughput.

c.f. a marketing funnel, focal depth (as in a camera), range, etc. 

In summary, disregard old bugs and cold bugs. Instead pick fertile current bugs those we have the energy and urgency to make a practical difference to in terms of the outcomes of our subsequent work.


\subsection{Analyse}
\textit{TODO expand this sub-section:}

Patterns and correlations across failure clusters... 

Trends, comparisons with previous periods, with peers, etc.

Historical analysis, transient nature of the data and reports, ...

\subsection{Triage}
Triaging the resulting analysis applies one of three categories to particular analysed failure clusters. These three categories are:
\begin{enumerate}
    \item Ready to action: these failure clusters have sufficient information and analysis to be ready to action when they have been prioritised sufficiently,
    \item Further work required: these failure clusters need, and are viable candidates for being actionable once additional work has been performed,
    \item Insufficient return-on-investment: these have marginal returns even if they were to be addressed, they may be impractical to address, and so on. Discount them.
\end{enumerate}

Of these categories, when further work is required sometimes the work may be viable within the current sprint, other instances may involve additional data collection in future.

\subsection{Prioritise}
The big, ongoing, and the ugly failures, together with a mix of easy-to-fix and satisfying bugs comprise the menu of what's likely to be addressed by the development team. 

\subsection{Next steps for prioritised ready to action failure clusters}

\begin{itemize}
    \item \textbf{source-code analysis}: for example, using stack traces from crash clusters,
    \item \textbf{bug investigation}: for example, to find out if the team can reproduce failures locally, predictably and consistently,
    \item \textbf{mitigation techniques}: where practical fixing the cause is generally the best option, other options may be available \textbf{MUST-DO} expand with materials from one of my previous papers.
    \item \textbf{evaluation of the mitigation techniques} pre- and post-release evaluation is important for various reasons including calibration, providing feedback to the teams, etc.
\end{itemize}



\section{The useful life of data and reports}
The are various cycles in app usage, including daily, weekly, and annual cycles. Some trends and patterns are more useful when compared against these cycles. 

As issues emerge it may be helpful to compare current and recent patterns with prior periods, to do so a similar richness of data for those periods facilitates the comparisons and analysis.

\subsection{Preserving material/evidence}
Here, material include what's provided by the analytics tools and evidence is where we've used, or applied, the material for a purpose such as reporting a bug based on reported failures.

For various reasons the analytics tools may limit access to prior material~\emph{e.g. Google removed access to crash reports in Google Play Console because of their policy changes.}, and glitches may mean the material is not available when needed even if it should be available. Therefore it may be useful and prudent for teams to preserve material and evidence independently of the analytics tool. 

\section{Cross-cutting concerns}
Privacy, peers, evaluation criteria (including testing, verification and validation).

\subsection{Privacy and Responsibilities for using passive analytics}
For passive analytics the developer does not actively choose what to collect or how it's collected, therefore they are constrained by whoever, or whatever if we discount the people involved in deciding what to collect, etc. and assume algorithms such as AI determine the data. Google, at least, is careful to only share non PII % MUST_DO expand and add PII to the Glossary.
data and with a few exceptions limits reports to populations that exceed thresholds determined by Google internally. %MUST_DO add reference to Google Help article(s).

Nonetheless, I recommend developers consider ethical and legal responsibilities if they discover that sensitive and other PII data is being collected through the passive analytics. This may include avoiding reports with such data in them and also reporting the concerns to upstream providers of analytics (and where appropriate internal and external legal authorities).

\subsection{Peers}
Who are the peers for my app?~\footnote{Inspired by:~\href{https://www.biblegateway.com/passage/?search=Matthew\%2012:46-50&version=NIV}{Matthew 12:46-50 New International Version}.} and how much can we glean of their app's performance?

Discuss options for peers, and in particular the facilities Google provides to enable developers to compare against their `peers'.


\subsection{Evaluation criteria for Analytics Tools}
One of the considerations in terms of using analytics tools is to decide on evaluation criteria. These criteria may range from informal and implicit evaluations to more rigorous and formal approaches. Considerations also include a mix of technical and non-technical aspects such as popularity, brand, perceived ease of initial use, and so on.

This section includes four types of criteria and a rubric for evaluating analytics tools.

\subsubsection{Evidence-based criteria}

\subsubsection{Auditability}
The reliability of software where the outcomes of failure are material has been a subject of discussion and research for decades. As (\cite{dobbing1998reliability}) notes, where the reliability requirements are modest black box testing techniques may be sufficient, however \emph{``When reliability claims cannot be justified from test results alone, safety standards accept evidence from the design process"}. This paper focuses on smart instrumentation for the UK nuclear industry, nonetheless given the widespread use and implicit trust of analytics software, similar approaches to assess the reliability of this software could help in terms of auditing the behaviours of the analytics tools. Indeed two of the authors of (\cite{dobbing1998reliability}) collaborated with a third author and published a paper on the relevance and importance of software in measuring systems, where \emph{``Both users and suppliers of such systems must be aware of the risks involved and take appropriate precautions."} (~\cite{wichmann2007software}).

\subsubsection{Functional-aspects}

\subsubsection{Transparency}

\subsubsection{Veracity}

\subsubsection{Faults and failures}


\subsubsection{Verification and Validation criteria}
These two terms, verification and validation, are often used in tandem, particularly in software testing standards including the retired~\cite{BS_7925_1_1998} and the standard that superseded it~\cite{iso29119-1-2013}. The definitions from the ISO standard are:
\begin{itemize}
    \item ``Verification is confirmation, through the provision of objective evidence, that specified requirements have been fulfilled in a given work item."~\cite{iso29119-1-2013}
    \item ``Validation demonstrates that the work item can be used by the users for their specific tasks."~\cite{iso29119-1-2013}
\end{itemize}

In terms of analytics tools, verification would focus on evaluating whether the tool has been implemented correctly. Validation considers human aspects such as whether users can perform intended tasks using the analytics tool(s). (In this research context software developers of mobile apps are the main users).

As the requirements for analytics tools are often proprietary, verification using the product specified requirements may be impractical to assess rigorously unless and until one has access to these requirements. Nonetheless common-sense requirements can be established based on heuristics and experience, \emph{etc}. this is covered in the section titled: \href{rubric-for-evaluating-analytics-tools}{\nameref{rubric-for-evaluating-analytics-tools}}.

\subsubsection{Perceptions-based criteria}

\subsubsection{Qualitative/quality criteria}

\subsubsection{Functional correctness}

\subsubsection{Performance}

\subsubsection{Safety}
Freedom from harm or danger, safety in other words, may be an unlikely consideration initially especially in terms of using analytics tools. 

Safety in terms of reputation, ability to try something out, and so on, considers human aspects of using (or not-using) various analytics tools. Safety became an emerging consideration in terms of assessing various analytics tools. Safety in relationship to the researcher, the health of apps and projects related to the research, and in terms of protecting the safety of end users privacy, \emph{etc}.

\subsubsection{Security}

\subsubsection{Time-aspects}

\subsection{A rubric for evaluating analytics tools}~\label{rubric-for-evaluating-analytics-tools}
Bugs can be exposed with various qualities, a heuristic the author learned in many years of evaluating the performance of systems is zero, one, several, and many, where:
\begin{itemize}
    \item Zero: can represent a system before it is actively used and/or the quiescent state with no active users, where there were users previously.
    \item One: the first user, session, account, and so on. Unless there are stated reasons to the contrary, as the first activity starts the analytics should be able to correctly indicate and report on the activity.
    \item Several: As several activities occur in parallel and concurrently race conditions, queuing, latency, and reuse of dirty memory values can all emerge.
    \item Many: As volumes increase from several to many issues of scaling may emerge, and some of the issues that appeared to be minor with several users may increase nonlinearly. 
\end{itemize}

\subsection{Validating Analytics}

TODO TBC

\subsection{Some practical complications} 
Access may be restricted or not available to some of the steps introduced in this section. For instance, the data collection and reporting servers may be off-limits. If so, aspects of verifying the testing may be answered, at least partially, in later steps, for instance appearance of activity in an analytics report may be used to infer that the messages arrived and were processed adequately.



\section{Summary of applying analytics to development practices}
TODO TBC
