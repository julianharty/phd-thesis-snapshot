\chapter{Research Questions}
My hypothesis is that using mobile analytics can help improve both the work development teams do and the quality of the product they create. By the work I'm focusing on development, bug investigation, and testing of the software being created. For the quality of the product I'm focusing on a subset of qualities, which are technology-centric.

I picked the domain of mobile apps as they are ubiquitous, extremely popular, and have interesting and challenging contexts of use. And within the range of mobile apps I ended up focusing on Android apps for various reasons including: the analytics tools available, the relative glut of suitable apps available to me, my prior experience and expertise, and their market share.

The core question I aim to consider is: 
\emph{How can applying analytics improve software development and software testing?} Here I am assuming that analytics can help, as stated by Buse and Zimmermann ~(\citeyear{buse_analytics_2010}).

This leads to several related questions that underpin this main question \emph{i.e.}, I have grouped these in three categories: sources, value, and impact.

\akb{There are a lot of sub-questions below. You will need to focus on the ones you have data to evaluate, or have more abstract formulations that cover groups of sub-questions.}

\section{Sources}
\begin{itemize}
    \item \emph{What sources of analytics are available?} at a superficial level there seem to be those that operate within the app and those that are external to the app, particularly those that gather data at the platform level. I investigated a couple of widespread analytics offerings and consider several more as part of my research and understanding the overall context.
    \item \emph{How do the sources I've investigated compare in terms of the data they collect and how they are used?}
\end{itemize}

\section{Value}
Does using analytics provide quantitative and/or qualitative value that can be measured? Could it provide value in terms of assessing the quality of our work that was invested into developing, testing and preparing software before it was launched?
\begin{itemize}
    \item \emph{How truthy are various analytics offerings?} We discovered numerous errors in various analytics offerings. Let's share these results with the research community.
    \item \emph{How much does the fidelity matter of the analytics offerings?} Can we use the results productively even if they are flawed?
    \item \emph{How does using the various analytics compare with other sources or reflections of software quality?} Research already studies how various sources, such as ratings and reviews, can be used to identify flaws in software. Where and how do analytics fit into the larger context of these tools. \textbf{Note: I've not actively compared the sources from a practical perspective, however the catrobat case-study my be relevant}.
    \item \emph{How can the analytics be used to inform and assess our work that went into creating and testing a particular release?} data from usage analytics can reinforce aspects of what our work discovered pre-release (c.f. how Google Android's pre-launch reports cross-identify crashes) it can also identify quality flaws we missed in our work.
    \item \emph{How can analytics help with bug investigation?} a single bug instance may be hard to assess in terms of the likely scope and impact on a user-base; how, where and when can analytics help with bug investigation? We might also consider practical limits e.g. that are enforced by the real-world analytics we used. 
\end{itemize}

\section{Impact}
Here the focus is on whether the value has sufficient impact for anyone else to be interested in using and applying analytics. Given the nature of the research the main measures are practical, \emph{i.e.} in the real-world.
\begin{itemize}
    \item \emph{Do development teams use analytics in their practice?} If development teams find practices useful they will generally try to use them intrinsically. Do they? And if so, how?
    \item It's one thing to be able to improve a measurement such as the crash rate, it's also worth considering whether that has any material impact on other relevant measurements. \emph{Can we discern changes, even improvements, in user satisfaction, retention, etc. through using analytics?} One of the presumptions (identified in research) is that improving quality improves the user's satisfaction with mobile apps. If so, presumably we should be able to measure the effects, even crudely.
    \item \emph{Has anyone else found the work of interest? are there additional evidence of the impact of the work?} Here I'm mainly considering feedback from other researchers, and from Industry e.g. Google.
\end{itemize}

\section{Describe my research methodology and my choices}
As my main research question considers the application of analytics, the research needs to include a combination of usage \& analytics data where the analytics data is then applied with the intent of improving the product quality. The developers may not be successful in achieving improvements, although we hope they will be. They may also be able to improve their practices, so again their current and revised processes are also of interest.

Although I had prior experience in industry of the efficacy and potency of applying usage analytics to improve software development and testing of mobile apps, that experience is generally covered by confidentially agreements, and also the analytics tools have changed and developed markedly since those experiences. Therefore, action research seemed appropriate, particularly as one of the long-term opensource projects had extremely high failure rates according to the de-facto Android analytics tool. I decided it was appropriate and necessary to see if I could directly help that project to improve their mobile apps - \emph{``physician heal thyself"}\footnote{\href{https://en.wikipedia.org/wiki/Physician,\_heal\_thyself}{wikipedia.org/wiki/Physician,\_heal\_thyself}.}.

The next level of validity was that even if this work achieved the desired results, could other development teams achieve similar results by applying a similar approach? To help gain evidence the research engaged a separate opensource development project and development team. 

Working with non-profit opensource projects, where the teams are generally willing to make their practices and results public helps with being able to obtain information and publish the results. However, as many research projects discover, what might work for opensource projects - while important and interesting to the research community - might not matter much to the industry of professional and commercial development teams. 

Opensource apps are a tiny proportion of mobile apps available to users, so another level of learning and validation could be achieved through the insights of these professional and commercial development teams. Surveys tend to have poor response rates and lack the depth or richness I was seeking therefore I chose to engage at a deeper level as a fellow developer interviewing developers of several apps. Of course we would need the permission of their organisations, and some shared examples of their tools, their practices and their results of applying analytics well, and even times when they hadn't.

This research is not immune from also being improved, and similarly the process is likely to have plenty of scope for improvement as we learn more from the various projects, teams, apps, and analytics tools. Similarly, there is scope to find flaws, limitations, and weaknesses in the analytics tools, therefore there was scope in the research to share findings with the teams responsible for these, and related, software tools and to use the experiences and insights from any such sharing as part of this research.

