\chapter{Evaluation}
\label{chapter-evaluation}

This chapter evaluates the research performed during the PhD.

%%% Notes on this chapter from call with Arosha and Yijun on 11 June 2021

% Evaluate the RQ's here at the highest level of abstraction. 

\textbf{Actions:} 
\begin{itemize}
    \item Remove the case study specific materials from this chapter. Echo / summarise/draw on what was presented in the case studies. 
    \item Refactor the current contents as it's moved around.
    \item This chapter should aim to close the questions. 
    \item Note: small case studies are unlikely to be major contributions to this chapter.
\end{itemize}



Do I have a methodological perspective I'd like to present here?
e.g.
% Methodological perspectives:
\begin{itemize}
    \item Top down approach? (This might apply if I were involved in designing an analytics service to help developers improve the reliability of their apps.)
    \item Crash reduction plan? (A common perspective for the projects I've worked with.) 
    \item Collect everything? analyse later? (An exploratory approach where the methodology emerges with experience.)
\end{itemize}


There's an opportunity to discuss limitations in current analytics infrastructure. Discussion for acceptable technical debt (c.f. countries debt burden as percent of GDP)


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\par\noindent\rule{\textwidth}{0.4pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\section{Evaluation of common threads in the case studies}
The case studies both led to significant improvements to the measured reliability of the treatment app. 

For the Kiwix project this reduction was one of the main reasons why they then chose to update the codebase for the majority of their custom apps~\footnote{The Kiwix project team decided some seldom used apps were not worth updating.}. They achieved similar improvements in the measured reliability despite only using the default analytics provided by Google Play Console. 

For the Catrobat project the development teams chose to add crash and mobile analytics to their iOS app which had neither previously and mobile analytics to their Android app, as it already incorporated crash analytics.

\section{Evaluation of the Catrobat case study}
The developers were able to significantly improve the reliability of their major Android app: Pocket Code, through applying a variation of the process identified in my research. The minor app, called Pocket Paint, is a separately packaged version of the drawing (paint) functionality in Pocket Code.

The project team were able to use two sources of analytics on crashes: Google Play Console incorporating Android Vitals, and Fabric Crashlytics. There were differences in the outputs and results of these tools, these are discussed later in this chapter, nonetheless they contained a common set of crashes and fixing the crash was reflected in both tools.

The project has a particularly and unusually mature process and focus on software quality as part of being an essential year-long module for various undergraduate courses and also used for post graduate research at Masters and PhD level. Nonetheless the crash rate was stubbornly high and exceeded Google's limit before we started the case study.

TBC

Still to do:
\begin{itemize}
    \item Intermittent focus on addressing crashes
    \item Loss of Crashlytics data for a period through a mistake configuring a release.
    \item The inclusion of Firebase Analytics in the Android app.
    \item The inclusion of Crash and Firebase Analytics in the iOS app, including ongoing work: See Firebase in iOS app https://jira.catrob.at/browse/CATTY-371?jql=text%20~%20%22firebase%22%20ORDER%20BY%20created%20DESC 
    \item Proposal to move to privacy-friendly analytics tools. 
\end{itemize}



\section{Evaluation of the Analytics Tools}

\textbf{TODO} \textit{Rewrite this section using the evaluation criteria in Chapter 5.}

The research primarily used and compared two analytics tools: Google Play Console incorporating Android Vitals, and Fabric Crashlytics, developed and maintained by Google. The research identified a wide range of flaws, summarised here.

\begin{enumerate}
    \item Testing discouraged: being encouraged and able to test the workings of the algorithm can increase the confidence in the analytics being generated. Flaws can be identified, discussed and either addressed or workarounds found.
    \item Negative user populations: Google Play Console provides two complementary graphs in addition to a third value useful for triangulation and for understanding the user-base for an app. For some apps the combination of the data in the first two graphs adds up to a negative volume of users.
    \item Gaps in the data: On numerous occasions Android Vitals graphs had gaps in the charts.
    \item No updates for 10+ days: in Autumn 2019. No explanation or information was provided to developers about the problem or the causes. %pigeon food experiment psychological effects.
    \item No service dashboard: Google Play Console does not provide anywhere to check the current status or previous outages (unlike many other similar tools and other Google services).
    \item Repeated graphs in dashboard report.
    \item Inconsistent data ranges for some of the graphs in the dashboard report.
    \item Incorrect data ranges for Crash and ANR statistics reports.
    \item Unexplained and unfounded headline warning alerts.
    \item Missing URL parameters for links to drill-down reports in Android Vitals for some Android Releases.
    \item Second most popular country's details conflated with the the most popular and used for the title of one of the acquisition reports.
    \item Poor grouping of crash clusters.
    \item Lack of reports for low to medium volume user-bases.
    \item 10x difference in calculated crash rate between the two analytics tools.
    \item Misleading date for the last update of apps
\end{enumerate}

\textbf{MUST-DO} Group into types of problem. 

\iffalse 
\begin{table}[!htbp]
    \begin{threeparttable}[t]
    \footnotesize
    \centering

    \begin{tabular}{p{0.15\textwidth}p{0.3\textwidth}p{0.4\textwidth}}
    Flaw &Example &Impact \\
    \hline

    Negative populations &
    Pocket Code (see lifetime report L) &
    Lack of trust in Google’s analytics.\\
    
    Gaps in the data &
    Data missing for 1+ days (see below for examples) &
    Calculations affected, reduction in value of the reports, incomplete information. \\
    
    Missing URL parameters &
    Links to drill down on crash clusters for particular Android versions\tnote{1} &
    Confusion, risk of misattributing the reported crashes to a particular Android version when they are actually for the overall, current range of Android releases an app is running on. \\
    
    Incorrect ranges (a possible off-by-one calculation?) &
    Mismatch between overview and timeseries reports for Crashes and ANRs (see figures W,X,Y,Z). \\
    
    Poor groups of crash clusters &
    Kiwix examples dated \nth{4} July 2020\tnote{2} &
    Flaws in bug identification (mistakenly believing the scope of a crash cluster), potential suboptimal prioritisation of bugs to address. \\
    
    Lack of reports for low-volumes of usage &
    e.g. for WikiMed in Japanese with 1842 active users &
    Some key reports not available during early growth stages of an app, those apps may fail to thrive if they’re failing and developers don’t know the causes are poor reliability. \\
    
    \nth{2} country’s data conflated with \nth{1} &
    Acquisition Reports in app dashboard (see screenshot C)\tnote{3} &
    Confusion and lack of confidence in the reports. \\
    
    No facility for testing and testing discouraged &
    Implicit from the Terms of Service, section\tnote{4}. Other analytics tools, including those provided by Google, do encourage testing. &
    Developers dissuaded from testing the functionality or reporting. They have to [decide whether to] take on trust what Google chooses to tell them. \\


    \end{tabular}
    \begin{tablenotes}
    \item[1]e.g. the URL includes the relevant numeric value for most but not all the Android Versions \texttt{androidVersion=28} is correct and present for Android 8.1
    
    \item[2] a) \texttt{java.lang.IllegalStateException org.kiwix.kiwixmobile.core.base.BaseActivity.onCreate} \\
    b) \texttt{tgkill}
    
    \item[3] Note: partly addressed in the June 2020 revamp of Google Play Console.
    
    \item[4] \emph{``Broken Functionality We don’t allow apps that crash, force close, freeze, or otherwise function abnormally.”}~\cite{google_play_developer_policy_center}
    \end{tablenotes}
    
    \caption{Issues in Google Play Console Reports}
    \label{tab:issues-in-google-play-console-reports}
    \end{threeparttable}
\end{table}
\fi

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage[normalem]{ulem}
% \useunder{\uline}{\ul}{}
\begin{table}[!htbp]
\scriptsize
\renewcommand\TPTminimum{\textwidth}
%% Arrange for "longtable" to take up full width of text block
\setlength\LTleft{0pt}
\setlength\LTright{0pt}
\setlength\tabcolsep{6pt}
\begin{tabular}{p{0.18\textwidth}p{0.46\textwidth}p{0.35\textwidth}} %{@{}lll@{}}

\toprule
Flaw &
  Example &
  Impact \\ \midrule
Negative populations &
  Pocket Code (see lifetime report below) &
  Lack of trust in Google’s analytics \\
Gaps in the data &
  Data missing for 1+ days (see below for examples) &
  Calculations affected, reduction in value of the reports, incomplete information \\
Missing URL parameters &
  Links to drill down on crash clusters for particular Android versions e.g. Android 8.0 vs. Android 8.1 &
  Confusion, risk of misattributing the reported crashes to a particular Android version when they are actually for the overall, current range of Android releases an app is running on. \\
Incorrect ranges (a possible off-by-one calculation) &
  Mismatch between overview and timeseries reports for Crashs and ANRs (see below for examples - 4 figures) &
  One day’s data ‘missing’ from the timeseries reports. Confusion, lack of trust in the reports. \\
Poor groups of crash clusters &
  Kiwix example 04 Jul 2020\\ java.lang.IllegalStateException &
  Flaws in bug identification (mistakenly believing the scope of a crash cluster), potential suboptimal prioritisation of bugs to address. \\
Lack of reports for low-volumes of usage &
  E.g. for WikiMed in Japanese with 1842 active users. &
  Some key reports not available during early growth stages of an app, those apps may fail to thrive if they’re failing and developers don’t know the causes are poor reliability \\
2nd country’s data conflated with 1st. &
  Acquisition Reports in app dashboard (see below for screenshots). Note partly addressed in the June 2020 revamp of Google Play Console. &
  Confusion and lack of confidence in the reports \\
No facility for testing and testing discouraged &
  Implicit from the Terms of Service, section “Broken Functionality We don’t allow apps that crash, force close, freeze, or otherwise function abnormally.” Other analytics tools, including those provided by Google, do encourage testing. &
  Developers dissuaded from testing the functionality or reporting. They have to {[}decide whether to{]} take on trust what Google chooses to tell them. \\
Repeated graphs &
  In dashboard report for apps 2 graphs are repeated: New users acquired, and Users lost. &
  Breaks ‘don’t repeat yourself’ (DRY). Waste of attention. \\
Inconsistent date ranges for some dashboard reports &
  The audience growth reports: new users acquired by country, and top countries graphs are for fixed periods - the period is not explained or visible on the report. In contrast the Android Vitals graphs are explicitly for the last 30 days, not necessarily ideal but at least documented that the period is fixed. &
  Confusion, inconsistent behaviour of the reports, potential for misinterpretation. \\
No updates for several days &
  In September 2019 the Android Vitals graphs and data were not updated for around 10 days &
  Inability to see or respond to stability issues, loss of confidence in the service. \\
Unexplained negative headline rate &
  Crash rate of 1.66\% - urgent warning, yet drilling down into the report none of the details reach 1.66\%, they are all significantly lower values. &
  Reduction in confidence in their alerts, and similarly a lack of trust in their calculations \\
No Service Problem Reporting &
  Google, and many others, provide a service status page, and many also include a history. For instance Salesforce provides https://trust.salesforce.com/en/ and status pages e.g. https://status.salesforce.com/incidents/5800 &
  A lack of transparency which leads to a lack of trust. \\ \bottomrule
\end{tabular}
    \caption{Issues in Google Play Console Reports}
    \label{tab:issues-in-google-play-console-reports}
\end{table}


\section{Evaluation of the software created for the research}
\textbf{MUST-DO} complete his section.

Placeholders for this section:
\begin{itemize}
    \item Testing logging:
    \item Zipternet app: Microsoft AppCenter, zero-to-ten, Kotlin.
    \item Vital Scraper and related projects: NPM package release process.
    \item ...
\end{itemize}



\section{Validity considerations}
In absolute terms, my research covers a minuscule percentage of all the apps available in Google Play, roughly 1 in 100,000. So these results may not apply to all the apps, or potentially even a majority of them. And yet, the results have consistently indicated that when development teams pay attention to stability metrics they are able to materially improve the reliability of their mobile apps even though their apps range across several app store categories and range in userbase from under 1,000 active users to over 160,000. These apps are spread across 6 of the 7 groups of downloads identified in AppBrain's `Download distribution of Android apps'~\cite{appbrain_download_statistics_june_2019} and similarly 5 of the 7 groupings representing over 94\% of the distribution of downloads in Google Play according to Wang ~\emph{et al} (2018)~\cite{wang2018_beyond_google_play}.

\textbf{MUST-DO} answer the following question: What exists in the literature, common practices, vs what I was able to achieve. \emph{From a question raised by Alistair Willis, OU, 30 April 2020.}

\subsection{How many developers are enough to ask?}
On of the key considerations for research is adequacy in terms of coverage. For my research there are several types of coverage, including: development teams, user-bases for the various apps, software tech stacks used (in terms of programming languages, analytics libraries, etc.), application domains, and so on. 

c.f. Krug is a well respected Usability guru whose work is inherently practical in nature. In the first edition of his~\emph{``Don't make me Think"} book he discusses ways to obtain practical results even with short timescales and few resources. In terms of obtaining value the author indicated that 3 to 4 people were capable of delivering more relevant feedback by involving them over time, (Chapter 9 in ~\cite{krug2000dont_make_me_think}). In terms of selecting the candidates his recommendation was to worry less about selecting 'representative users', instead\emph{``Recruit loosely, and grade on a curve."} (Chapter 9 in ~\cite{krug2000dont_make_me_think})~\footnote{Note: Krug made several chapters, including this one available online when the second edition of the book was published. I have copies of all three versions of the book and of these chapters as PDF files.}.

% exploratory personas.
% GTAC conference talk about developers not having engineering degrees (The one Isabel went to Mr Tupule, Googler.)

My research included working with two mature project teams and developers of three commercial apps. It is also based on work I did in industry that predates the PhD research, unfortunately I am not able to provide details of those projects in my thesis. 


\subsection{Validation of the concepts}
\textbf{MUST-DO}: Revise to reflect the recently added case studies. 

My practical research focuses on two sets of Android applications, those of the Kiwix and Catrobat project teams. According to data and reports Google provides the development teams their active user-bases are 362,595 for the Kiwix project across 18 published apps, and 148,966 for the Catrobat project across 6 published apps. %data obtained on 16th May 2020.
TODO map these apps to the buckets in the table from the 'beyond Google Play' paper.

While these apps include a useful variety of user populations (from 10's of users to 150K+ across many countries and tens of apps) they could be perceived as a \emph{drop in the ocean} of the millions of apps currently available in the Google Play app store. Also, both project teams are non commercial, and may have different working dynamics from commercial development projects and teams. As my research was inspired from my consulting work with businesses who rely on the success of their apps I chose to supplement these two projects by engaging with developers from several commercial development teams. These include: Moonpig, Moodspace, and LocalHalo. Each values and uses analytics data during their development process to assess post-launch issues with their apps. From time to time things go awry with the behaviours of one or more of their releases and analytics helps them to identify and respond to issues before they become pervasive. For the LocalHalo app, \emph{TODO add details}... For Moonpig \emph{TODO add details}...

\subsubsection{Validation by the Google Engineering Team}
In Spring 2019 I reported various flaws or potential anomalies in various reports Google Play Console provides to developers to the then Product Manager for Android Vitals, Mr Fergus Hurley. As the long-term product owner he has extensive and insider experience of the tools and reports Google provides to an estimated population of over 1 million Android developers \emph{TODO add references e.g. to the Beyond Google Play paper and the one about a few developers creating an exponential number of apps}. I asked for his perspective during both a long in-person meeting and a follow-up video call a few weeks later. He confirmed several of the issues and debated others. He was willing to go on record in one of my accepted peer-reviewed papers on the topic \emph{TODO add link} and asked me to continue to share my findings with them. During the next 12 months he and then they added more Google staff to the discussion and asked me to write up my findings in a document that became over 30 pages long. Their policy means they are unlikely to confirm changes they make as a result of my research and findings, nonetheless they accept and value the feedback that has been provided. They also confirmed various bugs were ones they want to address.


\section{Summary of evaluation of the research}

