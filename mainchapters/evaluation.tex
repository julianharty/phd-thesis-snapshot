\chapter{Evaluation}
\label{chapter-evaluation}

This chapter evaluates the \href{section-research-questions}{\nameref{section-research-questions}} presented in the Introduction. It also evaluates the research, case studies, and analytics tools. In addition this chapter establishes common themes that span more than one case study and discusses threats to validity of the research. 


\section{Evaluation of the Research Questions}
\textbf{MUST-DO}: Evaluate the RQ's here at the highest level of abstraction by applying section 1.4.1 on ~\href{section-sources}{\nameref{section-sources}}, ~\href{section-value}{\nameref{section-value}}, and ~\href{section-impact}{\nameref{section-impact}} for each case study.
% Suggestion: try creating a table.

Evaluation of the core research question~\href{core-research-question}{\emph{``How can applying analytics improve software development and software testing for mobile apps?"}}

The Kiwix Android project illustrated manifold reductions in crash rates for new releases of the apps that used the codebase that included various fixes to address failures reported by Android Vitals. There were ongoing improvements over a series of releases. A couple of these releases focused primarily on crash fixes, others included new features and migration of the codebase from Java to Kotlin in addition to interim crash fixes. 
The migration spans several years and is ongoing. The migration to Kotlin was for a range of reasons including the goal of obviating some of the causes of crashes.

The results for software testing were far more limited in terms of the Kiwix Android case study as we were unable to reproduce crashes on one of the device models that encountered higher than average crash rates. \emph{One of the as-yet unwritten industrial case studies provides several examples where mobile analytics was useful in terms of testing that project's Android app.}

\subsection{Evaluation of Sources}
There are many sources of mobile analytics. This research evaluated one of the most pervasive platform-level analytics and the one that explicitly aims to help developers improve the qualities of their mobile apps. It also evaluates several in-app analytics services that provide crash and error reporting and identifies various differences in the results of the platform-level and in-app analytics in terms of crash reporting in particular.

When projects include in-app analytics the developers tend to rely more on the reports provided by their respective services rather than using Android Vitals for crash reporting. 

Two of the projects (Kiwix and Catrobat) chose to exclude in-app analytics of any form because of the development teams' concerns related to the information gathered by the analytics provider potentially compromising the privacy of end users, and another project reluctantly migrated from Fabric Crashlytics to Firebase's implementation (which collects more data).

Preliminary investigation at a local (small) scale was conducted in two providers of heatmapping software (Appsee and Azetone)~\href{section-heatmapping} in the section: {\nameref{section-heatmapping}}. Of these, Appsee's offering worked reliably and indicated the potential of heatmapping to detect interaction flaws in screen design for Android apps based patterns detected in aggregated usage data. The Azetone service was problematic to commission and their reports had significant errors in them which were not corrected during the evaluation period despite discussions with their CEO and CTO. A combination of factors meant the research stopped at this preliminary stage: there are more privacy implications when using heatmapping compared to code-oriented reliability metrics and there wasn't a suitable, available app for a larger scale evaluation of the capabilities and behaviours of heatmapping in popular, production apps. As AppBrain indicates, the market penetration of Appsee is minute, in 0.07\% of Android apps~\footnote{As of \nth{12} July 2021} (not helped by the company being acquired and the product being discontinued).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Evaluation of Value}
\textbf{Truthiness}: The Kiwix Android case study was the first to surface numerous flaws and restrictions in the analytics provided by Google to Android developers. The relevant Google product and engineering teams acknowledged many of these explicitly and only questioned one of the reported flaws.

\textbf{How much does the fidelity matter?} As reported in~\citep{harty_improving_app_quality_despite_flawed_mobile_analytics} the flawed analytics were nonetheless useful and they have been used productively by every Android app development team who contributed to this research.

The Kiwix Android project was already using a build pipeline that ran automated tests written by the team on both emulators and physical devices. However the test results and indeed the automated testing were not necessarily useful. % MUST-DO add details to the case study on this topic.
The Kiwix Android project actively uses static analysis, in particular Android Lint. The use predates the case study and continues~\footnote{Commits that mention Lint started in 2013~\url{https://github.com/kiwix/kiwix-android/commit/ab91367f760bbf5ccc3ad1394daf575330591628} and the most recent set are listed in~\url{https://github.com/kiwix/kiwix-android/search?q=lint&type=commits}. One of the early issues that mention Lint is~\href{https://github.com/kiwix/kiwix-android/issues/43}{Issue 43}, which is from 2016 when the code was hosted on SourceForge.net.} and the lead developer at the time of the case study focused on tracking Lint issues~\href{https://github.com/kiwix/kiwix-android/issues/1271}{Issue 1271}. The project team also use \href{https://ktlint.github.io/}{ktlint} to provide static analysis for the Kotlin code.

\textbf{Comparing the use of Mobile Analytics with other sources or reflections of software quality}: The use of analytics continued in parallel with the use of static analysis. The project stopped using the automated testing on the device farm provided by BitBar.com~\footnote{\url{https://cloud.bitbar.com/}} for a mix of practical and logistical reasons. A key blocker emerged when the Kiwix project switched to split binary releases and the BitBar Gradle plugin could not upload the binary file to their cloud service~\footnote{\url{https://cloud.bitbar.com/}}~\citep{kiwixandroid_issue_1228_bibbar_not_working_with_split_apks}. Another factor was the unreliability of running the automated tests as part of the CI on multiple physical devices~\citep{kiwixandroid_issue_1223_bitbar_should_run_tests_on_multiple_devices}.

Meanwhile, the project switched from using Travis-ci to GitHub Actions for the continuous builds (which included running the automated tests on these devices in the travis-ci configuration) and BitBar was acquired by SmartBear~\citep{bitbar2019_smartbear_acquired_bitbar}~\footnote{Note: the project still has access to the rebranded BitBar cloud testing service as of \nth{16} July 2021}. There was some exploratory work to replace BitBar with BrowserStack, however neither service is currently used~\citep{kiwixandroid_issue_1223_bitbar_should_run_tests_on_multiple_devices}. In summary, during the case study the automated testing on Android devices was scaled back, and yet the reliability increased in production - for end users.

The reported crash clusters were sometimes raised as issues and addressed explicitly with bug fixes. In parallel, there was an ongoing push to replace Java code with Kotlin implementations with the aim of obviating some of the sources of crashes. The combination was highly effective in terms of reducing the overall crash rate for the Android apps. Meanwhile there were various code contributions by periodic volunteers, including by GSoC contributors, who were unlikely to have had access to Google Play Console - and therefore they would not have been able to see any of the analytics~\footnote{Larger development teams tend to only provide access to Google Play Console to a subset of the team. The ones who have access tend to be the more senior developers and those who are core to the project. The administration features provided in Google Play Console allow for read-only access and also allow administrators to limit access to the analytics for particular apps.}. 
  
\textbf{Using mobile analytics to inform and assess the work that went into creating and testing a particular release}: The main finding was Google Play Console's cross-linking of crash clusters in production with crashes detected by their pre-launch report [\textbf{MUST-DO} write this up and provide at least one concrete example]. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Evaluation of Impact}
The Kiwix Android team saw immediate, sustained and ongoing impact in applying crash (operational) analytics for the core app. As the codebase is shared with the various custom Kiwix Android apps, the improvements in the underlying codebase percolated to these apps as they were updated and refreshed in Google Play. The hackathon was for a week, of which less than one day was used to focus on addressing two of the three most frequent crashes, the project team continued to use Android Vitals on an ongoing basis for at least a year afterwards. That said, after the lead developer left the project, less emphasis has been placed on addressing crashes by the rest of the active developers, who are volunteers and working more sporadically on the project.


\newpage
\section{Evaluation of common threads in the case studies}
The opensource case studies both led to significant improvements to the measured reliability of the treatment app. 

For the Kiwix project this reduction was one of the main reasons why they then chose to update the codebase for the majority of their custom apps~\footnote{The Kiwix project team decided some seldom used apps were not worth updating.}. They achieved similar improvements in the measured reliability despite only using the default analytics provided by Google Play Console. 

For the Catrobat project the development teams chose to add crash and mobile analytics to their iOS app which had neither previously and mobile analytics to their Android app, as it already incorporated crash analytics.



\section{Evaluation of the Analytics Tools}

\textbf{TODO} \textit{Rewrite this section using the evaluation criteria in Chapter 5.}

The research primarily used and compared two analytics tools: Google Play Console incorporating Android Vitals, and Fabric Crashlytics, developed and maintained by Google. The research identified a wide range of flaws, summarised here.

\begin{enumerate}
    \item Testing discouraged: being encouraged and able to test the workings of the algorithm can increase the confidence in the analytics being generated. Flaws can be identified, discussed and either addressed or workarounds found.
    \item Negative user populations: Google Play Console provides two complementary graphs in addition to a third value useful for triangulation and for understanding the user-base for an app. For some apps the combination of the data in the first two graphs adds up to a negative volume of users.
    \item Gaps in the data: On numerous occasions Android Vitals graphs had gaps in the charts.
    \item No updates for 10+ days: in Autumn 2019. No explanation or information was provided to developers about the problem or the causes. %pigeon food experiment psychological effects.
    \item No service dashboard: Google Play Console does not provide anywhere to check the current status or previous outages (unlike many other similar tools and other Google services).
    \item Repeated graphs in dashboard report.
    \item Inconsistent data ranges for some of the graphs in the dashboard report.
    \item Incorrect data ranges for Crash and ANR statistics reports.
    \item Unexplained and unfounded headline warning alerts.
    \item Missing URL parameters for links to drill-down reports in Android Vitals for some Android Releases.
    \item Second most popular country's details conflated with the the most popular and used for the title of one of the acquisition reports.
    \item Poor grouping of crash clusters.
    \item Lack of reports for low to medium volume user-bases.
    \item 10x difference in calculated crash rate between the two analytics tools.
    \item Misleading date for the last update of apps
\end{enumerate}

\textbf{MUST-DO} Group into types of problem. 

\iffalse 
\begin{table}[!htbp]
    \begin{threeparttable}[t]
    \footnotesize
    \centering

    \begin{tabular}{p{0.15\textwidth}p{0.3\textwidth}p{0.4\textwidth}}
    Flaw &Example &Impact \\
    \hline

    Negative populations &
    Pocket Code (see lifetime report L) &
    Lack of trust in Google’s analytics.\\
    
    Gaps in the data &
    Data missing for 1+ days (see below for examples) &
    Calculations affected, reduction in value of the reports, incomplete information. \\
    
    Missing URL parameters &
    Links to drill down on crash clusters for particular Android versions\tnote{1} &
    Confusion, risk of misattributing the reported crashes to a particular Android version when they are actually for the overall, current range of Android releases an app is running on. \\
    
    Incorrect ranges (a possible off-by-one calculation?) &
    Mismatch between overview and timeseries reports for Crashes and ANRs (see figures W,X,Y,Z). \\
    
    Poor groups of crash clusters &
    Kiwix examples dated \nth{4} July 2020\tnote{2} &
    Flaws in bug identification (mistakenly believing the scope of a crash cluster), potential suboptimal prioritisation of bugs to address. \\
    
    Lack of reports for low-volumes of usage &
    e.g. for WikiMed in Japanese with 1842 active users &
    Some key reports not available during early growth stages of an app, those apps may fail to thrive if they’re failing and developers don’t know the causes are poor reliability. \\
    
    \nth{2} country’s data conflated with \nth{1} &
    Acquisition Reports in app dashboard (see screenshot C)\tnote{3} &
    Confusion and lack of confidence in the reports. \\
    
    No facility for testing and testing discouraged &
    Implicit from the Terms of Service, section\tnote{4}. Other analytics tools, including those provided by Google, do encourage testing. &
    Developers dissuaded from testing the functionality or reporting. They have to [decide whether to] take on trust what Google chooses to tell them. \\


    \end{tabular}
    \begin{tablenotes}
    \item[1]e.g. the URL includes the relevant numeric value for most but not all the Android Versions \texttt{androidVersion=28} is correct and present for Android 8.1
    
    \item[2] a) \texttt{java.lang.IllegalStateException org.kiwix.kiwixmobile.core.base.BaseActivity.onCreate} \\
    b) \texttt{tgkill}
    
    \item[3] Note: partly addressed in the June 2020 revamp of Google Play Console.
    
    \item[4] \emph{``Broken Functionality We don’t allow apps that crash, force close, freeze, or otherwise function abnormally.”}~\cite{google_play_developer_policy_center}
    \end{tablenotes}
    
    \caption{Issues in Google Play Console Reports}
    \label{tab:issues-in-google-play-console-reports}
    \end{threeparttable}
\end{table}
\fi

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage[normalem]{ulem}
% \useunder{\uline}{\ul}{}
\begin{table}[!htbp]
\scriptsize
\renewcommand\TPTminimum{\textwidth}
%% Arrange for "longtable" to take up full width of text block
\setlength\LTleft{0pt}
\setlength\LTright{0pt}
\setlength\tabcolsep{6pt}
\begin{tabular}{p{0.18\textwidth}p{0.46\textwidth}p{0.35\textwidth}} %{@{}lll@{}}

\toprule
Flaw &
  Example &
  Impact \\ \midrule
Negative populations &
  Pocket Code (see lifetime report below) &
  Lack of trust in Google’s analytics \\
Gaps in the data &
  Data missing for 1+ days (see below for examples) &
  Calculations affected, reduction in value of the reports, incomplete information \\
Missing URL parameters &
  Links to drill down on crash clusters for particular Android versions e.g. Android 8.0 vs. Android 8.1 &
  Confusion, risk of misattributing the reported crashes to a particular Android version when they are actually for the overall, current range of Android releases an app is running on. \\
Incorrect ranges (a possible off-by-one calculation) &
  Mismatch between overview and timeseries reports for Crashs and ANRs (see below for examples - 4 figures) &
  One day’s data ‘missing’ from the timeseries reports. Confusion, lack of trust in the reports. \\
Poor groups of crash clusters &
  Kiwix example 04 Jul 2020\\ java.lang.IllegalStateException &
  Flaws in bug identification (mistakenly believing the scope of a crash cluster), potential suboptimal prioritisation of bugs to address. \\
Lack of reports for low-volumes of usage &
  E.g. for WikiMed in Japanese with 1842 active users. &
  Some key reports not available during early growth stages of an app, those apps may fail to thrive if they’re failing and developers don’t know the causes are poor reliability \\
2nd country’s data conflated with 1st. &
  Acquisition Reports in app dashboard (see below for screenshots). Note partly addressed in the June 2020 revamp of Google Play Console. &
  Confusion and lack of confidence in the reports \\
No facility for testing and testing discouraged &
  Implicit from the Terms of Service, section “Broken Functionality We don’t allow apps that crash, force close, freeze, or otherwise function abnormally.” Other analytics tools, including those provided by Google, do encourage testing. &
  Developers dissuaded from testing the functionality or reporting. They have to {[}decide whether to{]} take on trust what Google chooses to tell them. \\
Repeated graphs &
  In dashboard report for apps 2 graphs are repeated: New users acquired, and Users lost. &
  Breaks ‘don’t repeat yourself’ (DRY). Waste of attention. \\
Inconsistent date ranges for some dashboard reports &
  The audience growth reports: new users acquired by country, and top countries graphs are for fixed periods - the period is not explained or visible on the report. In contrast the Android Vitals graphs are explicitly for the last 30 days, not necessarily ideal but at least documented that the period is fixed. &
  Confusion, inconsistent behaviour of the reports, potential for misinterpretation. \\
No updates for several days &
  In September 2019 the Android Vitals graphs and data were not updated for around 10 days &
  Inability to see or respond to stability issues, loss of confidence in the service. \\
Unexplained negative headline rate &
  Crash rate of 1.66\% - urgent warning, yet drilling down into the report none of the details reach 1.66\%, they are all significantly lower values. &
  Reduction in confidence in their alerts, and similarly a lack of trust in their calculations \\
No Service Problem Reporting &
  Google, and many others, provide a service status page, and many also include a history. For instance Salesforce provides https://trust.salesforce.com/en/ and status pages e.g. https://status.salesforce.com/incidents/5800 &
  A lack of transparency which leads to a lack of trust. \\ \bottomrule
\end{tabular}
    \caption{Issues in Google Play Console Reports}
    \label{tab:issues-in-google-play-console-reports}
\end{table}


\section{Evaluation of the software created for the research}
\textbf{MUST-DO} complete his section.

Placeholders for this section:
\begin{itemize}
    \item Testing logging:
    \item Zipternet app: Microsoft AppCenter, zero-to-ten, Kotlin.
    \item Vital Scraper and related projects: NPM package release process.
    \item ...
\end{itemize}



\section{Validity considerations}
In absolute terms, my research covers a minuscule percentage of all the apps available in Google Play, roughly 1 in 100,000. So these results may not apply to all the apps, or potentially even a majority of them. And yet, the results have consistently indicated that when development teams pay attention to stability metrics they are able to materially improve the reliability of their mobile apps even though their apps range across several app store categories and range in userbase from under 1,000 active users to over 160,000. These apps are spread across 6 of the 7 groups of downloads identified in AppBrain's `Download distribution of Android apps'~\cite{appbrain_download_statistics_june_2019} and similarly 5 of the 7 groupings representing over 94\% of the distribution of downloads in Google Play according to Wang ~\emph{et al} (2018)~\cite{wang2018_beyond_google_play}.

\textbf{MUST-DO} answer the following question: What exists in the literature, common practices, vs what I was able to achieve. \emph{From a question raised by Alistair Willis, OU, 30 April 2020.}

\subsection{How many developers are enough to ask?}
On of the key considerations for research is adequacy in terms of coverage. For my research there are several types of coverage, including: development teams, user-bases for the various apps, software tech stacks used (in terms of programming languages, analytics libraries, etc.), application domains, and so on. 

c.f. Krug is a well respected Usability guru whose work is inherently practical in nature. In the first edition of his~\emph{``Don't make me Think"} book he discusses ways to obtain practical results even with short timescales and few resources. In terms of obtaining value the author indicated that 3 to 4 people were capable of delivering more relevant feedback by involving them over time, (Chapter 9 in ~\cite{krug2000dont_make_me_think}). In terms of selecting the candidates his recommendation was to worry less about selecting 'representative users', instead\emph{``Recruit loosely, and grade on a curve."} (Chapter 9 in ~\cite{krug2000dont_make_me_think})~\footnote{Note: Krug made several chapters, including this one available online when the second edition of the book was published. I have copies of all three versions of the book and of these chapters as PDF files.}.

% exploratory personas.
% GTAC conference talk about developers not having engineering degrees (The one Isabel went to Mr Tupule, Googler.)

My research included working with two mature project teams and developers of three commercial apps. It is also based on work I did in industry that predates the PhD research, unfortunately I am not able to provide details of those projects in my thesis. 


\subsection{Validation of the concepts}
\textbf{MUST-DO}: Revise to reflect the recently added case studies. 

My practical research includes on two sets of opensource Android applications, those of the Kiwix and Catrobat project teams. According to data and reports Google provides the development teams their active user-bases are 362,595 for the Kiwix project across 18 published apps, and 148,966 for the Catrobat project across 6 published apps. %data obtained on 16th May 2020.
TODO map these apps to the buckets in the table from the 'beyond Google Play' paper.

While these apps include a useful variety of user populations (from 10's of users to 150K+ across many countries and tens of apps) they could be perceived as a \emph{drop in the ocean} of the millions of apps currently available in the Google Play app store. Also, both project teams are non commercial, and may have different working dynamics from commercial development projects and teams. As my research was inspired from my consulting work with businesses who rely on the success of their apps I chose to supplement these two projects by engaging with developers from several commercial development teams. These include: Moonpig, Moodspace, and LocalHalo. Each values and uses analytics data during their development process to assess post-launch issues with their apps. From time to time things go awry with the behaviours of one or more of their releases and analytics helps them to identify and respond to issues before they become pervasive. For the LocalHalo app, \emph{TODO add details}... For Moonpig \emph{TODO add details}...

\subsubsection{Validation by the Google Engineering Team}
In Spring 2019 I reported various flaws or potential anomalies in various reports Google Play Console provides to developers to the then Product Manager for Android Vitals, Mr Fergus Hurley. As the long-term product owner he has extensive and insider experience of the tools and reports Google provides to an estimated population of over 1 million Android developers \emph{TODO add references e.g. to the Beyond Google Play paper and the one about a few developers creating an exponential number of apps}. I asked for his perspective during both a long in-person meeting and a follow-up video call a few weeks later. He confirmed several of the issues and debated others. He was willing to go on record in one of my accepted peer-reviewed papers on the topic \emph{TODO add link} and asked me to continue to share my findings with them. During the next 12 months he and then they added more Google staff to the discussion and asked me to write up my findings in a document that became over 30 pages long. Their policy means they are unlikely to confirm changes they make as a result of my research and findings, nonetheless they accept and value the feedback that has been provided. They also confirmed various bugs were ones they want to address.


\section{Summary of evaluation of the research}


\section{Fieldstones for this chapter to be used where appropriate}
The following material includes various notes to guide the structure and writing of this chapter. Some are actions, others are notes to be incorporated into the thesis. 
% Software without boundaries. Bashar paper, via Yijun 01-Jul-2021

\textbf{Actions:} 
\begin{itemize}
    \item Remove the case study specific materials from this chapter. Echo / summarise/draw on what was presented in the case studies. 
    \item Refactor the current contents as it's moved around.
    \item This chapter should aim to close the questions. 
    \item Note: small case studies are unlikely to be major contributions to this chapter.
\end{itemize}



Do I have a methodological perspective I'd like to present here?
e.g.
% Methodological perspectives:
\begin{itemize}
    \item Top down approach? (This might apply if I were involved in designing an analytics service to help developers improve the reliability of their apps.)
    \item Crash reduction plan? (A common perspective for the projects I've worked with.) 
    \item Collect everything? analyse later? (An exploratory approach where the methodology emerges with experience.)
\end{itemize}


There's an opportunity to discuss limitations in current analytics infrastructure. Discussion for acceptable technical debt (c.f. countries debt burden as percent of GDP)


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\par\noindent\rule{\textwidth}{0.4pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Placeholder notes \nth{6} July 2021 on}
Stuff to integrate as this chapter is rewritten includes:

Various ideas described in~\cite{kinshuman2009_debugging_in_the_very_large, kinshuman2011_debugging_in_the_very_large} e.g. on statistics-based debugging. See the annotations on the printed copy of~\citep{kinshuman2011_debugging_in_the_very_large}. 

\emph{``An early mantra of our team was, ``data not decibels." Programmers use data from WER to prioritize debugging so that they fix the bugs that affect the most users, not just the bugs hit by
the loudest customers."}~\citep{kinshuman2009_debugging_in_the_very_large}. 

Microsoft has a relatively large pool of users for pre-releases (ITU releases) unlike most app developers therefore their approach may not be viable for app developers who may need to rely more on post-release analytics.

Platform providers who collect platform level analytics (e.g. Google Android, Huawei Harmony OS, Apple iOS, etc.) have insights into not only the behaviours of the application code, they also have data on the stability of libraries and could use this data to help both library developers and app developers to address flaws related to the libraries being used in apps.

Microsoft uses the term bucket while Android Vitals uses the term cluster as a container for grouping together failures considered to be common according to their respective algorithms. The Microsoft WER papers discuss various flaws in their bucketing algorithm and yet the pragmatic and productive results they obtain despite these flaws.

According to Microsoft, users have to opt-in and explicitly grant Windows permission to send reports to Microsoft. More recently Microsoft removed the ability for end users to opt-out of sending diagnostics data eg.. from 2015~\url{https://www.computerworld.com/article/2968288/windows-10-makes-diagnostic-data-collection-compulsory.html} and from 2021~\url{https://redmondmag.com/articles/2021/03/03/reclaim-privacy-from-windows-2.aspx} which mentions the ability for end users to at least see and delete the data that's collected \emph{``Microsoft not only allows you to delete the diagnostic data it has collected, you can also use the Diagnostic Data Viewer to see exactly what data has been collected."} NB the screenshot shows the viewer entails up to 1GB of storage! Interestingly in~\citep{kinshuman2009_debugging_in_the_very_large} they mention Microsoft provides corporate administrators with the ability to collect the data on a private server and to control what information is sent to Microsoft. It's called `Corporate Error Reporting' (CER).

Statistics-based debugging compare and contrast with analytics-based debugging.

MSFT measure deployment using WER. \emph{``Finally, both Microsoft and a number of third parties use the WER database to check for regressions. Similar to the strategies for measuring deployment, we look at error report volumes over time to determine if a software fix had the desired effect of reducing errors. We also look at error report volumes around major software releases to quickly identify and resolve new errors that may appear with the new release."}~\citep{kinshuman2009_debugging_in_the_very_large}.

WER seems far more dynamic in terms of data collection; and it also aims to provide recommendations to end users to obtain fixes~\citep{kinshuman2011_debugging_in_the_very_large}.

\emph{``Given finite programmer resources, WER helps focus effort on the bugs that have the biggest impact on the most users."}~\citep{kinshuman2009_debugging_in_the_very_large}. Similarly, Android Vitals and other crash analytics helps developers focus on the bugs that have the biggest impact on the most users.

\emph{``Bucketing Effectiveness"} Secondary (etc.) buckets/clusters were observed in the three main analytics services~\footnote{Android Vitals, Crashlytics, Microsoft App Center} studied during this research. None of the development teams systematically checked all the contents of all the buckets/clusters and the ephemeral nature of the reports and empirical nature of the research meant the error rate of the bucketing/clustering has not been calculated. (Microsoft in comparison published their analysis of the effectiveness of their bucketing algorithm in~\citep{kinshuman2009_debugging_in_the_very_large}.) Flaws in the bucketing/clustering reduce the likelihood app developers will maximise their effectiveness in terms of addressing the most frequent and pervasive bugs.

\emph{``While not ideal, WER's bucketing heuristics are in practice effective in identifying and quantifying the occurrence of errors caused by bugs in both software and hardware."}~\citep{kinshuman2009_debugging_in_the_very_large}. Similarly, while the bucketing/clustering wasn't ideal in any of the analytics tools studied in this research, the developers of the apps were able to address many of the issues being reported, and the reasons they did not address issues was not related to flaws in the grouping of the underlying failures.


\emph{``Results from the development of Windows Vista, mentioned in Section 1, suggest that present static analysis and model checking tools will find at least 20 bugs for every one bug found by WER. \textbf{However, the bugs found by WER are crucial as they are the bugs which have slipped past tools in the development cycle.}"}~\citep{kinshuman2009_debugging_in_the_very_large}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\par\noindent\rule{\textwidth}{0.4pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
