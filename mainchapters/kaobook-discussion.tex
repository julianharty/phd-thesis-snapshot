\setchapterpreamble[u]{\margintoc}
\chapter{Discussion}~\label{chapter-discussion}

\epigraph{Every honest researcher I know admits he's just a professional amateur. He's doing whatever he's doing for the first time. That makes him an amateur. He has sense enough to know that he's going to have a lot of trouble, so that makes him a professional.}{Charles Franklin Kettering (1876-1958)}~\footnote{Epigraph, with thanks to Zieris's PhD thesis for this quotation\cite{zieris2020_phd_qualitative_analysis_of_knowledge_transfer_in_pair_programming}.}

% The discussion chapter was shaped by (ab-)using an affinity diagram process. Here's a link with comment accesss to the MIRO board https://miro.com/app/board/uXjVOpTlE1E=/?share_link_id=149030074901 
% Similarly a draft of the new material is available in Google Docs for my collaborators using their Google accounts. {\small\href{https://docs.google.com/document/d/1tdmyTwoFuxMYJNMBw7yWxmpRDVeOuQ4yReNKQKcq9GM/edit?usp=sharing}{docs.google.com/document/d/1tdmyTwoFuxMYJNMBw7yWxmpRDVeOuQ4yReNKQKcq9GM/edit?usp=sharing}}

The previous three chapters explored the findings from the app-centric and tool-centric case studies in relation to the six perspectives of this research. Specifically, the findings relevant to the software development process aspects of using mobile analytics, identified motivations, real-world factors, competencies and optimisations, and benefits and achievements. Further, the exploration of the effects of using analytics on the apps themselves and associated artefacts discussed findings relating to the mobile app code, the apps themselves, and bugs, in addition to some broader topics of concern. Finally, the findings relating to the mobile analytics tools described how the case studies inform the design of these tools, their fitness-for-purpose, utility and dependability. 

Several of the discussion topics are more general in nature, for instance on privacy and on informed consent by users, and so they are covered in this chapter. We start with a general discussion of the socio-technical perspectives of mobile analytics. The subsequent sections are organised as follows: a critique on this research followed by a discussion of the utility of mobile analytics and scalability of the findings of this research to other app stores. %mobile software development contexts. 
This is followed by a discussion of how to empower users to participate and developers to improve their practices, which helps us consider two of the key populations of people who are most affected by the use of mobile analytics. Next we consider improvements in the analytical capabilities of the tools. Finally, there is a discussion of some of the safeguards needed for effective use of analytics and techniques for mitigating some of the challenges the affect effective use.


\section{Mobile analytics is a socio-technical system}
Mobile Analytics is a socio-technical system in which both the technical and the social aspects need to be addressed both to maximise the good effects and to minimise the fruitless and bad effects of using these systems~\sidecite{baxter2011_socio-technical-systems-from-design-methods-to-systems-engineering}.%\todo{Consider critiquing this paper here.}

There are several cross-cutting concerns with using mobile analytics: trustworthiness holistically with an additional focus on the ethical and legal aspects which are topics developers seldom get directly involved in the software they use and produce impacts, or interacts with, both of these aspects.

Studying a mobile analytics system is particularly challenging as there are few opportunities to observe the system in-use, as intended, by the designers and by the developers who use the system. White-hat study requires permission to access and use the system particularly while it is in production-use. Further, permission to publish the findings is also vital. To the best of my knowledge, none of the providers of mobile analytics services encourages independent verification of their service. 

Few developers want to actively investigate flaws in a mobile analytics service; they use it with an implicit expectation it’s sufficiently OK to be worth using it where they can mostly `set and forget’ their use of the SDK. This behaviour was evidenced by the experiences in many of the app-centric case studies and by the analysis of 107 opensouce Android apps that use Firebase Analytics where 50 of the projects simply used the default/basic initialisation and the rest modified their calls to the SDK infrequently~\sidecite{harty2021_logging_practices_with_mobile_analytics}. The clear exception to the set and forget practice was \myindex{Moonpig} who actively and attentively modified their use of Firebase Analytics to log additional information when the development teams had concerns about the app (in practice the service) adversely affecting one or more users. Their attentiveness was rewarded by their Android app being more reliable than similar apps.

The richest and most complex perspectives are for the processes, \emph{i.e.}, how development teams use mobile analytics. As the mobile analytics software improves, there is tremendous potential to improve the use of mobile analytics in order to deliver high-quality mobile apps. To achieve ongoing effective use, development teams need to obtain sufficient opportunities to use the tools and the tools need to facilitate gentle and adaptive approaches that suit the developers’ current context. At the time of writing, Google’s recent enhancements that integrate Firebase Analytics into the Android Studio IDE provide a tantalising opportunity to start to observe, and potentially measure, the engagement and the results in terms of more bugs being fixed and those fixes being made more quickly than developers did previously. 


\section{Critique on this research}~\label{discussion-critique-on-this-research}
Of necessity this research has been opportunistic, it has taken advantage of opportunities that arose for instance discussions with Google's Engineering team for Android Vitals emerged when I reported various flaws that were found during the Kiwix and Catrobat case studies. The work with Iteratively and subsequently Amplitude happened because the founders of Iteratively sought out people recognised as leaders in mobile analytics and my early publications led them to me. They then offered to share their experiences and findings in order to assist with the research, \emph{etc. etc.}. The research has applied a systematic approach to the various app-centric and tool-centric case studies that combine a mix of piecemeal and \emph{ad hoc} work. As Alan Hodgkin notes when introducing his auto-biography ``he believes that chance often plays as large a role as design in scientific discovery''~\sidecite[][inside front cover]{hodgkin1992_chance_and_design}, to a certain extent this research also depended on the effects of chance and of opportunities taken. Also, in writing this thesis I have done my best to not ```clutter up'' the literature with irrelevant personal reminiscences'~\sidecite[][p. 1]{hodgkin1976chance}; that may come in a future publication of this work for practitioners in industry. 

\subsection{On Measurement}
Ultimately mobile analytics is predicated with an acceptance that it is possible to measure pertinent and salient items through the use of software that collects data and other software that analyses that data in aggregate form.

Measurement may begin using rough and approximate measurements and tools, for instance the length of a yard which used to depend on the span of the king's arm~\footnote{One source is:~\url{http://nisltd.co.uk/asp/default.aspx?page=history_of_calibration}}. France, in particular, led to the standardisation of various measurements including the metre~\footnote{\emph{e.g.}~\url{http://www2.culture.gouv.fr/culture/actualites/celebrations/metre.htm} (in French).}. 

Software development and testing are still in flux where various people and groups have yet to coalesce or truly agree to common, unambiguous and definitive measurements. Even newer standards, including~\sidecite{iso29119-1-2013}, which were intended to provide a pragmatic and useful guide to practitioners \sidecite{reid2012_iso29119_eurostar}, would only be used by a minority (19\% according to a poll by EuroSTAR Conferences in 2013~\footnote{\href{https://conference.eurostarsoftwaretesting.com/poll-result-will-you-be-using-iso-29119-standards-in-your-testing/}{conference.eurostarsoftwaretesting.com/poll-result-will-you-be-using-iso-29119-standards-in-your-testing/}}) of an estimated 60 respondents to the poll~\footnote{\href{https://conference.eurostarsoftwaretesting.com/standards-a-case-for-the-defence/}{conference.eurostarsoftwaretesting.com/standards-a-case-for-the-defence/}}.

Mobile analytics has reached the point in terms of many of the elements that are measured but not necessarily correlated those measurements with the effects on business and project goals. My research into the \emph{use} of mobile analytics tools led to research into the characteristics of several of the actual tools in widespread use by developers of mobile apps. The measurements and assessments of these tools is immature and my work provides a possible starting point to enable these and other analytics tools to be measured and assessed.

\subsubsection{Inaccuracies in software analytics generally}
Other inaccuracies in various analytics tools provide a backdrop to this research in mobile analytics. For instance, from the domain of ecommerce web sites, a startup, \href{https://www.littledata.io/}{Littledata}, claims 80\% of Shopify merchants are missing at least 20\% of transaction data for those that use Google Analytics to track their transactions and revenues~\sidecite{littledata2020_google_analytics_doesnt_match_shopify}. The company provides software which claims to provide 100\% accuracy and offer their customers a 30-day free trial to enable a three-way comparison between Google Analytics, Shopify's standard tracker, and their `solution'. Unfortunately for them, the incomplete and misspelt eBook waters-down reduces the confidence in their claims of providing 100\% accuracy. Also, even if two of the systems agree on the transactions and finances these systems are not necessarily correct. Additional evidence, for instance, of all the source events would be needed to cross-verify that what any of the systems report actually matches the inputs.

\subsubsection{Seeking adequacy in mobile analytics}
In~\sidecite[][p. 3]{hubbard2014measure} the author claims that anything is measurable. In practice this research has both investigated and applied aspects of a four-point list~\sidecite[][p.7]{hubbard2014measure} reproduced here for ease of reference %[p. 7](\emph{ibid})

\begin{enumerate}
    \itemsep0em
    \item ``\emph{Decision makers usually have imperfect information (i.e., uncertainty) about the best choice for a decision.}
    \item \emph{These decisions should be modeled quantitatively because (as we will see) quantitative models have a favourable track record compared to unaided expert judgement.}
    \item \emph{Measurements inform uncertain decisions.}
    \item \emph{For any decision or set of decisions, there is a large combination of things to measure and ways to measure them - but perfect certainty is rarely a realistic option.}''
\end{enumerate}

The research demonstrates developers are able to achieve material improvements to the quality of their mobile apps when they choose to pay attention to mobile analytics even when the tools and the practices are flawed. An open question is the adequacy, or otherwise, in the currently available mobile analytics tools and services. They are able to help developers make some decisions adequately, but not necessarily to make an optimal or prioritised set of decisions. 

\subsubsection{Some limits of what can be measured}
Together with considerations on fidelity (which are discussed later in this chapter), we must also consider the limits on what the client-side \Gls{sdk} is able to measure, because what it doesn't measure is not going to be reported on by the mobile analytics. 

There are areas where the runtime can prevent some pertinent information being collected by some \Glspl{sdk}, for example: \myindex{React Native} runtime - within runtime crashes vs. application crashes where the runtime container crashes, which applied to two of the app-centric case study apps: (\myindex{LocalHalo} and Taskinator\index{GTAF!Taskinator}).

The other is where failures occur to early or too late in the runtime, for example crashes at startup as raised in the email correspondence with Google when comparing Android Vitals with Firebase Crashlytics.

\subsection{Some thoughts on measurements of reliability}
The measured reliability of the artefacts, here particularly of the application's binary, when it being used is often reported as a percentage. The absolute percentage of reliability scores is not necessarily a true reflection of the reliability of the app as the score is dependent on various factors including:

\begin{itemize}
    \item The settings on the device or in an app that gate (control) whether analytics is provided to the `mothership'\sidenote{The servers that receive the raw analytics-related data from devices and/or apps.}.
    \item The connectivity and transfer mechanism. For example Crashlytics only transferred crash data from previous sessions when the app was restarted. If the app was not restarted the data was never sent.
    \item Whether the ephemeral data is preserved on-device sufficiently to transfer it. Client-side SDKs generally have constraints on how much data they keep and for how long they keep it. Furthermore, adverse termination of the app might cause some or all of the data to be lost.
    \item the burstiness of aggregate incoming data, some service providers place limits on how much data they will accept and process. 
    \item Any filtering performed during transmission and/or reception of the data. As an example, Sentry supports inbound data filters~\sidenote{\href{https://blog.sentry.io/2018/01/03/delete-and-discard}{delete and discard}, \href{https://docs.sentry.io/product/data-management-settings/filtering/}{filtering}, and data \href{https://docs.sentry.io/product/data-management-settings/scrubbing/}{`scrubbing'}}
    \item Bugs in the SDK that affect the storage and/or transmission, \emph{e.g.} as occurred in Google Analytics for Android on a specific version of Android/Google Play Services \sidecite{google2014_google_analytics_exceptions}.
    \item Any filtering in the reporting.
    \item When and where the failure occurred. Timezones, daylight settings from source to destination, any reporting cutoffs (and also any data from an earlier period that arrives after the cutoff for the report generation), contractual constraints~\sidenote{Including the date in the month, \emph{e.g.} see \href{https://help.sentry.io/account/billing/what-happens-when-i-run-out-of-event-capacity-and-a-grace-period-is-triggered/}{Grace periods when capacity is exceeded}.} \emph{etc.} might affect recent and/or historical reports.
    \item The date and time on the device (and perhaps on intermediate equipment).
    \item Whether the failure conditions occur.
    \item Other aspects including processing failures, problems with cloud services, DDoS attacks, and potentially many others.
\end{itemize}

All these factors mean that it is better to treat the counts, percentages, improvements/degradations that are presented in reports as indicative values rather than absolute values. In other words, it may unwise to simply compare absolute values to determine 'better' and 'worse' in terms of the quality of the actual app. Confidence intervals are worthy of consideration.\sidenote{In some ways we're getting the equivalent of spot prices of how that release, etc. fared that period.} 
% Provide examples from Kiwix (WebView) and the large industrial case study. 
% c.f. https://gradcoach.com/nominal-ordinal-interval-ratio/ This point is probably worth expanding on

Probability has a part to play in terms of comparing measurements and reports in order to compare results and estimate the likely trajectory of progress in terms of any improvement or detriment. Of these, detriment may be easier to manage based on several factors:

\begin{itemize}
    \item The presence of failures are easier to identify than the absence of a failure (given some failures may not be visible in reports for various reasons mentioned previously).
    \item Developers are unlikely to want to ignore detriments if they are paying attention to the quality of their app.
\end{itemize}

\newthought{Measuring reliability: }
Failure rates include: un-handled exceptions, \Glspl{anr}, and errors. Errors include: handled exceptions that are recovered from, caught exceptions, logic errors, calculation and content errors, and many others. 

Even \emph{if} an app catches and 'handles' every exception it may not be truly reliable \emph{i.e.} there may be various errors that occur that do not lead to a runtime exception or an ANR. As such, the measured failure rate - assuming it is accurate or under-counting - is the lower bound of the true error rate.

\newthought{Gaming the system: }
In 2020 Weckert, an artist became popular for an experiment he performed using 99 smartphones, each running Google Maps, where he was able to trick Google Maps into showing traffic jams on otherwise free-moving streets~\sidecite{weckert2020_google_maps_hacks}. In \sidecite{businessinsider2020_artist_creates_fake_traffic_jams_on_google_maps_with_wagon_of_phones} Weckert explained he wanted to draw attention to the blind trust people have in tech companies and platforms. Google Maps and Mobile Analytics services share similar data collection, analysis, and reporting and it may also be possible to trick mobile analytics and feed data that biases the results of the reports.
% See also
%%%%% https://www.theguardian.com/technology/2020/feb/03/berlin-artist-uses-99-phones-trick-google-maps-traffic-jam-alert
%%%%% https://www.theguardian.com/technology/2015/sep/03/google-sued-paper-towns-rip-off-claim-phantomalert-waze
%%%%% The Power of Virtual Maps


\subsection{Considerations on the methodology and case study procedure}~\label{discussion-on-methodology-and-case-study-procedure}


% \subsubsection{Considerations on the methodology}~\label{discussion-considerations-on-the-methodology}
\newthought{Considerations on the methodology}: 
One of the largest challenges in terms of applying the methodology was on restrictions, real and assumed, in the systematic, timely, efficacious, and ongoing collection of data. The restrictions were particularly onerous in terms of collecting the analytics artefacts from Google Play Console and Android Vitals, however the effects also applied to other proprietary mobile analytics tools. There challenges were practical, risk-based, and ethical. 

\newthought{Power imbalances}: 
Power imbalances are immense between Google (and other major platform providers) and any developers/researchers interested in investigating the behaviour of the platform's mobile analytics services.
%
The depth and the breadth of Google's influence is vast in online services, in app development and sale, and in billions of people's lives generally (including those of many researchers). As Jacobides, \emph{et al.} noted in~\sidecite[][p. 2270]{jacobides2018_towards_a_theory_of_ecosystems}, \emph{``most ecosystem members are complementors (e.g., in July
2014 there were 2.3 million individuals working as app developers), with very limited power''}. While I have some reservations about his calculations, through my research I concur with his observation on app developers having limited power. %Pon's article does not provide the details he stated here and attributed to Pon.

The ongoing availability and use of these services are subject to Google's benevolence, and while policies and similar legal documents provide some indications of Google's requirements they do not address specifics and the Google engineering team chose not to answer requests to perform systematic evaluation of their service. This left aspects of the planned research in limbo. There have been numerous, sporadic, issues where Google has withdrawn access not only to an individual but to people and organisations Google decides are related to an individual. This led to a loss of income and even livelihood for some app developers. And the appeal process has been opaque and seemingly devoid of human intelligence.  (\emph{e.g.} \sidecite{martinez2019_google_just_terminated_our_startup_google_play_publisher_account_on_xmas_day, dodson2019_google_completely_terminated_our_new_business_etc, marcher2021_how_google_terminated-a-developer, stackoverflow2021_how_to_get_in_contact_with_google_play_to_challenge_an_app_rejection}). This research was therefore cautious in nature which is particularly key as the author has connections with multiple organisations who have popular apps in the Google Play Store. 

There is the potential for adverse real-world consequences of not having systematic, timely, efficacious, and ongoing collection of data. These include, missing pertinent information, incomplete data, an opportunity-cost (where spending time and resources devising ways to collect the data diverts from actually using, \emph{i.e.} applying, all the relevant\footnote{TBD whether to focus on ideal, relevant, sweetspot, or net-present-value, etc. of data.} data!). 

Express mechanisms and permission for authorised users to collect analytics artefacts would provide clarity and materially improve the ability of authorised researchers and authorised members of development teams to obtain, use, and preserve the analytics artefacts. Furthermore, the provision of suitable APIs would facilitate higher-speed and more efficient collection of the data. Microsoft's App Centre is currently one of the leading mobile analytics offerings in their provision of both cloud storage mechanisms and APIs to access and preserve analytics artefacts. While these do not include mechanisms to obtain the rendered (\emph{i.e.} graphical) reports, at least the underlying data can be collected, analysed independently and even integrated into subsequent, downstream processing.  


%\subsubsection{Considerations on the app-centric case study procedure}~\label{discussion-considerations-on-the-app-centric-case-study-procedure}
\newthought{Considerations on the app-centric case study procedure}: 
The app-centric case studies provided a useful variety of projects, apps, and business domains. Additional app-centric case studies may well increase the weight of at least some of the evidence and also provide new examples and insights - they are therefore desirable. Nonetheless, they do not negate the benefits of systematic testing and interrogation of mobile analytics services. This research was augmented by some small-scale and limited systematic testing and the results may be published at a later date since the findings did not massively disrupt the findings being published here.

\afterpage{\clearpage}

\subsection{The impact of COVID-19 and other unavailability}
The emergence of \myindex{COVID-19} materially affected several of the app-centric case studies and some planned research. It also adversely affected the working practices of various development teams.
Examples of the disruption include:

\begin{itemize}
    \item Catrobat\index{Catrobat}: the workshop planned in Poland to evaluate the use of mobile analytics to help software testers test the Pocket Code had to be abandoned.
    \item Catrobat: Planned follow-up work to integrate and evaluate Firebase Analytics in Pocket Code for both their Android and iOS apps was abandoned owing to knock-on effects on the project team in Graz, Austria. 
    \item GTAF\index{GTAF}: the main contact was unwell and unavailable for a month or so during the active period. Thankfully, having ongoing access to Google Play Console has enabled a longer-term analysis of the behaviours of their Android apps, nonetheless communications and collaboration were both abruptly interrupted.
    \item Commercial App project: much of the work was planned to be on-site in another country working directly with the development team, however there were no commercial flights available to that country for the duration of the active case study. Working remotely meant communications were more stilted and the work progressed more slowly than it might otherwise have done. 
\end{itemize}

\afterpage{\clearpage}
Despite the effects of these disruptions the research that did happen was productive. 

\subsection{Threats to Validity}~\label{discussion-threats-to-validity-section}
In the Methodology chapter, threats to validity of the methodology and the use of empirical studies were discussed (see page \pageref{methodology-threats-to-validity-section}). This section broadens the scope to include additional threats to the validity of the research.

Let us consider validity from a practical, pragmatic perspective. 
Research by ~\sidecite{scaffidi2007developing}, states real world developers and users base their decisions on what the paper describes as low-ceremony evidence~\emph{``such as reviews, reputation, advertising claims, qualitative information, or aggregation of group opinion"}. The paper proposes a notation that includes credentials and provenance to help people to systematically adapt their their confidence in software dynamically as new information emerges. Making decisions with imperfect information is also covered in the same paper (~\sidecite{scaffidi2007developing}) where they discuss \emph{good enough} decision making using \emph{``less-than-perfect information"}. In my industry experience, and in the research described in this thesis, their claims hold true. And in terms of my research I have aimed to assess aspects of the credibility of several analytics tools and provide provenance for the evidence that has been collected during the practical aspects of the research.

% TODO Using ratings and reviews to measure quality? Tim Menzies quote on software analytics. 


% \subsection{Internal validity}

% \subsection{External validity}

\subsection{Ecological validity}
As Wikipedia notes \emph{``Essentially, ecological validity is a commentary on the relative strength of a study's implication(s) for policy, society, culture, etc."} (\sidecite{wikipedia_ecological_validity}).

One of the aims of my research is to determine whether it is applicable for and relevant to real-world developers of mobile apps. 

Working with non-profit opensource projects, where the teams are generally willing to make their practices and results public, helps with being able to obtain information and publish the results. However, as many research projects discover, what might work for opensource projects -- while important and interesting to the research community -- might not matter much to the industry of professional and commercial development teams. 

Opensource apps are a tiny proportion of mobile apps available to users, so another level of learning and validation could be achieved through the insights of these professional and commercial development teams. Surveys tend to have poor response rates and lack the depth or richness I was seeking therefore I chose to engage at a deeper level as a fellow developer interviewing developers of several apps. Of course we would need the permission of their organisations, and some shared examples of their tools, their practices and their results of applying analytics well, and even times when they hadn't.

I have been fortunate to receive confirmation from various external sources that the research is of interest and appears to have some validity. These include: validation from the Google engineering team responsible for Android Vitals and Google Play Console; and developers of mature opensource and commercial Android applications. 

One of the challenges in this research has been to balance internal, external and ecological validity, a challenge others have faced in their research in other areas including educational software~\sidecite{ransdell1993educational_software_evaluation_research_validities} where they realised there are multiple factors that influence the outcomes and results of their experiments and research. Similarly, real-world development of mobile apps, and changes in the stability of their apps depend on multiple factors. And from a research perspective, it was, and is, impractical to observe or measure everything that might contribute to the changes in stability, etc. What does appear to be clear are two related factors:

\begin{enumerate}
    \item When developers pay attention to flaws reported in analytics tools they are able to effect improvements to the app which significantly reduce the failure rate and improve the stability of the apps for end users.
    \item Developers will release updates that unintentionally and or exacerbate the failure rate, despite their best intentions. They need to pay ongoing attention to the reported failures if they wish to maintain or decrease the measured stability of their apps as both their app and the ecosystems evolve~\footnote{Ecosystems evolve as new devices, operating system releases, networks, updates to libraries, and to other related apps and components (such as the Google \href{ection-webview-component}{WebView component}), etc. change.}.
\end{enumerate}

\newthought{Validity from the Google Engineering Team: }
The sustained level of active interest by the Google engineering team responsible for Google Play, including Google Play Console and Android Vitals was a surprise. It is a pity they chose not to elaborate on how they used the material they received from this research, nonetheless the lack of elaboration is consistent with the company's practices generally. The fact they requested ongoing updates and provided continuity of communications as and when their management changed is an indication they valued the material and it is likely they applied at least some of the findings about flaws in their service and similarly in recommendations that were made during the research.

\begin{comment}
%\subsection{Other validation}
In email discussions of my research in 2020 one of the leading authors in the field, \href{https://scholar.google.com/citations?user=zuUsFkgAAAAJ&hl=en&oi=sra}{Li Li}, confirmed the novelty, importance and relevance of my research.    
\end{comment}

\section{Utility}~\label{discussion-utility}
If we focus on Android apps what are the implications and how relevant are the approaches and results? In every one of the app centric case studies the developers were able to use the outputs of mobile analytics to successfully find and fix crashes that caused their app(s) to fail in use. The projects ranged from sole-developers to 100+ person teams at a major enterprise. The mobile analytics ranged from purely relying on what the app store/platform provided through projects that included in-app crash and error reporting, to those who incorporated multiple mobile analytics SDKs. Generally, those teams that included at least one mobile analytics SDK found they preferred using that mobile analytics service in preference to the platform provided analytics in so far as the mobile analytics service provided similar information. The possible exception was the GTAF\index{GTAF} project who weren't able to provide sufficiently specific answers. 

Where the in-app analytics service lacked particular types of failure, ANRs\index{ANR} is the key example, then the teams also used Android Vitals from time to time. Moonpig demonstrated the benefits of ongoing checking of both the platform-level and in-app analytics, rather than preferring one over the other.

Based on these results, on personal pre-research examples, and from examples drawn from grey materials, using mobile analytics appears to provide utility to many development teams. % and therefore to scale beyond the particular instances studied in the app-centric case studies.

Mobile Analytics has also demonstrated it can measure the effects of more strategic improvements as well as successfully measuring individual fixes. Two examples follow:
 
\newthought{Strategic improvements in code quality:}
Technology companies have adopted several approaches which may improve code quality, including the reliability of mobile apps. In particular `shift-left'~\sidenote{See \href{https://devopedia.org/shift-left}{devopedia.org/shift-left} for a clear overview.} encourages moving various development activities earlier in the software development lifecycle. According to Google \myindex{Kotlin} results in 20\% fewer crashes in the top 1000 Android apps that used Kotlin compared to those written in Java~\sidecite{googleblogs2021_androids_kotlin_first_approach, muntenescu2020_fewer_crashes_and_more_stability_with_kotlin} which is one way to shift-left by obviating some of the causes of crashes \emph{i.e.} poor programming practices. Kotlin requires developers to be explicit in the nullability of variables and parameters so some of the issues only found at runtime in Java code is found at compile time when Kotlin is used. As a concrete example, the developers of the Google Home app were able to reduce \texttt{NullPointerException}'s by a third through migration from Java to Kotlin~\sidecite{googleblogs2020_google_home_reduces_crashes_by_a_third}.
% And see 
% - https://en.wikipedia.org/wiki/Shift-left_testing (weak so not cited)
% Alan Richardson on shift-left is not shifting-left in software testing  instead it's starting the work earlier in the software development process. https://devopedia.org/shift-left and https://www.youtube.com/watch?v=AaMp5skiwqA
% Bjerke-Gulstuen K., Larsen E.W., Stålhane T., Dingsøyr T. (2015) High Level Test Driven Development – Shift Left. In: Lassenius C., Dingsøyr T., Paasivaara M. (eds) Agile Processes in Software Engineering and Extreme Programming. XP 2015. Lecture Notes in Business Information Processing, vol 212. Springer, Cham. https://doi-org.libezproxy.open.ac.uk/10.1007/978-3-319-18612-2_23 however it's a fairly mundane - provide a dedicated tester earlier in the dev't lifecycle, albeit the results seem to be productive according to their slidedeck on semanticscholar.


In research in code quality for Android apps, in the work of \sidecite{Hamdi2021empirical} the focus was on internal code qualities (\emph{i.e.} coupling, cohesion, complexity, design size, and inheritance) rather than on qualities in use. Their group of qualities are all internal qualities of the code, rather than quality-in-use metrics. 

While developers of the apps in the app-centric case studies may have performed some refactoring as defined by ~\sidecite{Hamdi2021empirical}, the amount is unknown. As such refactoring was not mentioned or observed in the app-centric case studies we can assume it was not explicitly performed and that it wasn't performed more than before or after the active case study period. Therefore, improvements in reliability of the apps did not stem from explicit code refactoring work~\sidenote{Some code quality improvements may well have been performed as part of bug fixes, however the primary purpose of the changes were to fix causes of failures.}.

\newthought{Aim to prevent errors rather than detect them using mobile analytics:}. % TODO: Add link to Countly's reporting service. 
Developers may have opportunities to improve their software more holistically, for example to obviate a type of mistake or flaw in their working practices.  Mobile analytics could facilitate this by grouping causes of failures, for instance by the type of Exception thrown, and help find ways to obviate those Exceptions being generated in practice. Techniques such as improving development practices, explicit training and mentoring of the development team, \emph{etc}. in addition to any changes in programming languages may help achieve this aim. %
Crash analysis, such as the research performed in \sidecite{su2020_why_my_app_crashes_etc_android_framework_exceptions}, focuses specifically on exceptions specific to the Android framework; however their approach also extends to exceptions in third-party libraries and in the application code. Mobile Analytics would be one of the ways exceptions are detected (their third research question). Furthermore, their two prototype tools: \href{https://github.com/tingsu/Stoat}{Stoat} and \href{https://github.com/crashanalysis/ExLocator}{ExLocator} might help with bug localisation and crash reproduction.

%%%%% And see https://developer.android.com/topic/performance/vitals/crash#prevent-crashes-null-pointer
The Kiwix project chose to migrate code from Java to \myindex{Kotlin}, in part to prevent some errors (including errors that led to crashes) from occurring. The Commercial project, similarly, was migrating code from Java to Kotlin sporadically.

A final brief point is the analytics needs to have low latency in order to be actionable. This research did not go into depth in terms of tradeoffs between latency and the value of the analytics, this might be a topic for future research.~\sidenote{A starting heuristic could be to explore whether there is a half-life in terms of various forms of mobile analytics.} There is research in another field, in research decisions in the field of human health economics, which considers the expected value of information (EVI), this research might also be of use~\sidecite{philips2008_the_half_life_of_truth__what_are_appropriate_time_horizons_for_research_decisions} when considering the value of information provided by mobile analytics.


\section{Scalability}~\label{discussion-scalability}
This research combines the results of various case studies where the use of mobile analytics was effective in helping app developers improve the quality of their mobile apps. Here we consider the scalability of the practice. 

\subsection{Do the concepts scale further?}
This research is limited to apps available in Google Play for practical reasons. Here we consider whether the approach could scale beyond Google Play for Android apps, and later whether the approach could work for other platforms.

Platform providers who collect platform level analytics (e.g. Google Android, Huawei Harmony OS, Apple iOS, etc.) have insights into not only the behaviours of the application code, they also have data on the stability of libraries and could use this data to help both library developers and app developers to address flaws related to the libraries being used in apps. Indeed with the launch of the SDK Index, \href{https://developer.android.com/distribute/sdk-index}{developer.android.com/distribute/sdk-index}, Google have recently done so; this extends their work with library developers where they provide them with crash statistics, \emph{etc.}~\sidecite{androiddevelopers2022_google_play_sdk_index}. They also provide app developers with various statistics on the top libraries~\sidecite{androiddevelopersblog2022_new_google_play_sdk_index_helps_you_choose_the_right_sdks_for_your_app}.

\subsubsection{Beyond Google Play?}
% According to https://www.businessofapps.com/guide/app-stores-list/ there were over 300 app stores (their source is no longer available)
% More info on Xaomi's market share https://www.enterpriseappstoday.com/stats/xiaomi-statistics-2022-market-share-demographics-and-news-update.html

Pon, \emph{et al} discussed how three companies: Google, \myindex{Amazon}, and \myindex{Xiaomi}, have developed their own platforms~\sidecite{pon2014_android_and_the_demise_of_operating_system_power_etc}. \sidenote{Their research predates ex-President Donald Trump and the furore of \myindex{Huawei} providing the infrastructure for mobile networks in the USA, UK, in particular, and the backlash against various Chinese companies who are heavily used in the `West' generally.}
This research has focused on Google Android's ecosystem, both Amazon and Xiaomi have app stores and these research findings may therefore be applicable to those ecosystems assuming suitable mobile analytics exists. 

Additional Android app stores are available, particularly in China as~\sidecite{wang2018_beyond_google_play} describes. Of the Chinese app stores, in 2018 only 2 (Tencent Myapp, and 360 Market) provided a quality rating. Their work indicates that at least some app stores are not likely to provide stability analytics similar, or equivalent, to those Google provides in Google Play Console and Android Vitals. Developers, therefore, would need to implement any analytics into their app rather than rely on the app store. Two more recent app stores are discussed next: Huawei and Amazon\sidenote{I was not able to obtain sufficient information on Xiaomi to usefully assess it.}

\newthought{Huawei: } 
\myindex{Huawei} needed a comprehensive alternative to the Google Android ecosystem, driven by the ramifications of the ongoing ban in the USA~\sidecite{androidauthority2021_the_huawei_ban}. 
% And https://www.cnet.com/news/huawei-ban-timeline-chinese-company-android-rival-coming-phones-tablets/
They have been ramping up their app store~\sidecite{androidauthority2021_huawei_app_gallery}, known as \href{https://appgallery.huawei.com/}{HUAWEI App Gallery}. According to~\sidecite{vodafone2021_huawei_appgallery}, it had over 180 billion downloads in the past year - which would infer lots of usage of those downloaded apps and very likely lots of crashes! 
%
In 2020, Huawei announced \emph{``HUAWEI is one of the world's top three application store"} [sic]~\sidecite{huawei2020_press_release_on_hms_ecosystem}, serving 600 million Huawei device users. In Spring 2021, Huawei stated they have over 2.7 million developers involved in their mobile ecosystem~\sidecite{sarkar2021_huawei_now_has_2700000_devs_etc}. 
And Huawei are rolling out their Android operating system HarmonyOS\index{HarmonyOS} to a wide range of their existing devices~\footnote{\href{https://www.huaweicentral.com/huawei-harmonyos-upgrade-plan-devices-and-rollout-time-list/}{www.huaweicentral.com/huawei-harmonyos-upgrade-plan-devices-and-rollout-time-list/}} which may accelerate the growth of activity in their app ecosystem. In short, they have a large and growing ecosystem with lots of apps and users of those apps.

Huawei provide an optional Analytics SDK for developers~\sidecite{huawei_analyticskit}, which provides a crash and error reporting SDK together with a Codelab~\sidecite{huawei_crashservice_codelab} and examples~\sidecite{huawei_crashservice_github_examples}. They encourage developers to test the crash reporting and their API includes facilities to enable and disable crash collection, to generate a test crash, to record Exceptions, and to report custom events~\sidecite{huawei_ag_connect_crash}. 
Their crash reporting appears to be integrated in an online console for developers~\footnote{\url{https://developer.huawei.com/consumer/en/console}}~\sidecite{huawei_introduction_to_appgallery_connect_crash_service}. 

They also provide a programmatic data export service~\sidecite{huawei_analyticskit_dataexport_codelab}. According to one of their developer-oriented articles, \emph{``The crash rate of a problem is greater than the threshold 1\%"}~\sidecite{huawei_introduction_to_appgallery_connect_crash_service}, which is similar to Google's bad behavior threshold of 1.09\%. In their AppGallery Connect Service whitepaper they mention crashes and ANRs in an image titled \emph{``Analysis: Driving Operations Decision-Making with Data"}, however the whitepaper did not provide any more details on how the information would be collected or provided~\sidecite{huawei_appgallery_connect_service_whitepaper}. 

% List of references for Huawei Analytics temporarily removed to see if doing so cures the Float(s) lost. The contents are in outtakes/DiscussionOuttakes.tex

\myindex{Huawei} have published a ``success story'' of how Doodle Draw used Huawei's crash service to detect and then resolve an \emph{``unexpected exit problem"} on the day they started using the crash service. The crash rate of the app was then \emph{``decreased from 1\% to 0.06\%, greatly improving the app quality"}~\sidecite{huawei_doodle_draw_success_stories_crash}. In 2021 they only had one success story, three more were published in 2022. Of these, the developers of Sleep Sounds were able to save between 1 and 2 days to locate crashes and resolve them `immediately'. By fixing the crashes more quickly user retention was increased by 15\% according to the success story~\sidecite{huawei_sleep_sounds_success_stories_crash}. 

It's implausible crashes were resolved immediately or that end users received the improved app immediately, nonetheless on the face of the success stories development teams were able to effect various improvements through using Huawei's in-app crash reporting.

Key differences between their provision and that provided by Google include the lack of integrated platform (device-level) analytics from the Operating System, the need for developers to integrate a full Analytics SDK in order to then use the crash and error reporting SDK. They do not mention of the equivalent of Android Vitals. In terms of pre-release checks and testing, they provide various elements that offer similar capabilities to the pre-packaged pre-launch reports that Google Play Console provides. For example, they have an Integration Check~\sidecite{huawei_appgallery_integration_check} which appears to perform various forms of static analysis and Cloud Testing~\sidecite{huawei_appgallery_cloud_testing} that in turn includes similar automated testing capabilities. 

As Huawei develops \myindex{HarmonyOS} and their app store, potentially platform-level analytics could be added and made available by Huawei in future.

\newthought{Amazon app store}: 
Another major app store for Android apps is the \href{https://developer.amazon.com/apps-and-games}{Amazon appstore}\index{Amazon}, which includes `millions of devices in over 236 countries and territories'. They provide an \href{https://developer.amazon.com/settings/console/home}{amazon developer console} which offers a subset of the Google Play Console's features together with some Amazon specific product and service offerings. 

Many, but not all, of Google Firebase Android SDKs are able to be used beyond Google Play. The other SDKs require Google Play services which is part of the Google Android platform. And uniquely, firebase-analytics states `\emph{automatic insights such as demographics are only available on devices with Google Play services.}'~\sidecite{firebasesupport2020_dependencies_of_firebase_sdks_on_google_play_services}. 

\newthought{Could the techniques improve reliability beyond Google Play?} 
These brief introductions into Huawei's and Amazon's, global app store ecosystems indicate they provide sufficient developer-oriented tools and services for developers to be able to measure reliability of their Android-like apps were they distributed in these app stores, and to be able to use the analytics offered by these providers (and potentially other analytics providers, for instance \myindex{Count.ly} claims their Android SDK should work on \myindex{HarmonyOS}~\sidecite{countly_which_operating_systems_are_supported}) to identify reliability issues and potentially address them. Neither of these ecosystems currently provide the richness of the \myindex{Google Play Console} or \myindex{Android Vitals}, they could choose to do so in future and the underlying Android based operating systems could gather similar data, at least from a technical perspective.


\subsubsection{Beyond Android to other mobile platforms?}

Apple also provides a platform and an app store (called the `App Store'). 

For iOS Apple describes how developers can collect crash reports from \myindex{TestFlight} and from the App Store. In the same article they describe how users can locate and email crash reports and email them to the app's developer, for instance if the developer does not have them. They do not explain how the developer or user reach each other to communicate about the crash or the contact details~\footnote{\href{https://developer.apple.com/documentation/xcode/diagnosing_issues_using_crash_reports_and_device_logs/acquiring_crash_reports_and_diagnostic_logs}{Acquiring Crash Reports and Diagnostic Logs}.}. 
%
Apple also provide practical advice on how to diagnose issues using crash reports, memory inefficiencies using `jetsam event reports' and problems using `device console logs'~\footnote{See Xcode documentation:~\href{https://developer.apple.com/documentation/xcode/diagnosing_issues_using_crash_reports_and_device_logs}{Diagnosing Issues Using Crash Reports and Device Logs}.}. 
% 
From various online discussions~\footnote{Examples of discussions on iOS crash logs include:~\href{https://stackoverflow.com/questions/10145665/crash-reports-from-app-on-app-store}{Crash reports from app on App Store}, and the poorly titled forum post ~\href{https://developer.apple.com/forums/thread/30934}{App Analytics Crashes}.} 
% See also https://stackoverflow.com/questions/50709109/ios-app-store-get-crash-reports
The iOS analytics include some of the reporting and analysis provided by Google in the Google Play Store tools, for instance~\emph{``...how crashes break down across OS versions and different devices"}~\sidecite{apple2020_how_to_review_your_apps_crash_logs}. Their reports currently lack some of the more sophisticated reporting that Google Play and Android Vitals provide. Nonetheless, sufficient raw elements are available to developers to enable them to discover crashes and memory issues from end users in order to address these. 

Apple also launched Feedback Assistant in 2019 with the aim of enabling people to submit effective bug reports using either an app or a website. Authorised members of the app's development team can access and review the feedback reports using similar tools~\sidecite{appledeveloper2020_bug_reporting_feedback_assistant_for_developers}.


\subsubsection{Applicability to other app stores?}
App stores were popularised for mobile apps for Android, iOS, and other smartphone platforms now extinct (\emph{e.g.} Windows Mobile), even if app stores existed several years before smartphones did. % COULD_DO cite early versions of the Mobile Developer's Guide to the Galaxy, etc.

% Isabel: add a figure across the domains, inter-domain considerations. See her scanned notes.

They have spread to other mainstream operating systems including \Gls{glossary-osx}\index{OS X}, some Linux distributions, etc. using a similar business and software development model to the Android and iOS app stores. Recently, e.g. in 2019, app stores were also reported for Radiology - for instance in an article in Harvard Business Review on \emph{What AI ``App Stores" will mean for Radiology}~\cite{hbr_what_ai_app_stores_mean_for_radiology}. The authors of the article envisage various benefits that emerge from applying a \emph{marketplace model} for these apps, including better feedback for developers. In tandem, the importance of incorporating appropriate analytics and ensuring they are trustworthy also seems worthy of further research and analysis. 

\subsection{A retrospective comparison with WER}
Research into \acrfull{wer}\index{Windows Error Reporting (WER)} has already been discussed in \secref{rw-software-analytics-tools-research}, here we revisit the topic in the light of my research.
% Various ideas described in~\cite{kinshuman2009_debugging_in_the_very_large, kinshuman2011_debugging_in_the_very_large} e.g. on statistics-based debugging. See the annotations on the printed copy of~\citep{kinshuman2011_debugging_in_the_very_large}. 



\newthought{Differences and Similarities}
Microsoft had a relatively large pool of users~\sidenote{Particularly employees} for pre-releases, unlike most app developers, therefore their exact approach may not be viable for app developers who may need to rely more on post-release analytics.

% Statistics-based debugging compare and contrast with analytics-based debugging.

Microsoft measured deployment using \Gls{wer}\index{Windows Error Reporting (WER)}. 
\emph{``Finally, both Microsoft and a number of third parties use the WER database to check for regressions. Similar to the strategies for measuring deployment, we look at error report volumes over time to determine if a software fix had the desired effect of reducing errors. We also look at error report volumes around major software releases to quickly identify and resolve new errors that may appear with the new release."}~\sidecite[][p. 114]{kinshuman2009_debugging_in_the_very_large}.

Microsoft uses the term bucket while \myindex{Android Vitals} uses the term cluster as a container for grouping together failures considered to be common according to their respective algorithms. The Microsoft WER papers discuss various flaws in their bucketing algorithm and yet the pragmatic and productive results they obtain despite these flaws.

WER seems far more dynamic in terms of data collection; and it also aims to provide recommendations to end users to obtain fixes~\sidecite[][p. 103]{kinshuman2011_debugging_in_the_very_large}.

\newthought{User-choices}
In their published research, according to Microsoft, users had to opt-in and explicitly grant Windows permission to send reports to Microsoft. More recently Microsoft removed the ability for end users to opt-out of sending diagnostics data eg.. from 2015~\sidecite{keizer2015_windows_10_makes_diagnostic_data_collection_compulsory} and from 2021~\sidecite{posey2021_how_to_reclaim_your_privacy_from_Windows10_part_2} which mentions the ability for end users to at least see and delete the data that's collected \emph{``Microsoft not only allows you to delete the diagnostic data it has collected, you can also use the Diagnostic Data Viewer to see exactly what data has been collected."} NB the screenshot shows the viewer entails up to 1GB of storage!

Interestingly in~\sidecite[][pp. 105-106]{kinshuman2009_debugging_in_the_very_large} they mention Microsoft provides corporate administrators with the ability to collect the data on a private server and to control what information is sent to Microsoft, as part of their \acrfull{cer} service.



\emph{``Given finite programmer resources, WER helps focus effort on the bugs that have the biggest impact on the most users."}~\sidecite[][p. TBC]{kinshuman2009_debugging_in_the_very_large}. Similarly, Android Vitals and other crash analytics helps developers focus on the bugs that have the biggest impact on the most users.


\emph{``An early mantra of our team was, ``data not decibels." Programmers use data from WER to prioritize debugging so that they fix the bugs that affect the most users, not just the bugs hit by
the loudest customers."}~\sidecite[][p. 104]{kinshuman2009_debugging_in_the_very_large}. 


\emph{``Bucketing Effectiveness''} Secondary (etc.) buckets/clusters were observed in the three main analytics services (Android Vitals, Crashlytics, and Microsoft App Center) studied during this research. None of the development teams in the app-centric case studies systematically checked the contents of the buckets/clusters, and the ephemeral nature of the reports and empirical nature of the research meant the error rate of the bucketing/clustering has not been calculated. (Microsoft in comparison published their analysis of the effectiveness of their bucketing algorithm in~\sidecite[][pp. 112-113]{kinshuman2009_debugging_in_the_very_large}.) 

Flaws in the bucketing/clustering reduce the likelihood app developers will maximise their effectiveness in terms of addressing the most frequent and pervasive bugs.
% \afterpage{\clearpage}


\emph{``While not ideal, WER's bucketing heuristics are in practice effective in identifying and quantifying the occurrence of errors caused by bugs in both software and hardware."}~\sidecite[][p. 113]{kinshuman2009_debugging_in_the_very_large}.


Similarly, while the bucketing/clustering wasn't ideal in any of the analytics tools studied in this research, the developers of the apps were able to address many of the issues being reported, and the reasons they did not address issues was not related to flaws in the grouping of the underlying failures.


\emph{``Results from the development of Windows Vista, mentioned in Section 1, suggest that present static analysis and model checking tools will find at least 20 bugs for every one bug found by WER. \textbf{However, the bugs found by WER are crucial as they are the bugs which have slipped past tools in the development cycle.}"}~\sidecite[][p. 115]{kinshuman2009_debugging_in_the_very_large}.


\section{Empower users}~\label{discussion-empower-users}
Now may be the ‘golden age’ in terms of app developers being able to default to gathering mobile analytics data, as the legal context may constrain this practice in the future. In particular, legislation may require developers to change the default to not collecting mobile analytics data unless users explicitly opt-in. There may be further constraints on what data can be collected and who is responsible for protecting and preserving the data~\sidecite{bbcnews2022_google_signup_fasttrack_to_surveillance, beuc2022_google_surveillance_action}. 

Conversely, currently users have little autonomy or control about data being collected and analysed related to their use of mobile apps. For example, when users create a new Google account the default one-step activation enables extensive tracking including web and app tracking~\sidecite{bbcnews2022_google_signup_fasttrack_to_surveillance, beuc2022_google_surveillance_action}. A Google account is essential for using an Android device. % https://www.beuc.eu/FastTrackToSurveillance
This means Google at a platform-level is likely to collect analytics from the vast majority of Android users about their app usage. 

From an app developer's perspective, this extensive collection of platform-level analytics can provide benefits including the ability to learn about and address various problems in their apps. Furthermore, they are not directly responsible for any aspect of the service apart from as users of the analytics reports.

As discussed earlier, at a per-app level, the majority of Android apps include at least one mobile analytics library. These apps could offer \textbf{privacy by default} however developers and/or their organisations mainly choose to collect data by default. Some apps offer users an option to control the data collection by mobile analytics SDK(s); in turn the mobile analytics SDK needs to  provide developers with a mechanism to disable/enable collection of the underlying data.

As the vast majority of Android apps in Google Play are free~\footnote{96.3\% on \nth{30} June 2022 according to \url{https://www.appbrain.com/stats}} the app developers may consider the provision of analytics data as a \emph{quid pro quo}. And yet there may be plenty of scope to improve the situation for these two key stakeholders in the mobile app ecosystem, who in turn are part of a larger platform. An interesting business-focused discussion on platforms in \sidecite{KAPOOR2021_socio_technical_platform_ecosystems_etc} also applies to various Android platforms (from Google, Amazon, and others) and Apple's iOS platform.

Some users may wish to have access to the analytics data that's being collected. There are no fundamental reasons why this should not be technically possible, especially for opensourced SDKs where app developers and/or the SDK developers could modify the SDK to provide such access. Furthermore, although current mobile analytics SDKs transmit the data transparently and automatically, it is possible to involve the users in the transmission either upon request by the user or more generally. As a simple example the author and the Kiwix project have implemented the information can be sent in an email generated by the app; the Kiwix UI to send a diagnostics report is shown in Figure \ref{fig:kiwix-diagnostic-report-ui}, it was based on another opensource project \href{https://github.com/Ereza/CustomActivityOnCrash}{Ereza/CustomActivityOnCrash}. % As an aside and as a historical note: the original design and intention was to collect crash reports, see https://github.com/kiwix/kiwix-android/wiki/Crash-Reporting dated 2 Jul 2019. And see https://github.com/kiwix/kiwix-android/issues/335

\begin{figure}
    \centering
    \includegraphics[width=6cm]{images/kiwix/Screenshot-Diagnostic-Report-30-Jun-2022.pdf}
    \caption{Kiwix: asks user what data they would like to send}
    \label{fig:kiwix-diagnostic-report-ui}
\end{figure}

The Android platform does not currently provide any documented way, such as an API, for an app to check the user's setting for providing diagnostics information. If it did, app developers could then make their apps adapt accordingly. Note: The adaption might be more nuanced than simply applying the same setting within the app as users may wish to control the provision of analytics on a per-app or contextual basis, for instance.
% At the time of writing neither Apple (iOS) nor Android appear to provide an API for programmers to enable apps to check per-user or per-device preferences for usage and diagnostics information sharing. 
The lack of such an API means each app developer is responsible for deciding whether to ask user's for permission or simply assume their app can collect and send analytics data.

Listing~\ref{code:androidx_preferences_example} is an example Google provides for Android developers to learn how to use the AndroidX preference library~\sidenote{Reproduction permitted as the code sample is released under their~\href{https://developer.android.com/license}{Content License}.}. This example generates a GUI to ask users if they wish to enable message notifications and/or send feedback including reporting technical issues. 

\begin{listing}[H]
\caption{AndroidX preference library example} \label{code:androidx_preferences_example}
\begin{minted}[fontsize=\footnotesize]{XML}
<PreferenceScreen
    xmlns:app="http://schemas.android.com/apk/res-auto">

    <SwitchPreferenceCompat
        app:key="notifications"
        app:title="Enable message notifications"/>

    <Preference
        app:key="feedback"
        app:title="Send feedback"
        app:summary="Report technical issues or suggest new features"/>

</PreferenceScreen>
\end{minted}
Source: {\small \href{https://developer.android.com/guide/topics/ui/settings}{developer.android.com/guide/topics/ui/settings}}
\end{listing}


Preferences, permissions, and usage analytics share similarities in terms of considerations such as informed consent, whether the settings are temporary or permanent, and so on.


\newthought{Informed consent}
is an awkward conundrum for both end users and app developers as neither is particularly interested in the minutiae when compared to the functionality of an app.  
At the time of the Catrobat case-study their Android app opened with a mandatory screen where users were faced with the following text in Listing \ref{pocketcode-privacypolicy}:

\input{empirical-studies/pocket-code-android-policy}

Welcome back, 727 words 4,590 characters later~\sidenote{According to \url{https://wordcounter.net/}}. This example of an app privacy policy here provides you with an idea of how adding a mandatory, relatively comprehensive privacy policy interrupts the flow. % One of the topics in the future work chapter, \href{enhancing-quality-vs-enhancing-ux}{\emph{\nameref{enhancing-quality-vs-enhancing-ux}}}, considers whether developers may obtain greater return on their investment by improving user experience rather than improving technical qualities of an app. As \href{enhancing-quality-vs-enhancing-ux}{Figure \ref{fig:Firebase-pocketcode-android-7-day-new-user-retention-29-may-2020}} shows, the Pocket Code app only retains 4\% of new users by day 2. Perhaps, the low retention rate may restrict the growth of the user-base even though the quality has improved markedly.
Few users would voluntarily read these terms, and, via Firebase Analytics, we discovered that the Pocket Code app only retained 4\% of new users by day 2 which indicates many may simply abandon the app when presented with these terms at startup.  % IIRC there's quite a bit of research that aims to identify improvements to UIs and to presenting information to end users. For now, I'll skip digging for details. 

Both `informed' and `consent' are important considerations - how to improve (and perhaps even check for) end users being sufficiently informed, and then also providing mechanisms where end users can freely consent to whatever data is gathered for analytical purposes. Or is it enough for developers to deem use of an app as providing sufficient consent? This appears to be an area well worth further research. 

Other changes to the relationships between end-users, their organisations where appropriate, app-developers, mobile analytics providers, the app store, and platform providers may need to be re-envisaged as changes to the availability of analytics data may upset current business models.

% Additional topics could include rewards and payments for information, users paying to use the software, other sources of funding e.g. along the lines of https://creativecommons.org/2019/09/16/grant-for-the-web/

In summary, currently end-users of mobile apps have unnecessarily limited choices in terms of the data collected by app developers or the Android platform. It is practical to improve the service provided to end-users and to provide them with greater freedom and control over the data collection. Some may wish to have access to the underlying data too. The platform could provide app developers with information about the user's data collection preferences and could also implement changes to help give users more choice while also protecting them from intrusive data gathering (both Google and Apple have made changes in relationship to other tracking mechanisms~\sidecite{bbcnews2022_google_moves_to_make_android_apps_more_private}). 


\section{Empower developers}~\label{discussion-empower-developers}
Helping to improve the competence of developers so they create higher-quality apps pre-release is an area on which platform providers (e.g. Google and Apple) can focus their efforts.  Given the breadth and depth of usage and failure data that the platforms collect, there is scope to develop tools that use that data to forewarn developers of probably-flawed code in their codebase. Cloud-based developer-oriented utilities including Amazon’s CodeWhisperer~\sidecite{desai2022_amazon_codewhisperer}, and CodeGuru~\sidenote{\url{https://aws.amazon.com/codeguru/}}, and GitHub’s copilot~\sidenote{\url{https://github.com/features/copilot}} services demonstrate several large-scale approaches aimed at helping developers improve their productivity and code quality. Mobile Analytics has the potential to feed tools and algorithms that would provide developers seamless mechanisms to similarly improve the quality of their mobile apps based on real world measurements and real world results.

\subsection{Using hackathons}~\label{discussion-half-life-of-hackathons}
Both the Kiwix and Catrobat case studies included a hackathon\index{Hackathon} early on. For Kiwix the improvements in the crash rate were almost immediate as a new release of the app with several fixes was released within a week of the session at the hackathon. The project team also increased their focus on addressing crashes and other stability issues which drove ongoing significant improvements over a series of releases of the core app; and when the custom apps were also updated their stability also improved markedly. %TODO link to the findings 

For the Catrobat case study, the improvements took a bit longer to take effect based partly on the more involved pre-release work. The second release after the hackathon had further improvements and between them they improved the stability significantly. %TODO link to the findings 
However, the improvements then petered out. We had arranged for a second more involved hackathon in the form of a 1-day pre-conference workshop at TestFest 2020, a conference with about 500 participants. %TODO link to more details about the planned workshop.

The workshop happened; however, the early effects of \myindex{COVID-19} becoming a global pandemic meant that some workshop participants did not come and remained at home and many of the rest were also the conference organisers who ended up spending their time hurriedly reorganising many aspects of the conference instead of participating in the workshop. The subsequent increase in restrictions related to COVID-19 are a likely cause of why there was little further progress in terms of addressing stability issues for the Catrobat project.

\newthought{Half-life of hackathons: }
The two hackathons were both immediately effective in enabling the respective development teams identify and fix various live issues in their mobile apps. In both cases the effects were cumulative in terms of improvements to the product (the app), however the effect on the teams was short-lived in terms of improving their engineering practices. Unfortunately, the planned second hackathon for the Catrobat project was ineffective because of the rollout of \myindex{COVID-19} so we were not able to measure the effects of cumulative hackathons in the case studies~\sidenote{From previous industry experience, sequences of hackathons have been effective at building on previous work and in some cases hackathons became an established ongoing practice used by development teams.}

\newthought{Some limitations of these hackathons: }
By their nature, hackathons\index{Hackathon} comprise the various participants and people who are not participants do not contribute to them. 

For the Kiwix hackathon the participants were already in Stockholm, Sweden, and they were a subset of the Kiwix contributors who had gathered in Stockholm (the others were involved in other aspects of the project and/or Wikimedia projects). At the end of the week-long event they departed and were no longer co-located. Most of them were part-time volunteers for the project.  

For Catrobat, the hackathon was scheduled for both days of a weekend: Saturday and Sunday. Towards the end of the first day the participants decided they would prefer to finish the hackathon early rather than return on the Sunday. They contributed to the project on a longer-term and more formal basis (as under-graduate or post-graduate students) and would be working on the project the following week.

As discussed previously, the hackathons had an immediate positive effect on the quality of the product but little effect on the engineering practices. Effective engineering leadership\index{Engineering leadership} (see \secref{aiu-engineering-leadership-topic}) is key to effect longer-term improvements in the engineering practices.

\subsection{The effects of pricing}
The topic of pricing was not actively discussed as part of the case studies. Nonetheless, pricing may also be a factor in the choice of SDK, service, and in determining who on a development team has access. Several of the in-app mobile analytics tools provide a free basic service limited to one or a few team members.  Some also offer bands of pricing where they increase the data retention, and/or provide addition integrations with tools used as part of the software development practices of the team \emph{e.g.} to Slack, \emph{etc.} 

\subsection{Left untended, entropy returns}
Several of the projects described in the case studies have had increased failure rates post the case studies, as confirmed by reviewing the relevant analytics reports which continued to be made available for research purposes. In each case, the increase occurred after a loss in focus on actively addressing causes of production errors being reported by the respective analytics tools.

\newthought{Post case study - Kiwix Android}
The Kiwix Android case study was scaled back in early 2020, however I continued to monitor it as a background activity with the agreement of the project leads. One of the many side-effects of the \myindex{COVID-19} pandemic was a significant reduction in funding for the Kiwix project in Summer 2020 which culminated in the end of the funded lead-developer role for the Android codebase. The lead developer finished working on the project in late 2020 and since then the project has been mainly maintained by a mix of volunteers on a part-time basis. 

The crash and ANR rates have both increased in 2021, and are 0.73\% and 0.46\% respectively for the 30 days to \nth{11} June 2021 for the Kiwix Android app (the most recent release is v3.4.4 released on \nth{20} May 2021). Many of the most common crashes are related to the embedded WebView component.

\subsection{Strategic vs Tactical Uses}~\label{aiu-strategic-vs-tactical-uses-topic}
The \secref{aiu-motivating-factors-theme} introduced the concept of strategic vs tactical behaviours by developers in their use of mobile analytics. Figure \ref{fig:aiu-strategic-tactical-example} illustrates several of the app-centric case studies on a continuum (some weren't able to be assessed for various reasons). Of these:

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/rough-sketches/aiu-strategic-tactical-example.pdf}
    \caption{Strategic-Tactical}
    \label{fig:aiu-strategic-tactical-example}
\end{figure}
%%%% Source Arosha's sketch, TODO replace with a more complete version.

\begin{itemize}
    \item \myindex{GTAF} became more strategic in their use after the case study.
    \item \myindex{LocalHalo} started out strategically but appeared to lose focus and their developer(s) were not given access to \myindex{Sentry} despite it being used.
    \item \myindex{Kiwix} became significantly more strategic in the use \myindex{Android Vitals} while the professional lead developer was part of the team.
    \item \myindex{Moonpig} were highly strategic in their use, and also used adapted their use tactically when unexpected issues occurred.
\end{itemize}

\subsection{ROI and Fidelity}~\label{tata-roi-and-fidelity-topics}
These two topics, ROI and Fidelity, draw on various items mentioned in \secref{tata-fitness-for-purpose-section} and flaws \secref{tata-flaws-topic}.

\newthought{\Gls{roi}: } developers may make both implicit and explicit choices on what to invest in. One motivating factor for actually applying mobile analytics is the perceived return on investment in terms of time, energy, opportunity costs, and so on. Developers are more likely to incorporate mobile analytics into their practices when both they and their organisation perceive the return on investment is highly net positive. The analytics tools need to convince developers a) to invest and then b) whether to increase that investment (and if so what forms of investment e.g. in terms of writing more code, spending [more] money, using the tool more, etc.). If a tool or service fails to convince developers of the \Gls{roi} then \emph{in practice} it is not fit for purpose.

\begin{kaobox}[frametitle=ROI in software development]
Note: this calculation is not limited to use of mobile analytics, for instance it may also apply to the practice of developers creating and maintaining automated tests, \textit{etc}.

As one of the answers to an internal survey at Microsoft noted: \emph{``Telemetry allows us to prioritize investment for code cleanup and bug fixing in a way that has substantial
customer impact.''}~\sidecite[][p. 992]{buse2012_information_needs_for_software_development_analytics} Mobile Analytics provides telemetry and therefore helps inform discussions regarding \Gls{roi}. 
\end{kaobox}


The CTO of \myindex{LocalHalo} clearly recognised the \Gls{roi} of fixing crashes. As he said during the initial interview~\sidenote{Repeated here for ease of reading.}: 

\begin{quote}
    ``If you have lots of crashes you have zero chance of being promoted [by the app store].''
\end{quote}

The \myindex{Kiwix} project provides a good example of the ROI where two major crashes were addressed within a few hours of elapsed time and resulted in a decrease in the crash rate of the app of approximately a third. The next chapter provides more details of this example in \secref{aata-improvements-in-crash-rates-topic}.

A common and consistent theme that emerged was the importance of engineering leadership\index{Engineering leadership} who sees the value of applying mobile analytics and related software quality engineering practices in order to use mobile analytics effectively to improve the mobile app development processes. 

    
\newthought{Fidelity: }
an accurate, faithful representation of what has happened in terms of the stability, reliability, and performance of an app. In \secref{tata-fidelity-topic} differences of up to 30 fold were reported by developers in the results presented by different mobile analytics, and similarly various concerns emerged about the fidelity of the reports in several of the app-centric case studies, including \myindex{Catrobat} and the commercial, \myindex{C1} project, and in my mini-experiments.

Fidelity does not seem to have been a topic for research in software analytics generally, for instance it's not mentioned (and nor are threats to validity) in the work of Microsoft where they used software analytics within their Lync Windows software (a forerunner to Skype for Business)\sidecite{musson2013_leveraging_the_crowd_how_48000_users_helped_improve_lync_performance}. There seems to be an implicit assumption the reports and measurements are correct, at least in terms of what has been published on the topic. This research has considered fidelity as a topic and also includes some preliminary measurements and results of flaws in the fidelity.

\subsection{What's ``good enough"? and for whom?}
Developers in the app-centric case studies seemed to be willing to accept the mobile analytics reports at face value. Various flaws were discussed in the previous chapter, nonetheless even flawed mobile analytics may serve the needs of the vast majority of app developers. 

\newthought{Conservation of energy: } means developers may not be willing or practically able to fix some issues, they may lack the capacity and/or the time and/or the information and/or the environment/context needed to fix particular issues. These issues are hard to fix for these developers currently, making conservation of energy a potential anti-pattern when considering code quality.
Therefore, one of the strategic considerations is how to enable developers to have justified confidence they can address the more challenging issues.

Additionally, the developers are not the only stakeholders. 
The first group of stakeholders are closely related to the developers, where the people work for the same organisation. There may be organisation or business owners who want the app to be successful. There may also be a corporate IT department and Compliance, and so on, who have an influence on the use of mobile analytics and the success criteria. 
The end-users may also have a view on what's good enough both in terms of any use of mobile analytics and in the quality of the app and the service the app provides. 
The other significant stakeholder would be whoever provides the mobile analytics service, again they may have their own criteria for what's good enough in terms of the service they provide and in terms of the measured quality of the app. As we have identified Google and Huawei have documented criteria for failure rates of apps in their app store.

\subsection{Choosing mobile analytics}~\label{discussion-choosing-mobile-analytics-topic}
Many factors may affect the choice of mobile analytics, ranging from a wish to not track usage within the app, as demonstrated by \myindex{Kiwix} and \myindex{Catrobat}, though to projects that choose to integrate two or more mobile analytics SDKs into an app, such as \myindex{LocalHalo} and the \myindex{C1} project. None of the development teams mentioned explicitly having requirements to fulfil as part of choosing the services they use, nor of testing that the SDKs actually behaved correctly.  

\afterpage{\clearpage}
\subsection{Using Mobile Analytics as a fulcrum}~\label{discussion-using-mobile-analytics-as-a-fulcrum}
% Source a discussion with Joe Reeve on \nth{10} September 2021 where he proposed the idea of mobile analytics being used as a pivot point by teams to encourage, enable, and allow changes in behaviour without blaming the team. Here's this new source of data which can help us improve our app...
\begin{kaobox}[frametitle="On changing behaviour"]
``The most valuable outcome of adopting a new tool is usually a change in business practices, rather than the tool itself.
Changing people’s behaviour is difficult, because it means implicitly telling them that the current behaviour is wrong. A new tool is a pivot point that acts as an excuse to change without having to acknowledge flaws.

There are various ways that adopting a tool impacts business practices, for enterprises this is typically the function of a product’s Customer Success team. I believe an important function of product design is to move as much education from Customer Success into the product. A great product should not only facilitate best practices, it should be a vector of change management to enforce them." Joseph E. Reeve, \nth{10} September 2021 in discussion.
\end{kaobox}

Inspired by the work of \sidecite{winter2022_lets_talk_with_developers_etc_automatic_program_repair} we may envisage ways mobile analytics can help developers to repair programs, in this context, their mobile apps and supporting software, though a range of automated analysis and possibly even proposed automatic program repairs. Mobile analytics can already help developers understand what went wrong where, and reduce the time and effort needed to debug issues~\sidecite[][p. 10]{winter2022_lets_talk_with_developers_etc_automatic_program_repair}.



\section{Improving analytical capabilities}~\label{discussion-improving-mobile-analytics}
Areas currently underserved by mobile analytics tools include bug localisation and bug reproduction. Mobile analytics based on failures that occur for end users are derived from presence-only data, based on ~\sidecite[][p.1383]{warton2010_poisson_point_process_models_solve_the_pseudo_absence_problem_for_presence_only_data_in_ecology}. 

\begin{itemize}
    \item``\emph{Note that this does not consist of all locations where an Angophora costata tree is found—rather it is the locations where the species has been reported to be found.}'' (from ~\sidecite[][p.1383]{warton2010_poisson_point_process_models_solve_the_pseudo_absence_problem_for_presence_only_data_in_ecology})
    \item The analytics report contains [all] the failures that have been reported, it does not consist of all the failures that occurred. The word all is in square brakets as some failures might be screened-out or removed from the reports by the processing of the data for various reasons including: suspected fraud/spam, issues with the content of the message, \emph{etc.}
\end{itemize}
\afterpage{\clearpage}

Furthermore the analytics do not report on when the same code did not fail in use. Both bug localisation and bug reproduction could be improved when the entire population of usage data were available to be analysed. Currently mobile analytics tools such as Microsoft App Center provide developers with the option to integrate their general purpose mobile analytics SDK into an app, by doing so, App Center is able to calculate a percentage of the percentage of the users that a crash or error affects. 
%
However, as mentioned previously, neither the developers nor the analytics services are likely to obtain data for the entire [underlying] population of users. I propose we do not yet know enough to determine the effects of only the calculations being based on subsets of a population of mobile app users. In a business context there has been debate on the effects, flaws, and limitations of Google Analytics using sampling (for example in \sidecite{mangold2020_google_analytics_data_sampling_what_you_need_to_know, rauhut2021_how_accurate_is_sampled_google_analytics_data_a_simple_experiment_to_find_out}).
% See also:
% https://matomo.org/blog/what-is-google-analytics-data-sampling-and-whats-so-bad-about-it/
% https://support.google.com/analytics/answer/2637192?hl=en

Certainly in the app-centric case studies there have been examples of frequent volumes of failures being attributed to a single user, for instance as shown in \secref{section-intermittent-appearances-of-bugs-55-crashes}. 

Hypothesis: At this stage I estimate developers are likely to be able to find and address somewhere between 90\% and 99\% of material failures using mobile analytics gathered from a majority of the overall population (active userbase). Both the 90\% and 99\% are based on the 70:20:9:1 concept in~\sidecite{rugg2015_beyond_the_80_20_principle}; while it may be an oversimplification it seems adequate as a heuristic. And given that even the best performing app developers have software where failures, including crashes, occur in use it's unlikely that any development team with a popular app will ever reach zero failures in use~\sidenote{As a concrete example, even Google was only able to reduce the crashes caused by \texttt{NullPointerExceptions} by 30\% despite a year spent migrating news feature development to \myindex{Kotlin} (\href{https://developer.android.com/topic/performance/vitals/crash\#prevent-crashes-null-pointer}{Prevent crashes caused by null pointer exceptions}).}. There are likely to be `black swan events' in terms of failures \emph{i.e.} failures not visible to the development team even though they occur~\footnote{See comments in the following online article \url{https://theconversation.com/friday-essay-a-rare-bird-how-europeans-got-the-black-swan-so-wrong-161654}.}.


An approach similar to delta debugging~\sidenote{Nicely described and illustrated in \url{https://www.st.cs.uni-saarland.de/dd/}} as currently practiced may help identify relevant factors/context while also removing irrelevant factors/context that led to failures in the app. (Related work exists for web applications: On the use of delta debugging to reduce recordings and facilitate debugging of web applications their work relied on recording user input and replaying these using Selenium and reproduced a small number of examples.) Locating the causes of failures is a topic long discussed in the literature. In contrast to that work, at least some failures in mobile apps depend on elements close to the user, such as their mobile device, their network connectivity, their profile, their past and/or current use of the software, and so on. Furthermore there’s unlikely to be an existing automated test that reproduces the failure, otherwise why wasn't the failure addressed pre-release?

Once the conditions have been found that reproduce the failure, techniques can be applied to simplify conditions that reproduce it~\sidecite{zeller2002_simplifying_and_isolating_fault_inducing_input}. If users are willing to participate in the bug reproduction process, approaches such as Reproducing Context-sensitive Crashes of Mobile Apps using Crowdsourced Monitoring might be viable. Using Mobile Analytics for crashes, freezes, and other performance issues addresses these topics described in App Store 2.0~\sidecite{gomez2017_app_store_2.0_from_crowdsourced_info_to_actionable_feedback_in_mobile_ecosystems}. 

\subsection{Bug reproduction}
In the author's experience, app developers are frequently faced with failures they are not able to reproduce practically. Practical limitations of being able to reproduce crashes is also corroborated by numerous \emph{ad-hoc} informal discussions with development teams for many apps in industry. This indicates app development teams may need other practical mechanisms to determine whether failures have been ameliorated or even fixed. Concepts such as relative correctness e.g. in~\sidecite{ghardallou2016debugging_without_testing} show promise in terms of comparing the failures reported in subsequent releases of an app, the absence of a particular failure \emph{might} indicate the failure has at least been partly addressed provided the developers have made attempts to address at least one possible cause of the failure.
% https://www.whyprogramsfail.com/

\newthought{Crash reproducibility:} Reproducing and repairing crashes in Android apps has received significant attention from academic researchers. In~\sidecite{tan2018_repairing_crashes_in_android_apps} they were able to automatically fix some of the crashes that were able to be repeatably triggered by existing GUI-driven automated tests. In at least one case the automated fix was more correct than the flawed one the original developer had implemented. 

As their approach relies on having crashes that can be consistently reproduced via the GUI it is orthogonal to using mobile analytics as a source for crashes, nonetheless it could potentially help provide developers with adequate fixes for failures where the developers have been able to devise and either record the sequence of events or create automated tests the perform the necessary sequence of events. Developers could compare and contrast any fixes they create with those created using the repair tool, \textit{Droix}, devised as part of this research~\sidecite{tan2018_repairing_crashes_in_android_apps}. Note: as the authors admit, their approach worked for relatively uncomplicated crashes that could be reproduced on Android emulators. Virtually all the mobile analytics failures occurred on real Android devices so there is a gap that would need to be bridged between their work and crashes reported via mobile analytics.  

Reproducibility of a failure is clearly a factor in order to demonstrate any improvement and in both academic research and in practice reproducibility is desirable. That said, as various of the app-centric case studies demonstrated, in \secref{aiu-bug-reproduction-topic}, developers cannot reproduce some of the failures being reported by mobile analytics. % Relative Correctness,

\newthought{Bug localisation: } Orthogonal to bug reproduction, bug localisation is a similar challenge developers would like solving. 


\subsection{Debugging in the large}
Debugging in the large is a term presented in Microsoft's research into Windows Error Reporting~\href{glossary-wer}{(WER)} where large scale collection of errors from end-user devices can help developers detect, prioritise, and address various errors that may not appear during development and local testing of the software~\sidecite{kinshuman2011_debugging_in_the_very_large}. The various mobile analytics services analysed during this research also support debugging in the large (and they suffer from some of the flaws that were reported in WER a decade ago). 

This research corroborates arguments presented in \sidecite{gomez2017_app_store_2.0_from_crowdsourced_info_to_actionable_feedback_in_mobile_ecosystems} where the app store (Google Play) generates actionable feedback for developers with the aim of helping developers improve the performance and quality of their apps. To be fair to Google, their tools in Google Play predate the paper. However the authors might not have had first hand experience of using those tools. % Abstract: App Store 2.0 will exploit crowdsourced information about apps, devices, and users to increase the overall quality of the delivered mobile apps. App Store 2.0 generates different kinds of actionable feedback from the crowd information. This feedback helps developers deal with potential errors that could affect their apps before publication or even when the apps are in the users' hands. The App Store 2.0 vision has been transformed into a concrete implementation for Android devices.

It may be viable to significantly accelerate troubleshooting and reduce the latency by using techniques that were described and applied in a project called CellScope by~\sidecite{padmanabha2018_mitigating_the_latency_accuracy_tradeoff_in_moile_data_analytics_systems}. They demonstrated their model was able to make useful and relatively accurate predictions of anomalies in power consumption by mobile apps within one day of the Carat app in~\href{https://play.google.com/store/apps/details?id=edu.berkeley.cs.amplab.carat.android&hl=en_GB&gl=US}{Google Play}. Unfortunately this Android app was last updated for end users in 2018 and the Carat power recommendation app project no longer appears to be active according to the respective codebases for: Android\footnote{\url{https://github.com/carat-project/carat-android}, last updated in August 2019} and iOS\footnote{\url{https://github.com/carat-project/carat/}, last updated in February 2018}. Furthermore, the CellScope software does not appear to have been published so future research may need to recreate something similar from scratch.



\section{Safeguards}~\label{discussion-safeguards-section}
The research identified several issues that could affect end users' privacy and safety when using apps that include mobile analytics. Additionally, the observations on how analytics are used has some potential legal risks to the development teams as well. This section discusses these issues together with relevant safeguards that could improve the use of mobile analytics and the functionality of the associated tools. 

\subsection{\emph{Quis custodiet ipsos custodes?}}~\label{discussion-quis-custodiet-ipsos-custodes-topic}
Or, `Who watches the watchers?'~\sidenote{\href{https://en.wikipedia.org/wiki/Quis_custodiet_ipsos_custodes\%3F}{en.wikipedia.org/wiki/Quis\_custodiet\_ipsos\_custodes\\\%3F}} 
The app store is all powerful, promoting one app, demoting or even blocking another. What it reports is expected to be taken on trust. And who has time or resources to check or verify the veracity of what is being reported and to whom? As the Editorial Board of the Financial Times proposed three rules to build trust in the use of automated algorithms in decision making~\emph{``algorithms that companies and governments deploy in sensitive areas such as healthcare, education, policing, justice and workplace monitoring should be subject to audit and comprehension by outside experts."}\sidecite{ft2021_building_trust_in_ai_systems_is_essential}. Given the revenues, powers and business implications of the major app stores, I believe they should also be subject to audit and comprehension by outside experts.

To the credit of several providers of analytics SDKs, including Crashlytics and Huawei's crash reporting SDK, they provide mechanisms to test their crash reporting service. However, these tests do not necessarily mean the analytics are completely accurate, reliable, and scalable. For example, in an online tutorial, a developer at Huawei explained~\emph{``Note : If an error is occurred in try catch block, Crash service won’t catch it. Crash Service just catches critical error that is stopped app"}~\sidecite{huawei2020_appgallery_connect_crash_service_article_on_medium}. Conversely, Google Android collects usage statistics from small percentages of users from WebView components incorporated into Android apps including crash reports. An example of WebView crash reports for specific Samsung device models is discussed on Stack Overflow~\sidecite{ebling2018_so_s9_specific_webview_device_crash_report}. Google discusses user privacy implications in these crash reports~\sidecite{android_webview_privacy}.

Also, as Huawei acknowledged in the version history for their Android SDK their SDK previously caused some crashes (in versions 1.5.0.300, for instance) and needed improvements in the crash rate (in version 1.5.2.300) accuracy~\sidecite{huawei_android_crashservice_sdk_version_change_history}. Similar issues may well exist in other Analytics SDK and related services. 

It's encouraging to see Huawei's honesty which increases their credibility in this area. Even better would be for all the analytics providers to release the source code and the measurement techniques they use so both of these can be independently checked and verified.


\subsection{Ethics of incorporating mobile analytics}
Tradeoffs; comparing other approaches which may have similar effect with less/no tracking.

Our work exists within a wider context and in society, and the choices we make when we design and implement software and related systems may affect the lives of many people. This applies for those working with mobile apps. Recent investigations into 5000 Android apps discovered 200 leaked sensitive information through logging this data to the device log~\sidecite{zhou2020_mobilogleak}. Contents of the device logs can be read by other software that has, or obtains, permission to the log. 
%
Similarly to the ethics and concerns of what to log into a log file, what is logged through using mobile analytics also matters.

Mobile analytics may also log similar sensitive information and automatically send it for analysis and processing therefore similar concerns and similar research into the practices of developers and the behaviours of the apps their create may be pertinent. Some of the sensitive data is logged automatically by the mobile analytics library where the developer (and potentially their management and/or clients) is effectively making the decision to share the sensitive information by incorporating the library into the software they create.

The ownership and who has a) access b) control of the data collected by mobile analytics are also germane. For example, can users stop the data being collected at source? (and is it legitimate for them to do so?) do they have easy to use facilities to address the `right to be forgotten'~\sidecite{gdpr_article_17_right_to_erasure}. The right to be forgotten is also a popular topic for news articles, for instance with an estimated 129 articles on the topic in The Guardian newspaper in the UK~\sidecite{guardiannewspaper_right_to_be_forgotten_articles} at the time of writing. 

The data collected by mobile analytics may have ethical implications a) for the operator/provider of the service, b) for their partners and customers, c) for the developers, d) for end-users. In this research our main focus is on the implications for the developers, nonetheless the other aspects are also important.

As the Catrobat team discovered when they migrated from Fabric to Firebase there were the additional, unexpected analytics that appeared. They considered this sufficiently unethical and intrusive that they removed the product from their mobile app.

The topic of what app developers should do in terms of disclosing personal data lawfully was covered in \sidecite{vandersype2014_on_lawful_disclosure_of_personal_user_data_what_should_app_developers_do} by considering the EU Data Protection Directive, in particular. In their work they identified various limitations of their work including the ``legally undefined'' role of app developers [p.33](\emph{ibid}) nonetheless their encouragement for developers to take privacy-friendly design decisions [p.31](\emph{ibid}) still holds especially as another conundrum emerges when considering whether the data collected by a particular app's use of mobile analytics would include personal data. 

Research into \emph{Data Ethics} is emerging, for example, the first international workshop on ethical data-centric design of intelligent behaviour~\sidecite{datethics2020_workshop}. I participated in this inaugural online workshop particularly for the following topics in the call for participation: `Personal activity data as design material', `Responsibility and values in designing with data', and ```from data to actions"'. My particular request for the workshop was to consider ways to truly enable new users of mobile apps to make~\emph{informed consent} when they need to be informed the app may collect analytics. We had rich, enlightening discussions on this and related topics and the potential to collaborate on future research on this and the larger topic of ethics in data-centric design; initial results are available as a visual PDF at~\url{https://mobilehci-2020.datacentricdesign.org/#results}.

\subsection{Litigation on unwanted data collection}~\label{discussion-litigation-on-unwanted-data-collection-topic}
On \nth{14} July 2020, a case was filed in Northern California, USA as a class action against Google and the parent company Alphabet~\cite{rodriguez_et_al_v_google_llc_et_al_2020} stating \emph{``No matter what safeguards are put in place, mobile app users cannot prevent Google intercepting, collecting, tracking and selling for profit their browsing histories and internet activity."} where the plaintiffs investigate the behaviours of Google with a particular emphasis on the use of \myindex{Firebase Analytics} which does not honour Google's claimed commitments to end users to be able to disable such data from being shared by their Android device. Blogs, including~\sidecite{winder2020_forbes_on_the_class_action_firebase_analytics}, discuss the claims and several related recent incidents pertaining to Firebase.

These incidents include data leakages where developers did not secure their Firebase databases appropriately, as reported in~\cite{bischoff2020_firebase_missconfiguration} the configurations do not appear to be secure by default. Firebase databases are distinct from Firebase Analytics, however one might infer that developers who don't secure their Firebase databases (which are incorporate into their apps by the same development team) they may be similarly insecure in their use of Firebase Analytics. According to a news article by Reuters~\sidecite{dave2020_reuters_firebase_squeeze} Google are pushing developers to integrate Firebase into their apps through offering improved business benefits for that app's ecosystem. If the claims are accurate then even more developers are likely to use Firebase and Firebase Analytics. Unless those developers actively consider and mitigate for privacy related features that users can control there may be additional ethical concerns.

% https://www.classaction.org/news/always-watching-class-action-against-google-alleges-user-privacy-doesnt-exist#embedded-document
% https://lawstreetmedia.com/tech/google-is-always-watching-class-action-complaint-says/
% https://www.reuters.com/article/us-alphabet-google-privacy-lawsuit/google-faces-lawsuit-over-tracking-in-apps-even-when-users-opted-out-idUSKCN24F2N4
% https://www.scribd.com/document/469160855/2020-07-14-Dkt-1-Rodgriguez-Et-Al-v-Google-LLC-Et-Al#download
% 

\subsection{Beware of implicit, automated data collection} \label{discussion-beware-implicit-automated-data-collection-topic}
There are various considerations of the adverse implications of allowing, and using, implicit automated data collection, such as often performed by mobile analytics libraries in industry. A simple example is that the library implementer may choose to change the functionality of the automatic data collection, rename, restructure, or remove content developers have come to expect and rely on, etc. This topic is discussed in a blog post by Iteratively~\cite{mukherjee_implicit_versus_explicit_event_tracking_hits_and_misses}.

A more involved example started in March 2017 where the \myindex{Mixpanel} JavaScript \Gls{sdk} which inadvertently collected passwords and might have collected other highly sensitive data such as \emph{``where browser plugins (such as the 1Password password manager) and website frameworks place sensitive data into form element attributes."}~\cite{mcclintok_mixpanel_update_on_autotrack_data_collection}. 

According to the post, the problem did not exist when they designed and launched the service in 2016, it was triggered when they updated the version of an external and well regarded library in March 2017. The issue was discovered and reported by a customer in January 2018. Mixpanel deleted the sensitive data their library had collected on behalf of their customers and implemented various corrective actions including in-depth security reviews of existing code. They also had to put in place filters to delete new data on arrival as some of their customers were still using the ill-mannered implementation of their JavaScript SDK. % MUST-DO Also cite paper about when devs do and don't update library versions. 

The \myindex{Catrobat} case study also contributed insight into the retiring Fabric Crashlytics mobile analytics service and the replacement, \myindex{Firebase Crashlytics}, and how their reports and capabilities compared with those provided by \myindex{Google Play Console} with \myindex{Android Vitals}.

\newthought{A related finding in our PocketCode case study: } 
In February 2020 the \myindex{Catrobat} team migrated the reporting for their app crash analytics (Crashlytics) from the Fabric website to the Firebase website using an inbuilt migration tool provided by Google who own both services. The migration meant the app's crash data could be viewed in both these websites independently until Google disabled the Firebase website (planned for March, but postponed until \nth{4} May 2020 in response to the effects of the \myindex{COVID-19} pandemic). 

We noticed additional data was available and presented in the reports provided in the Firebase console, as Google describe the website. This includes demographic data \emph{even though this is not collected by our app or - officially - by the crashlytics library}! Figure \ref{fig:Firebase-event-demographics-pocketcode-android} provides an example of the relevant section of the `Events' report for the \myindex{Pocket Code} Android app. App developers may, understandably, be unaware that using a library for collecting crash data is also somehow gathering data about demographics, therefore their end user licence agreements (EULA), privacy policy, and so on, may not reflect or incorporate this information to inform the user of what is being collected and why (in potential contravention with GDPR and other legislation). As mentioned previously, in \secref{aiu-choosing-mobile-analytics-topic}, the Catrobat team then chose to stop using Crashlytics and removed it from the \myindex{Pocket Code} app.

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=9cm]{images/firebase/Firebase-event-demographics-pocketcode-android.pdf}
    \caption{Firebase Event Demographics for Pocket Code Android}
    \label{fig:Firebase-event-demographics-pocketcode-android}
\end{figure}

There are a range of practical options developers can take to safeguard themselves and their users, which are not mutually exclusive:
\begin{itemize}
    \item Do as others do: as a form of herd immunity.
    \item Proportionate exchange of value, along the lines of the following rationale: users do not pay for the app, and few actively provide feedback, therefore collecting mobile analytics, especially to improve the app is fair and proportionate. 
    \item Offer an opt-out: where the default setting is to collect data using mobile analytics. Relatively few users find and actively disable the default, nonetheless the developers can argue the users have a free choice.
    \item Offer an opt-in: inverts the previous configuration, users would have to find and enable the data collection. The tradeoff is participation and therefore the data collection is likely to be low and patchy.
    \item Passive feedback, for example: leave the user to take action to provide information as the \myindex{Kiwix} project did and as I did with one of my older opensource Android apps called Android Daisy Reader.
    \item Disable and remove any in-app mobile analytics.
\end{itemize}

As an example of changes in society, organ donation is far more intrusive and yet English law is changing\footnote{\href{https://www.organdonation.nhs.uk/uk-laws/}{www.organdonation.nhs.uk/uk-laws/}} to presume consent~\sidecite{NHS_organ_donation_in_england}. Mobile analytics is currently mandatory for users of many apps yet there may be changes that mean this tenet no longer holds at some point in the future. Legislation may be a source of these changes, a topic discussed next.

\subsection{Legal aspects of using mobile analytics for development and testing} \label{discussion-legal-aspects-of-using-mobile-analytics-topic}

Data is being bought and sold freely online~\sidecite{nytimes20210721_the_nightmare_of_our_snooping_phones} what's to stop the analytics data being sold. How would an app developer like their competitors buying and using their failure data? or a hacker who's looking to break into a mobile banking app? See also~\sidecite{nytimes20191221_total_surveillance_is_not_what_america_signed_up_for} and the series~\href{https://www.nytimes.com/interactive/2019/opinion/internet-privacy-project.html}{www.nytimes.com/interactive/2019/opinion/internet-privacy-project.html}. \emph{``According to noyb, the unique tracking code generated by each iPhone lets Apple and all iPhone app developers see how users behave without their knowledge or agreement"}~\sidecite{ft2020_apple_tracks_iphone_users_without_consent}.

The Financial Times newspaper argues for a separation of roles of app store providers who are simultaneously the market owner and \emph{`` an app developer while operating as judge, jury, executioner and court of last appeal for all others."}. The article argues \emph{``If Apple does not itself update its App Store to distinguish between those roles and become more flexible and transparent, then it can hardly complain if legislators eventually deploy far more blunt instruments to enforce those changes."}~\sidecite{ft2020_apple_risks_losing_an_epic_challenge}.

This leads to the topic of \emph{``Who should make the online rules?"}~\sidecite{nytimes20210111_who_should_make_the_online_rules} 
Businesses in the USA have the right to make rules for what happens in their systems. Having basic rules, applied consistently, is vital in order for these businesses to operate effectively. However, the article notes \emph{``these companies’ rules are extensive, they are also capriciously applied and revised solely at their whim."}. The appeal processes and how they're enacted are seldom clear or effective. In \secref{discussion-on-methodology-and-case-study-procedure} and \secref{discussion-quis-custodiet-ipsos-custodes-topic}, the power dynamics between the developers and the app store was introduced, where legitimate developers ejected from Google Play Store~\sidecite{dodson2019_google_completely_terminated_our_new_business_etc, martinez2019_google_just_terminated_our_startup_google_play_publisher_account_on_xmas_day, marcher2021_how_google_terminated-a-developer}. %Plenty more material available from and via https://jilliancyork.com/

\begin{comment}
% Removed as I can't recall where these were cited from and they also look messy as a list of quotes.
\begin{itemize}
    \item \emph{``I can think all these tech companies made the right decision in the last few days but still feel extremely uncomfortable that they are in the position of acting as a Supreme Court — deciding for billions of people what is appropriate or legal expression and behavior."}
    \item \emph{``And while these companies’ rules are extensive, they are also capriciously applied and revised solely at their whim."}
    \item \emph{``Apple and Google are largely the only places for people to download smartphone apps. Amazon is one of a tiny number of companies that provide the backbone of many websites. Facebook, Google and Twitter are essential communications services for billions of people."}
\end{itemize}
\end{comment}


The design and implementation of the mobile analytics may invoke GDPR considerations for whoever is responsible for the mobile app. In one of the appendices there is an example where concerns about the Firebase Android \Gls{sdk} and the dubious use of a ContentProvider.

\begin{kaobox}[frametitle="Firebase Android SDK's use of a ContentProvider"]
Although the SDK for Firebase was designed to minimise code developers needed to write there are various circumstances when developers need to take control and explicitly write code to configure and initialise the firebase SDK. These are discussed in~\sidecite{firebaseblog2017_take_control_of_your_firebase_init_on_android}.

In~\sidecite{techyourchance2021_contentprovider_in_android_libraries_considered_harmful}, the author rebukes Firebase's choice of using a ContentProvider for initialisation and cites Legal (GDPR), development, maintenance, and performance challenges together with the shift in complexity for developers who use Firebase, from writing a few lines of initialisation code, to dealing with an essential configuration file \texttt{google-services.json}. 

As of \nth{11} September 2022 there are 3,921 search results for the file on Stack Overflow~\sidenote{\href{https://stackoverflow.com/search?tab=relevance&q=google-services.json}{stackoverflow.com/search?tab=relevance\&q=google-services.json}.}. Perhaps this is a good example of needing to beware of unintended consequences?
\end{kaobox}

The final consideration for this topic is \Gls{pii} \emph{vs.} non-PII information. Would the use of non-PII information be subject to any legal considerations, and - of course - decisions would need to be made on what is actually considered to be classified as PII in a court of law in various jurisdictions.

\afterpage{\clearpage}

Data ownership and safeguarding may well emerge as key considerations for app developers at some point in future even if the catalyst for what triggers the emergence is hard to pin down precisely. 

\begin{comment}
%\subsection{Considerations and concerns when using Mobile Analytics}~\label{discussion-considerations-and-concerns-topic}
The following list of considerations are an \emph{aide-memoire} for whoever wishes to use mobile analytics.

\begin{itemize}
    \item Privacy: protecting the privacy of the end users. 
    \item Ownership and uses: especially in terms of who owns the data and who gets to use it?:
    \item Stewardship: Impact(s) of having access to sensitive and valuable data.  
    \item Sufficiency: in the context of collecting sufficient to enable us to achieve our objectives of improving our software and our processes.    
    \item Costs: financial, data, privacy, performance, bloat. These are closely aligned to performance aspects.
    \item Performance: which includes runtime overhead, transmission overhead, latency and scalability. 
    \item Trust: [Over] trust in decisions made by technology, see \secref{discussion-human-behaviours-around-automation-topic}.
\end{itemize}
    
\end{comment}


\section{Mitigation techniques}~\label{discussion-mitigation-techniques-section}
Summary: \emph{Reduce the incidence, reduce the severity, prevent the trigger, fix if and when practical given the many constraints the team, and the individuals,  are operate under.}

\subsection{Approaches to reducing failure rate}
There are many approaches to reducing the measured failure rate. The `logical' one is to fix whatever the cause is, or at least write code that handles causes robustly, reliably and in ways that serve the user well. And if this is easy and practical many developers and projects will adopt this approach. However like many things that involve people and their motivations they may choose other approaches for various reasons, such as:

\begin{itemize}
    \item Information gathering: where the cause of a failure is not known and not understood, programmers may add code to collect and report additional information to enable them to learn enough about the context of the failure in order to address it once they have learned enough to do so.
    \item Error masking: for instance, catching an exception so it does not get reported or counted.
    \item Error avoiding: change the flow of control of the code so the unreliable path is avoided where practical.
    \item Reduce the incidence: in a similar way to back-off retry algorithms used elsewhere in software~\sidenote{For example, back-off algorithms were designed into Ethernet retransmissions, in logins after previous logins failed, and used in GMail for retrying failed connections, \emph{etc.}} apps can be coded to reduce the frequency of failures. 
    %%%% See also https://github.com/OWASP/owasp-masvs/blob/master/Document/0x09-V4-Authentication_and_Session_Management_Requirements.md
    % https://mobile-security.gitbook.io/mobile-security-testing-guide/general-mobile-app-testing-guide/0x04e-testing-authentication-and-session-management
    \item Functional amputation: generally that of failing code deemed so flawed it is not worth trying to correct. Post amputation a replacement may be provided, perhaps from another source and either used instead or grafted into the current project (for instance using an external software library). The replacement of the custom downloader in Kiwix Android is one example where this approach was used.
    \item Blocking: Prevent users from using the app on devices where severe problems occur, \emph{``... exclude those devices until a fix is available."} is one approach recommended in Google's Android Vitals Best Practices~\sidecite{android_vitals_best_practices}.
\end{itemize}
\afterpage{\clearpage}

Note: a reviewer of an earlier version of the above list argued they were merely bad software development practices - perhaps. And yet, empirically they are practiced to varying degrees and may mitigate against high failure rates when it is impractical to fix a cause quickly, completely, and efficaciously.

Fixing bugs may be considered a chore~\sidecite{scrumdictionary_chore} by many, and there are discussions in the context of Agile software development practices whether work on bugs should be allocated story points at all~\emph{etc.}~\sidecite{se2012_story_points_for_bug_fixing_tasks_in_scrum} where one respondent was adamant that they should not, otherwise developers would be earning points for poor quality work~\sidenote{Source: \href{https://softwareengineering.stackexchange.com/a/162166/93935}{softwareengineering.stackexchange.com/a/162166/93935}}; furthermore the tester is blamed for not finding the bugs (as if that were a) practical or b) appropriate). In response to seeing three crash clusters for their app a project manager responded~\emph{``I don’t know how broad these categories are, but my understanding is that fixing bugs isn’t the sexiest thing to do in life. What if we could free up some budget and hire a team to do the boring work?"}~\sidenote{From an unpublished email discussion on addressing crash clusters.}  Those who fix bugs rather than work on features may be perceived as adding less value to a project or team, hence the seemingly illogical `bad' practices may actually be logical in terms of game theory. Interesting research is emerging in applying game theories to software development~\sidecite{GAVIDIACALDERON2021_game_theoretic_analysis_of_software_development_practices} and may help provide insights into how sources of crashes and other failures are addressed in practice and lead to improvements to reliability of mobile apps through applying game theories.


\subsection{Trade-offs of: ``Fail Early"?}~\label{discussion-tradeoffs-of-fail-early-topic}
Fail Early has been argued as a helpful policy in various contexts. %MUST-DO expand and provide examples. 
Fail Fast mentioned in~\sidecite{gray1986_why_do_computers_stop_and_what_can_be_done_about_it} but not explained or discussed. Make bugs easy [easier] to find~\sidecite{shore2004_fail_fast_software_debugging}.

The benefits of applying a fail fast approach are described in~\sidecite{shore2004_fail_fast_software_debugging} with the aim of immediate and visible failure. For rare failures and those the developer cannot easily track down, this approach may help the developer learn about causes of the failures (for instance by raising an explicit exception in the program) where the failure is reported by mobile analytics. For platform level analytics, these fail fast failures would result in crash clusters reported by the analytics. However, \myindex{Android Vitals} does not currently include the message portion of exceptions. The examples in~\sidecite{shore2004_fail_fast_software_debugging} make extensive use of the messages for communications, so when developers rely on platform-level analytics they may need to consider ways to augment the communications of failures (such as incorporating a crash reporting library or service into their app).


\begin{kaobox}[frametitle=An alternative approach]
We did explore an alternative approach, where the application code added meta-information as an entry in the stack trace as a proof-of-concept. 
The implementation is available online at~\href{https://github.com/ISNIT0/AndroidCrashDummy/blob/cfa7f0817c436d7e657741e0a5d9a76644e5a898/app/src/main/java/com/example/user/androidtestapp/MainActivity.java\#L28-L50}{AndroidCrashDummy MainActivity.java\#L28-L50}, and reproduced in \secref{app-mini-experiment-meta-dat-to-crash-logs}. 

We decided not to release an app in Google Play with this capability at the time, not least in case it caused Google to suspend our Google Play Store account. 
\end{kaobox}

For in-app analytics the developer has the option to report the error using the respective API.\index{Microsoft App Center} An example of reporting an error with additional parameters is shown in Listing~\ref{listing:microsoft-app-center-handled-exception}~\sidenote{The Microsoft App Center documentation is released under a Creative Commons Attribution 4.0 International Public License~\href{https://github.com/MicrosoftDocs/appcenter-docs/blob/live/LICENSE}{App Center Docs LICENSE}.}.

\begin{listing}
\begin{minted}[
    gobble=0,
    frame=single,
    fontsize=\footnotesize,
    breaklines=true
  ]{java}
try {
    // your code goes here.
} catch (Exception exception) {
    Map<String, String> properties = new HashMap<String, String>() {{
        put("Category", "Music");
        put("Wifi", "On");
    }};
    Crashes.trackError(exception, properties, null);
}
\end{minted}
\caption[Microsoft AppCenter: example of reporting a crash in Android]{Example of reporting a crash in Android.  Source~\href{https://docs.microsoft.com/en-us/appcenter/sdk/crashes/android}{ Microsoft App Center documentation}}
\label{listing:microsoft-app-center-handled-exception}
\end{listing}


When developers do not write exception handlers for parts of their code, unhandled exceptions cause the app to crash when they occur. The user may be annoyed by this behaviour if the crash visibly~\sidenote{Here visibility refers to the user noticing the crash and being aware of the crash adversely impacting their experience.} affects their use of the app. The aphorism, \emph{`every cloud has a silver lining'}, may yet hold true in these circumstances provided developers become aware of the crash and choose to address it suitably, as these crashes are detected by various mobile analytics tools and platforms and then reported to the development team.

In contrast a lukewarm, and sometimes expedient approach, is where developers add minimalist \texttt{try\{\}...catch\{\}} handlers to wrap the code that may throw an exception. These exceptions are no longer `unhandled' however nor are they adequately handled in terms of the application providing error management, or attempts to recover from the error, or informing the user of a problem that's occurred, and so on. %MUST-DO provide some code snippet examples and analysis of the prevalence of the use of this approach.

To add to the complexities, app stores are seldom disinterested observers. If the measured crash rate increases beyond what they deem an acceptable threshold (1\% for \myindex{Huawei} %~\sidecite{huawei_introduction_to_appgallery_connect_crash_service}
, 1.09\% for Google Play's bad behavior threshold) they may reduce the visibility of the app in the app store and/or otherwise mark it down. Therefore developers need to be mindful of their strategy to crashes and crash reporting and aim to keep the overall failure rate of their app well below any bad behaviour threshold.

Using software libraries can also add to the complexities; and some have sophisticated error handling strategies which include expectations on how developers will write their code to use that library adequately. \myindex{RxJava} is a very popular library for Android apps. That library explicitly changed their error handling strategy in version 2.0 and require developers to abide by their requirements for error handling~\sidenote{\href{https://github.com/ReactiveX/RxJava/wiki/What's-different-in-2.0\#error-handling}{RxJava/wiki/What's-different-in-2.0\#error-handling}}. In version 2.0~\emph{``RxJava defaults to printing the Throwable's stacktrace to the console and calls the current thread's uncaught exception handler."}. The wiki page goes on to say~\emph{``Unfortunately, RxJava can't tell which of these out-of-lifecycle, undeliverable exceptions should or shouldn't crash your app."} So developers who use this library may end up with their app terminating abruptly where the reported crash rate also increases as a result.

% Subject to approval or a suitable alternative example: add the OkHttpv3 example from the large case study in the relevant section, then discuss the example here.


\subsection{Disproportionate returns}~\label{discussion-disproportionate-returns-topic}
The failure footprint~\sidenote{A first approximation of a suitable measure of a failure footprint is multiplying the frequency by the number of affected users. Projects may choose to develop their own algorithm if this is not sufficient for their context and needs.} varies, as does the effort needed to address specific failures. As noted in the \myindex{Kiwix} case study, fixing two of the top three most frequent crashes reduced the overall crash rate by around a third. Neither of the fixes were complicated. This does not hold true for all failures, some have a tiny footprint, for instance one developer (for the opensource \myindex{SmartNavi} Android app)
mentioned they had seen a few sporadic crashes on phone models they had no access to and decided it would be futile to try and address these crashes in the circumstances. Similarly, the lead developer for the \Gls{gtaf}\index{GTAF} Android apps explained the developers consider the ease of fixing crashes before tackling them. 

Developers therefore can obtain higher returns in terms of improvements in measured quality if they choose to fix causes that lead to the most frequent failures where the time-to-repair is small. In 2020 Google confirmed the most common crash type reported in Google Play Console is the \myindex{\texttt{NullPointerException}} \sidecite{googleblogs2020_google_home_reduces_crashes_by_a_third}.  % https://developer.android.com/topic/performance/vitals/crash#prevent-crashes-null-pointer

Google's example corroborates the findings by Microsoft in terms of the distribution of error buckets in \secref{rw-windows-phone-store-crash-analysis-section}. There's an even stronger grouping according to grey literature where a company called OverOps analysed 1 billion java errors recorded in logs and determined that fixing the top ten most frequent errors would reduce the crash rate by 97.3\%~\sidecite{overops2021_what_causes_97pct_of_1billion_java_logged_errors}. One of their top ten is also the \texttt{NullPointerException}. Their article does not explore how practical it would be to address those ten errors. 

This (my) research, on using mobile analytics, currently has only found one instance where a single failure cluster was the cause of so many errors~\sidecite{kiwixandroid_issue_2907_anrs_exceed_bad_bhaviour_threshold_for_android_10_and_11}, where there is a common pattern for 98\% (+/- 1\%) of the ANRs. (At the time of writing the project has yet to determine whether there is a single cause and a single fix.) Nonetheless, both the research and this example from industry illustrate the value in fixing the worst offenders provided the bugs are tractable and easy to fix within the timescales available to the developers. Furthermore, mobile analytics done well helps keep the signal-to-noise ratio high in terms of aggregating and reporting on patterns of failures.


\subsection{Human behaviours around automation}~\label{discussion-human-behaviours-around-automation-topic}
% Confirmation bias (via Isabel)
The use of software automation can have deep implications on the humans involved, ranging from emotional stress and worries where testers on development teams can get stuck where tools are expected to solve problems (but don't)~\cite{evans2020stuck}, to potentially over-reliance on the decisions made by automation~\cite{cummings2004automation} and a complacency and bias in human interaction with automated and decision support systems~\cite{parasuraman_complacency_and_bias_in_human_use_of_automation}. 
My experiences of email discussions with the Google Engineering team provided an impression of a misplaced belief in the correctness of their analytics tools and the inability of any outsider being able to understand their system. % From Google's perspective I'm an outsider even though I was an employee for 4 years.
This is somewhat ironic given that I was a senior software quality engineer at Google for four years and worked on assessing the behaviours and qualities of Google's mobile apps for several years during that role.



\section{Summary of the Discussion Chapter}~\label{discussion-chapter-summary-section}

%%%%%%%%%%
% Include discussion that the world moves on.

This chapter contains various discussion topics, including: validity of my research and of the analytics tools, where the various analytics tools provide the most value, ethics and legal aspects, abandoned apps, and finally other app stores beyond Google Play.

In \secref{rw-caveats-with-software-analytics-topic} one of the key questions was \emph{``Software Analytics: so what?''}, now is a suitable moment to consider the so what aspects of using mobile analytics to improve the quality of mobile apps. 

Firstly, mobile analytics has been seen to be effective when app developers address failures being reported by these mobile analytics. Reflecting on the proposals in~\sidecite{rugg2015_beyond_the_80_20_principle}, in some cases addressing a small percentage of the reported issues reduces the failure rate by well over 70\% the failures are not even in magnitude nor in their ease of being addressed (a reminder that the \myindex{GTAF} developers only chose to fix crashes they deemed easy to fix). Some failures are far harder to address, for example in the \myindex{Kiwix} project, issue 2907~\sidecite{kiwixandroid_issue_2907_anrs_exceed_bad_bhaviour_threshold_for_android_10_and_11} remains unresolved at the time of writing and it may require a wholesale replacement of software in order to stop these failures.


\subsection{Decision making by development teams}~\label{discussion-decision-making-by-dev-teams-section}
As an untested hypothesis, as part of their decision making when triaging an issue: developers may apply an informal form of failure mode and effect analysis (FMEA) based on multiple-criteria decision making (MCDM) using similar criteria to the work described in \sidecite{lo2018_novel_multi_criteria_decision_making_based_FMEA_model_for_risk_assessment}, \emph{i.e.}, severity, occurrence, detection (of a cause they can fix), and expected cost. An area of future research could be to further explore the habits of decision making during triage and in ways to improve the decision making and the outcomes of those decisions.


\subsection{Red pill or blue pill?}~\label{discussion-red-pill-or-blue-pill}
% This topic may be too controversial, speculative, or awkward for my thesis. It may be deferred e.g. for the proposed game and mechanics paper.
Like various software development activities beyond writing the code for the core product (mobile apps in the context of this research) developers have a choice whether to perform and incorporate various non-core activities. These include using mobile analytics first as a consumer of existing content (such as that provided by Android Vitals or the default offerings of in-app analytics) and then potentially as activists, designing and implementing, modifying and enhancing, the client side code including any mobile analytics SDK, and potentially taking responsibility for the server-side code and infrastructure. 

Conversely some developers may choose to insulate themselves from the effects of mobile analytics, for instance by adding try/catch code to smother errors and exceptions reported when the code runs. Doing so can artificially improve the measured reliability of an app as the crash rate decreases, however it may lead to other errors and to a poor end user experience.

Developers have an ongoing choice whether to take ``red pills'' or ``blue pills''~\sidecite{wikipedia_red_pill_or_blue_pill} depending on their willingness to be exposed to messy reality in terms of how their software performs in use. This choice is not unique to software developers and applies to researchers~\sidecite{pecorini_blue_pill_or_red_pill} amongst others. There's an interesting debate in \sidecite{bringsjord2012_red_pill_robots_please} on whether to create blue pill robots or not. In the paper's Objections section raised the concept of a `\emph{correspondence account} of truth'~\sidecite[][p. 396]{bringsjord2012_red_pill_robots_please} mobile analytics enables developers to investigate reality by providing such a correspondence account. The harder challenge to overcome is for developers who may be indifferent to the truth of the situation in terms of the performance of their app. While ~\sidecite[][p. 396]{bringsjord2012_red_pill_robots_please} argues no such engineer exists not all app developers are engineers, and in some of the app-centric case studies there were often times when the mobile analytics were far removed from the daily activities of the development teams.

\subsection{Necessary? Sufficient?}~\label{discussion-necessary-sufficient-topic}
Freezes and crashes may adversely impact a user's perspective, Google may limit the visibility of apps that score poorly in terms of 'stability' as I will cover later in my thesis. However, are performant applications sufficient to thrive? There may be other barriers, or hurdles, developers need to overcome such as providing software that is attractive, serves whatever the user wants to do, and is intuitive.

Research published by Microsoft on \Gls{wer} states~\emph{``WER augments, but does not replace, other methods for improving software quality."}~\sidecite{kinshuman2009_debugging_in_the_very_large, kinshuman2011_debugging_in_the_very_large}. Similarly, using mobile analytics to find and prioritise failures to address augments rather than replaces other methods for improving software quality. For failures that are able to be detected and reported using mobile analytics - relying solely on using mobile analytics to detect and address them may overwhelm developers in fix, patch and release cycles rather than other improvements to the app. Furthermore users of these failing apps may simply stop using it, reject it, and the developers who provide the app~\sidecite{dimensionalresearch2015_mobile_app_use_and_abandonment}. Other quality issues that mobile analytics does not capture, need to be addressed using other techniques and tools.

In 2019 Menzies in his paper \emph{``Take Control: On the Unreasonable Effectiveness of Software Analytics"}~\cite{menzies2019take} indicates that a small number of predictable software analytics variables can be used to predict software qualities for projects. 

\emph{``Specifically, the number of variables required to make predictions about SE projects is remarkably small which means that (a) most of the things we think might affect software quality have little impact in practice; (b) controlling just a few key variables can be enough to improve software quality."}

This research indicates mobile analytics can be used to effectively and efficiently improve the measured stability qualities of various Android apps from disparate sources, development teams and software stacks. And yet, there are appear to be material flaws in key analytics tools. This research touches on various implications of using analytics including which projects are best served by the integral analytics Google provides in Google Play Console. There are numerous areas of potential interest to researchers and industry alike including the integrity and accuracy of the various tools and reports.

The next chapter provides a summary of how this research provides us with a deeper understanding of how mobile analytics are used and can be used more effectively. Much remains to do in the area of applying mobile analytics to the development practices. Real-world bugs, including several I experienced, are not likely to be detected by the developers without a combination of some of all of: improved testing practices, rich in-app feedback, and using and applying in-app mobile analytics. Based on these observations, related areas of future work will also be discussed in the next chapter. 
